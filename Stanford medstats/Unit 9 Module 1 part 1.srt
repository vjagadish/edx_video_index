1
00:00:07,466 --> 00:00:08,894
[BLANK_AUDIO].
In this next module,

2
00:00:08,894 --> 00:00:14,177
we're going to dive right into the
logistic regression model.

3
00:00:14,177 --> 00:00:16,013
We've talked about, when you have binary
or

4
00:00:16,013 --> 00:00:20,075
categorical outcomes, that, that is when
you're looking at proportions.

5
00:00:20,075 --> 00:00:24,120
We've talked about some statistical tests
when you don't want to adjust for

6
00:00:24,120 --> 00:00:25,400
anything that you can use.

7
00:00:25,400 --> 00:00:28,495
So, this is like the risk different,
relative risk, the chi-square test,

8
00:00:28,495 --> 00:00:29,850
the McNemar's chi-square test.

9
00:00:29,850 --> 00:00:30,970
So, we've talked about those.

10
00:00:30,970 --> 00:00:34,900
But what about when you want to do a
regression analysis when you

11
00:00:34,900 --> 00:00:37,660
have a binary or categorical outcome?

12
00:00:37,660 --> 00:00:41,150
In that case, you're going to be using
logistic regression,

13
00:00:41,150 --> 00:00:43,380
which we're going to talk about in this
module.

14
00:00:43,380 --> 00:00:47,740
I want to just give you a little overview
of logistic regression.

15
00:00:47,740 --> 00:00:50,550
So, to remind you when we did linear
regression,

16
00:00:50,550 --> 00:00:52,390
the model looked like the following.

17
00:00:52,390 --> 00:00:56,465
We were predicting the value of some
continuous outcome variable,

18
00:00:56,465 --> 00:00:58,236
call it Y as a function of X.

19
00:00:58,236 --> 00:01:01,085
Here's the simple linear regression model.

20
00:01:01,085 --> 00:01:04,165
We have the intercept alpha and beta is
our slope.

21
00:01:04,165 --> 00:01:07,322
So, that was the simple linear regression
model in

22
00:01:07,322 --> 00:01:09,360
the multivariate regression model.

23
00:01:09,360 --> 00:01:14,700
We can add some additional betas and add
Xs additional predictors.

24
00:01:14,700 --> 00:01:19,760
That makes it than a plane or a higher
dimension thing.

25
00:01:19,760 --> 00:01:22,900
But essentially, we are fitting lines,
linear regression.

26
00:01:22,900 --> 00:01:25,690
Logistic regression actually isn't that
different.

27
00:01:25,690 --> 00:01:30,340
The right-hand side of the equation is
identical in logistic regression.

28
00:01:30,340 --> 00:01:31,870
So, it's going to look something like
this.

29
00:01:31,870 --> 00:01:37,130
There's a linear combination of beta's
and, and X's risk factors.

30
00:01:37,130 --> 00:01:42,070
It looks exactly the same, and everything
is going to be set up exactly the same,

31
00:01:42,070 --> 00:01:45,492
in terms of finding the predictors of the
model.

32
00:01:45,492 --> 00:01:47,750
You put can binary predictors and

33
00:01:47,750 --> 00:01:50,730
you can put continuous, you can put
categorical, you can do dummy coding.

34
00:01:50,730 --> 00:01:53,670
All of that is the same as with linear
regression.

35
00:01:53,670 --> 00:01:57,800
What's different in logistic regression is
the left-hand side of the equation.

36
00:01:57,800 --> 00:02:02,129
We are no longer modeling Yi, the outcome
variable, directly.

37
00:02:02,129 --> 00:02:07,111
Rather, we're going to be modeling a
transformation of the outcome variable.

38
00:02:07,111 --> 00:02:10,060
And logistic case, logistic regression
we're going to

39
00:02:10,060 --> 00:02:15,120
be modeling the log odds of the outcome
and I know that sounds terrible.

40
00:02:15,120 --> 00:02:18,130
That's called a logit function, but don't
worry.

41
00:02:18,130 --> 00:02:19,880
I'll explain why in a minute.

42
00:02:19,880 --> 00:02:22,097
We're going to be modeling this logit
function.

43
00:02:22,097 --> 00:02:23,550
The log odds of the outcome.

44
00:02:23,550 --> 00:02:26,250
That's our outcome variable.

45
00:02:26,250 --> 00:02:28,880
We're going to be modeling that as a line
though.

46
00:02:28,880 --> 00:02:33,670
As an alpha plus a beta X, an intercept
and the slope.

47
00:02:33,670 --> 00:02:38,560
Because we made this transformation, the
beta is just slightly more

48
00:02:38,560 --> 00:02:41,860
complicated to interpret than the beta
from linear regression.

49
00:02:41,860 --> 00:02:44,930
So, for linear regression, the beta was
just slopes.

50
00:02:44,930 --> 00:02:48,390
As X goes up by 1 unit, how much does Y go
up?

51
00:02:48,390 --> 00:02:49,888
So, there was a direct interpretation.

52
00:02:49,888 --> 00:02:54,330
It's just slightly more tricky to
interpret the betas from

53
00:02:54,330 --> 00:02:55,300
logistic regression.

54
00:02:55,300 --> 00:03:00,582
But actually its not terrible because the
beta's from logistic regression,

55
00:03:00,582 --> 00:03:04,780
if you exponentiate them, what it gives
you out is Is the odds ratio.

56
00:03:04,780 --> 00:03:08,230
And I referred to, in the second week of
the course, we talked a lot about

57
00:03:08,230 --> 00:03:12,690
odds ratios, and I told you, hey, logistic
regression spits out odds ratios.

58
00:03:12,690 --> 00:03:14,300
And, and this is where it comes from.

59
00:03:14,300 --> 00:03:19,540
The betas can be directly interpreted as
odds ratios just by exponetiating.

60
00:03:19,540 --> 00:03:22,440
And the reason there's an exponential
involved is because our

61
00:03:22,440 --> 00:03:24,750
outcome variable involves a natural look.

62
00:03:24,750 --> 00:03:26,650
So, this is just the overview of where
we're going.

63
00:03:26,650 --> 00:03:29,560
Just keep in mind that the right hand side
of

64
00:03:29,560 --> 00:03:32,400
the equations the same as with linear
regression.

65
00:03:32,400 --> 00:03:33,605
So, we're still fitting a line.

66
00:03:33,605 --> 00:03:35,340
It's just we're fitting the outcome
variable,

67
00:03:35,340 --> 00:03:39,590
there's something trickier going on.

68
00:03:39,590 --> 00:03:45,360
Let me start here with a simple example,
using that example data set, data set

69
00:03:45,360 --> 00:03:50,730
on my Stanford students, that I've been
using since the beginning of the course.

70
00:03:50,730 --> 00:03:54,550
One of the questions I asked my Standford
students is, whether or

71
00:03:54,550 --> 00:03:58,900
not they consider their selves to be book
smart or street smart.

72
00:03:58,900 --> 00:04:00,550
This is a binary variable.

73
00:04:00,550 --> 00:04:04,102
Book smart if, if the variable is equal to
1.

74
00:04:04,102 --> 00:04:06,260
That means that they consider themselves
book smart.

75
00:04:06,260 --> 00:04:10,030
If the variable is equal to 0, the way
I've coded it,

76
00:04:10,030 --> 00:04:12,790
that means that they consider themselves
to be street smart.

77
00:04:12,790 --> 00:04:16,321
So, we can see that this is the book smart
group over here.

78
00:04:16,321 --> 00:04:20,546
And this is the street smart group group
over here.

79
00:04:20,546 --> 00:04:25,450
And I just as as aside, I've been asking
this question to lots of

80
00:04:25,450 --> 00:04:27,420
Stanford students just for fun over the
years.

81
00:04:27,420 --> 00:04:32,260
And it generally comes out about 80% of my
Stanford students consider themselves to

82
00:04:32,260 --> 00:04:35,610
be book smart, and about 20% consider
themselves to be street smart.

83
00:04:35,610 --> 00:04:38,530
And that was indeed, the case in this
little practice data set.

84
00:04:38,530 --> 00:04:41,130
It's right around 80%.

85
00:04:41,130 --> 00:04:44,910
I wondered if book smart students did
more,

86
00:04:44,910 --> 00:04:50,500
spent more time on homework, were more
studious than street smart students.

87
00:04:50,500 --> 00:04:53,390
And so, I wanted to look at the variable
homework and

88
00:04:53,390 --> 00:04:55,310
the number of hours per week that they
spent on homework.

89
00:04:55,310 --> 00:04:58,000
That's another question that I had asked
these students.

90
00:04:58,000 --> 00:04:59,310
And we can look at that here, and

91
00:04:59,310 --> 00:05:01,614
I've done this, I've looked at this
graphically here.

92
00:05:01,614 --> 00:05:05,650
So, I'm graphing my predictor variable is
just a binary, a yes, no,

93
00:05:05,650 --> 00:05:06,640
book smart or not.

94
00:05:06,640 --> 00:05:10,900
And my outcome variable here is homework,
in the number of hours per week.

95
00:05:10,900 --> 00:05:14,560
And now, surprisingly the students who
consider themselves book smart, as you

96
00:05:14,560 --> 00:05:19,020
can see on this graphic here, actually, on
average, do slightly less homework than

97
00:05:19,020 --> 00:05:23,000
the students who consider themselves
street smart, for whatever reason.

98
00:05:23,000 --> 00:05:27,730
So, it went kind of in the opposite
direction from what I might have expected.

99
00:05:27,730 --> 00:05:30,230
This analysis, this graphic that I'm
showing you here,

100
00:05:30,230 --> 00:05:34,330
actually coincides with a linear
regression.

101
00:05:34,330 --> 00:05:38,910
So, the way I set up the problem now is
I've got a binary predictor variable and

102
00:05:38,910 --> 00:05:42,080
a continuous outcome variable, homework.

103
00:05:42,080 --> 00:05:44,820
And we could model this with a linear
regression.

104
00:05:44,820 --> 00:05:46,890
Using what we talked about last week.

105
00:05:46,890 --> 00:05:50,750
When we have a binary predictor, it's a
very simple linear regression.

106
00:05:50,750 --> 00:05:53,830
Right, that line is just you connect to
the mean for

107
00:05:53,830 --> 00:05:56,360
the street smart group with the mean for
the book smart group.

108
00:05:56,360 --> 00:05:59,765
And if you connect two points with a line,
that, that defines a line.

109
00:05:59,765 --> 00:06:01,510
So, we end up with a very simple line.

110
00:06:01,510 --> 00:06:04,360
So, we could answer this question.

111
00:06:04,360 --> 00:06:08,150
Look at this question in this data set
with a linear regression, or

112
00:06:08,150 --> 00:06:11,990
even a t-test, which of course is a
special case of a linear regression.

113
00:06:11,990 --> 00:06:15,140
This is cross-sectional data, however.

114
00:06:15,140 --> 00:06:19,353
So, I've happened to pick just for the
purposes of illustration, in for

115
00:06:19,353 --> 00:06:23,650
this slide to put book smart as the x
variable as the predictor and

116
00:06:23,650 --> 00:06:25,040
homework as the outcome variable.

117
00:06:25,040 --> 00:06:30,480
But because this is cross sectional data,
I could ask, I could flip that.

118
00:06:30,480 --> 00:06:34,970
I could ask whether or not whether you
classify yourself as book smart,

119
00:06:34,970 --> 00:06:36,750
predicts the amount of homework you would
do.

120
00:06:36,750 --> 00:06:39,130
But I could ask it the other way.

121
00:06:39,130 --> 00:06:43,060
Does the amount of homework you do, amount
of time you spend on homework,

122
00:06:43,060 --> 00:06:45,790
predict whether or not you consider
yourself book smart?

123
00:06:45,790 --> 00:06:47,980
So, I'm just going to flip the x and y
variables.

124
00:06:47,980 --> 00:06:51,550
So, here's the graphic when I flip the x
and y variables.

125
00:06:51,550 --> 00:06:54,720
So, now my x, my predictor variable, is
continuous.

126
00:06:54,720 --> 00:06:57,870
The number of hours spent on homework per
week.

127
00:06:57,870 --> 00:07:00,550
And my outcome variable is this binary.

128
00:07:00,550 --> 00:07:01,480
This yes, no.

129
00:07:01,480 --> 00:07:02,900
Zero is street smart.

130
00:07:02,900 --> 00:07:03,825
One is book smart.

131
00:07:03,825 --> 00:07:06,700
We've got a new graphic.

132
00:07:06,700 --> 00:07:10,860
And we could try to fit a line to these
data.

133
00:07:10,860 --> 00:07:13,470
But look what happens when I try to fit a
line to these data.

134
00:07:13,470 --> 00:07:17,470
Notice that all of my my outcomes are
either all one or all zero.

135
00:07:17,470 --> 00:07:19,090
So, you get this two stripes.

136
00:07:19,090 --> 00:07:22,110
And it doesn't, it doesn't seem optimal to
try to fit a line.

137
00:07:22,110 --> 00:07:24,850
I did fit a line to these data.

138
00:07:24,850 --> 00:07:25,570
You can do it.

139
00:07:25,570 --> 00:07:27,010
You, this is a linear regression line.

140
00:07:27,010 --> 00:07:30,310
So, I fit the linear, the best fit line to
these data.

141
00:07:30,310 --> 00:07:33,310
But you can see that if you look at this
picture,

142
00:07:33,310 --> 00:07:37,187
it doesn't really look like that line fits
the data very well, right?

143
00:07:37,187 --> 00:07:41,580
I mean, when we're doing linear regression
last week, we have these scatter plots and

144
00:07:41,580 --> 00:07:44,430
we were trying to find the line that was
close all those dots.

145
00:07:44,430 --> 00:07:46,860
Well, this line doesn't, you can fit it,
but

146
00:07:46,860 --> 00:07:48,840
it doesn't look very close to all of those
dots.

147
00:07:48,840 --> 00:07:51,360
It doesn't look like a very good fit.

148
00:07:51,360 --> 00:07:55,150
So, a linear regression model is not
optimal here.

149
00:07:55,150 --> 00:07:57,070
Well, what do we do?

150
00:07:57,070 --> 00:08:00,000
Again, I told you were're still going to
want to fit lines here.

151
00:08:00,000 --> 00:08:05,840
So, how can we still fit a line but do
something other than linear regression?

152
00:08:08,940 --> 00:08:10,820
How could we, what could we do?

153
00:08:10,820 --> 00:08:15,270
So, what we could do here is to transform
the outcome variable.

154
00:08:15,270 --> 00:08:17,450
Again, I still want to fit a line
ultimately.

155
00:08:17,450 --> 00:08:22,440
But I don't want a zero, one variable as
my outcome because a zero,

156
00:08:22,440 --> 00:08:25,620
one variable just doesn't fit well to a
line.

157
00:08:25,620 --> 00:08:27,200
It has only two possible values.

158
00:08:28,250 --> 00:08:29,520
So, what could I do?

159
00:08:29,520 --> 00:08:32,894
How could I transform that binary, that
zero,

160
00:08:32,894 --> 00:08:37,230
one variable into something that better
fits a line?

161
00:08:37,230 --> 00:08:38,230
Well, here is one thing.

162
00:08:38,230 --> 00:08:39,788
This is where, just a starting point,

163
00:08:39,788 --> 00:08:41,020
because we're going to go further with
this.

164
00:08:41,020 --> 00:08:44,145
But let's just start with, with some
simple thing that we can do.

165
00:08:44,145 --> 00:08:47,225
So, recognize that instead of modeling it
as a yes no,

166
00:08:47,225 --> 00:08:52,580
a zero, one, what if we could somehow
model it as a probability?

167
00:08:52,580 --> 00:08:56,930
A probability, as you know, is a value
that ranges from 0 to 1,

168
00:08:56,930 --> 00:09:01,902
but it can take any value between 0 and 1.

169
00:09:01,902 --> 00:09:03,408
So, we would do a little bit better.

170
00:09:03,408 --> 00:09:08,036
We would put in all these values between 0
and 1, back into play.

171
00:09:08,036 --> 00:09:11,676
If somehow we could model this as we could
make the outcome variable the probability

172
00:09:11,676 --> 00:09:15,957
of being book smart, rather than yes, no,
you are being book smart.

173
00:09:15,957 --> 00:09:18,582
Just looking at the data here, you can
see,

174
00:09:18,582 --> 00:09:25,047
well, that's not immediate obvious,
immediately obvious how you would do it.

175
00:09:25,047 --> 00:09:27,584
In fact, we're going to do something
called the logit,

176
00:09:27,584 --> 00:09:30,525
which is a function of the probability.

177
00:09:30,525 --> 00:09:33,910
And, in fact, what the computer is
going to do is do a little algorithm

178
00:09:33,910 --> 00:09:38,160
involving some calculus to calculate the
probability or the logit.

179
00:09:38,160 --> 00:09:41,170
But we can kind of approximate it, just
actually looking at the data.

180
00:09:41,170 --> 00:09:44,040
And I think this is the best way to, to
understand it.

181
00:09:44,040 --> 00:09:47,960
So, if I wanted to get a probability here,
rather than a 0, 1 variable.

182
00:09:47,960 --> 00:09:51,300
Imagine I did the following, what if I
just to get a probability,

183
00:09:51,300 --> 00:09:52,940
I'm going to have to look at more than one
person.

184
00:09:52,940 --> 00:09:54,520
I'm going to have to look at a group of
people.

185
00:09:54,520 --> 00:10:00,170
So, what if I kind of arbitrarily look at,
you know, a group of people, let's just

186
00:10:00,170 --> 00:10:06,800
pick, say the, the ten people with the
lowest amount of homework time per week.

187
00:10:06,800 --> 00:10:08,440
Notice that these little red dots here.

188
00:10:08,440 --> 00:10:11,110
Actually, some of these dots represent
more than one person,

189
00:10:11,110 --> 00:10:14,520
because there were a lot of people who had
tied values.

190
00:10:14,520 --> 00:10:16,400
They selected the same number of homework
and

191
00:10:16,400 --> 00:10:18,580
also called themselves, book smart, for
example.

192
00:10:18,580 --> 00:10:21,495
So that, each of these dots may represent
more than one person.

193
00:10:21,495 --> 00:10:22,530
So, imagine that this,

194
00:10:22,530 --> 00:10:27,480
this is what I've put in this box here
represents ten people.

195
00:10:27,480 --> 00:10:32,520
I could look at the, say the ten people
with the lowest amount of homework time,

196
00:10:32,520 --> 00:10:33,986
putting in the lowest amount of homework
time.

197
00:10:33,986 --> 00:10:38,150
And I could count up how many of them
called themselves book smart, and

198
00:10:38,150 --> 00:10:41,710
how many called themselves street smart.

199
00:10:41,710 --> 00:10:45,220
And I could calculate a proportion, a
percentage, a probability.

200
00:10:45,220 --> 00:10:48,690
So, let's say I'm just making this up, but
let's say when I calculate that.

201
00:10:48,690 --> 00:10:50,230
Again, some of these dots are overlapping,

202
00:10:50,230 --> 00:10:53,520
I can't do it exactly, but from the
graphic.

203
00:10:53,520 --> 00:10:57,770
But let's say I count them up, and I find
among them doing the least amount of

204
00:10:57,770 --> 00:11:03,610
homework, let's say that 90% of them,

205
00:11:03,610 --> 00:11:06,293
again, I'm just making this up, let's say
90% of that ten.

206
00:11:06,293 --> 00:11:10,810
Nine out of ten, consider themselves book
smart rather than street smart.

207
00:11:10,810 --> 00:11:14,570
Now, I've got a probability, rather than a
zero, one.

208
00:11:14,570 --> 00:11:19,914
And I could do this for the next group of
ten, the next highest in homework.

209
00:11:19,914 --> 00:11:27,440
Maybe in the next group the probability is
again I'm just making this up, is 85%.

210
00:11:27,440 --> 00:11:30,750
And I can't tell because, again, these
dots are overlapping.

211
00:11:30,750 --> 00:11:32,030
Maybe that comes up to be 85%.

212
00:11:32,030 --> 00:11:35,460
But I could go along and I could group
people.

213
00:11:35,460 --> 00:11:38,880
I have to group people in order to be able
to get a percentage, a probability.

214
00:11:38,880 --> 00:11:41,350
But I could come up with some
probabilities.

215
00:11:41,350 --> 00:11:44,380
And of course, the probabilities I come up
with are going to depend on

216
00:11:44,380 --> 00:11:46,350
exactly how I group them.

217
00:11:46,350 --> 00:11:50,590
But just realize that the grouping is just
for the purposes of illustration.

218
00:11:50,590 --> 00:11:52,490
When you actually do this with calculus,

219
00:11:52,490 --> 00:11:54,810
you don't have to make any arbitrary
groupings.

220
00:11:54,810 --> 00:11:57,310
Of course, we won't, go through the
calculus here, but

221
00:11:57,310 --> 00:11:58,710
the computer does that for you.

222
00:11:58,710 --> 00:12:03,735
So, if I could get that probability now, I
can make a graphic.

223
00:12:03,735 --> 00:12:07,760
Which is my predictor variable is the
number of hours of homework per week,

224
00:12:07,760 --> 00:12:09,820
that is a continuous variable.

225
00:12:09,820 --> 00:12:14,590
Now, my outcome variable is a probability,
a number that ranges between zero and one.

226
00:12:14,590 --> 00:12:18,050
I have fewer dots on this graphic.

227
00:12:18,050 --> 00:12:22,700
I'm going to just, for now, talk about the
graphic on the left side, this 10 groups.

228
00:12:22,700 --> 00:12:24,600
I, notice I've fewer dots on that graphic.

229
00:12:24,600 --> 00:12:26,370
None of these dots are overlapping
anymore.

230
00:12:26,370 --> 00:12:31,060
So, I actually have, I had a my previous
graphic represented 48 people.

231
00:12:31,060 --> 00:12:34,180
This graphic only represents ten
observations.

232
00:12:34,180 --> 00:12:40,160
I divided those 48 people up into roughly
even groups of 10 groups of,

233
00:12:40,160 --> 00:12:43,500
you know, somewhere between of about four
or five per group.

234
00:12:43,500 --> 00:12:45,870
And I got those percentages, and I plotted
them here.

235
00:12:45,870 --> 00:12:50,010
What you notice is this is much more
amenable, this,

236
00:12:50,010 --> 00:12:52,290
this scatter plot, to fitting a line.

237
00:12:52,290 --> 00:12:55,210
Right now, we've got dots all around the
range.

238
00:12:55,210 --> 00:13:00,060
And you can imagine actually fitting a
meaningful line to those dots.

239
00:13:00,060 --> 00:13:01,690
As I mentioned, this is just for

240
00:13:01,690 --> 00:13:05,570
the purposes of graphing, just to show you
the concept.

241
00:13:05,570 --> 00:13:08,275
And so, it's arbitrary how I group people.

242
00:13:08,275 --> 00:13:11,280
So, of course the graphics going to look a
little bit different depending on how I

243
00:13:11,280 --> 00:13:11,805
group people.

244
00:13:11,805 --> 00:13:14,440
So, just to show you that, I also divided
it into five groups.

245
00:13:14,440 --> 00:13:15,420
There's 48 people.

246
00:13:15,420 --> 00:13:18,640
So, this is roughly groups of, you know,
eight, nine, or ten.

247
00:13:18,640 --> 00:13:21,920
And I calculated the proportion who
consider themselves book smart,

248
00:13:21,920 --> 00:13:25,370
that probability, in each of those five
groups and I plotted it here.

249
00:13:25,370 --> 00:13:30,800
By the way when I each of these groups of
course has a range of homework values.

250
00:13:30,800 --> 00:13:35,360
I'm using the mean homework time for each
group.

251
00:13:35,360 --> 00:13:36,700
That's what I'm putting in the plot here.

252
00:13:36,700 --> 00:13:40,080
So, for example, the lowest homework group
on this plot here,

253
00:13:40,080 --> 00:13:45,390
they have a mean homework time of just,
you know, a maybe 30 minutes to an hour.

254
00:13:45,390 --> 00:13:49,280
And there, the proportion that consider
themselves book smart was right about 78%.

255
00:13:49,280 --> 00:13:52,775
So, I'm using the mean just for graphical
purposes.

256
00:13:52,775 --> 00:13:58,590
But the picture looks a little different
depending on how you group it,

257
00:13:58,590 --> 00:13:59,800
but you can get the idea.

258
00:13:59,800 --> 00:14:03,570
Now, we've put back all the numbers
between zero and one into play.

259
00:14:03,570 --> 00:14:05,115
It makes much more sense to fit a line.

260
00:14:05,115 --> 00:14:07,580
But this is still limited, okay.

261
00:14:07,580 --> 00:14:10,360
So, this is not, we're going to go further
than that with this.

262
00:14:10,360 --> 00:14:15,114
Because a probability is a number that's
bounded between 0 and 1.

263
00:14:15,114 --> 00:14:17,584
So, it's still a bounded quantity.

264
00:14:17,584 --> 00:14:18,760
You can't go lower than 0.

265
00:14:18,760 --> 00:14:20,230
You can't go higher than 1.

266
00:14:20,230 --> 00:14:22,890
The problem with that is imagine when we
fit a line.

267
00:14:22,890 --> 00:14:27,261
When you fit a line theoretically, that
line can go anywhere from negative in,

268
00:14:27,261 --> 00:14:30,830
the y value can go anywhere from negative
infinity to positive infinity.

269
00:14:30,830 --> 00:14:34,320
So, with this, it's still not perfect.

270
00:14:34,320 --> 00:14:39,780
So, what we're going to do is instead of
directly having the probability

271
00:14:39,780 --> 00:14:42,610
be the outcome variable, we're going to
make a little transformation.

272
00:14:42,610 --> 00:14:44,695
So, probability is bounded from zero to
one.

273
00:14:44,695 --> 00:14:51,170
If I divide the probability however, by 1
minus the probability,

274
00:14:51,170 --> 00:14:54,950
that of course, is what we have learned
earlier is the odd.

275
00:14:54,950 --> 00:14:58,641
So, divide the probability by the
probability of something not happening.

276
00:14:58,641 --> 00:15:01,650
So, the probability divided by 1 minus the
probability.

277
00:15:01,650 --> 00:15:05,710
That's what we learned in week two of the
course is an odds.

278
00:15:05,710 --> 00:15:07,650
Why would I want to do that?

279
00:15:07,650 --> 00:15:10,640
Well, a probability can't be bigger than
1,

280
00:15:10,640 --> 00:15:16,050
but an odds can go from anywhere, anywhere
from 0 to positive infinity.

281
00:15:16,050 --> 00:15:18,060
The odds has no right hand bound.

282
00:15:18,060 --> 00:15:21,960
So now, we've in all the positive real
numbers back into play.

283
00:15:21,960 --> 00:15:24,590
We're still missing the negatives.

284
00:15:24,590 --> 00:15:26,080
How can we get the negatives back in?

285
00:15:26,080 --> 00:15:28,302
So, here's where the natural log comes in.

286
00:15:28,302 --> 00:15:34,370
So, if I then take the natural log of the
odds, it turns out that when you take

287
00:15:34,370 --> 00:15:40,000
a natural log, any value that was below 1,
now becomes a negative number.

288
00:15:40,000 --> 00:15:44,930
So, this, the natural log of the odds,
actually ranges from negative,

289
00:15:44,930 --> 00:15:48,180
can theoretically range from negative
infinity to positive infinity.

290
00:15:48,180 --> 00:15:50,600
So now, we've put all the numbers back
into play.

291
00:15:50,600 --> 00:15:54,020
So now, this is the most optimal for
fitting a line.

292
00:15:54,020 --> 00:15:58,460
So, the outcome variable for logistic
regression, as I've mentioned earlier is

293
00:15:58,460 --> 00:16:02,110
actually going to be the natural log of
the odds of the outcome.

294
00:16:02,110 --> 00:16:06,127
A little complicated, but it's for
mathematical reasons that we,

295
00:16:06,127 --> 00:16:08,190
we choose that.

296
00:16:08,190 --> 00:16:12,810
And just to illustrate, I took those, the
same groupings,

297
00:16:12,810 --> 00:16:17,210
ten groups and five groups, where I had
calculated the percent,

298
00:16:17,210 --> 00:16:20,320
the probability of being book smart in
each of those groups.

299
00:16:20,320 --> 00:16:22,780
And I transformed that into a log odds.

300
00:16:22,780 --> 00:16:26,100
So, I took the percentage, divided by 1
minus the percentage, and

301
00:16:26,100 --> 00:16:28,700
then took the natural log of that number.

302
00:16:28,700 --> 00:16:31,140
This gives me some numbers like 0.5, 1.0,
1.5, 2.0 the,

303
00:16:31,140 --> 00:16:33,690
the log odds has no inherent meaning to
me.

304
00:16:33,690 --> 00:16:34,606
But that's okay,

305
00:16:34,606 --> 00:16:40,310
because when we eventually will translate
it back into something meaningful.

306
00:16:40,310 --> 00:16:43,485
But now, this outcome variable is a number
that

307
00:16:43,485 --> 00:16:47,330
could theoretically go anywhere from
negative infinity to positive infinity.

308
00:16:47,330 --> 00:16:49,130
Makes a very fit, nicer fitting align.

309
00:16:49,130 --> 00:16:52,070
I'm graphing here the logit for each of
these groups,

310
00:16:52,070 --> 00:16:57,300
against again, the mean homework time in
each of those groups.

311
00:16:57,300 --> 00:17:00,890
And, you could fit a line to that, and
that's what we're doing in

312
00:17:00,890 --> 00:17:03,330
logistic regression, that's the line that
we're fitting.

313
00:17:03,330 --> 00:17:06,320
So, we're going to estimate an alpha and a
beta on intercept and

314
00:17:06,320 --> 00:17:07,960
a slope for that particular line.

315
00:17:09,210 --> 00:17:12,640
As I mentioned, the groupings here are
completely arbitrary.

316
00:17:12,640 --> 00:17:16,290
When you actually do this in a computer,
there's calculus going on.

317
00:17:16,290 --> 00:17:19,780
And so, you're going to get the best alpha
and the best beta.

318
00:17:19,780 --> 00:17:23,670
When I do this graphically, it's a little
bit of an approximation.

319
00:17:23,670 --> 00:17:26,550
You can see that the line is slightly
different when I group it

320
00:17:26,550 --> 00:17:28,490
into 10 groups than when I group it into 5
groups.

321
00:17:28,490 --> 00:17:30,610
These lines are slightly different.

322
00:17:30,610 --> 00:17:34,310
But we can actually kind of look at these
graphics and

323
00:17:34,310 --> 00:17:37,390
just kind of visually approximate the
equation of that line.

324
00:17:37,390 --> 00:17:38,840
So, I'm going to do that now.

325
00:17:39,900 --> 00:17:43,000
This is not as good as doing the calculus
in the computer, but it is,

326
00:17:43,000 --> 00:17:45,000
it will give us a little bit of an
approximation.

327
00:17:45,000 --> 00:17:46,220
And I think it's also good for

328
00:17:46,220 --> 00:17:48,270
illustrating what's actually going on
here.

329
00:17:48,270 --> 00:17:50,180
So, what is the equation of this line?

330
00:17:50,180 --> 00:17:51,180
There's two lines here,

331
00:17:51,180 --> 00:17:56,220
and we can kind of say, the, the real line
is going to be somewhere around these two.

332
00:17:56,220 --> 00:17:58,160
So, first of all, what's the intercept
value?

333
00:17:58,160 --> 00:18:00,200
So, if I just look at the graphic, my
intercept for

334
00:18:00,200 --> 00:18:05,470
this line over on the left side is I guess
about 1.6.

335
00:18:05,470 --> 00:18:07,100
You know, I'm just approximating here.

336
00:18:07,100 --> 00:18:09,810
And the intercept over for

337
00:18:09,810 --> 00:18:14,290
the line on the right hand side of the
page is maybe about 1.8.

338
00:18:14,290 --> 00:18:19,350
So, let's just kind of call it that the,
that the equation for this line, again,

339
00:18:19,350 --> 00:18:22,880
if we're kind of saying there's one line
here, that we're trying to approximate.

340
00:18:22,880 --> 00:18:24,040
Maybe it's around 1.7.

341
00:18:24,040 --> 00:18:27,380
I'm just making, it's somewhere in that
ballpark.

342
00:18:27,380 --> 00:18:30,730
Maybe around 1.7 is the intercept.

343
00:18:30,730 --> 00:18:33,620
How would I get the slopes of these lines,
just visually here?

344
00:18:33,620 --> 00:18:36,100
How could I approximate the slopes of
these lines?

345
00:18:36,100 --> 00:18:37,890
So, what's a slope, it's rise over run.

346
00:18:37,890 --> 00:18:43,540
So, I could say well, how much have I gone
up in the y variable?

347
00:18:43,540 --> 00:18:46,860
And how much have I gone up in the x
variable?

348
00:18:46,860 --> 00:18:49,850
So, I can just kind of assess the triangle
here, and

349
00:18:49,850 --> 00:18:53,860
say, well, what 's the, how, how far did I
go on, on the x axis?

350
00:18:53,860 --> 00:18:55,230
I went from about 0 to 40.

351
00:18:55,230 --> 00:18:58,690
So, the x changes about 40, here, for this
triangle.

352
00:18:58,690 --> 00:19:02,210
On the y end of things, yeah if you kind
of approximate it,

353
00:19:02,210 --> 00:19:06,765
it's maybe going from about 0.3, maybe to
1.6.

354
00:19:06,765 --> 00:19:10,771
I'm going to just call it 1.3.

355
00:19:10,771 --> 00:19:17,200
So, sorry the, you, the, we go up 1.3 in
the log odds.

356
00:19:17,200 --> 00:19:19,260
So, the slope here is going to be rise
over run.

357
00:19:19,260 --> 00:19:22,700
So, 1.3 divided by 40 or approximately
0.03.

358
00:19:22,700 --> 00:19:24,690
So, the slope here is about 0.03.

359
00:19:24,690 --> 00:19:28,730
For this line, we can use the same kind of
logic.

360
00:19:28,730 --> 00:19:31,620
So, for the x, we went over about 35 here.

361
00:19:31,620 --> 00:19:33,510
That's about zero to 35.

362
00:19:33,510 --> 00:19:38,078
On the y end, we've gone up a little bit
bigger, maybe about 1.8 from 1,

363
00:19:38,078 --> 00:19:39,820
to about 1.8.

364
00:19:39,820 --> 00:19:44,400
The slope then would be 1.8 divided by 35,
which is about 0.05.

365
00:19:44,400 --> 00:19:47,823
So, you know, this is somewhere, I'm going
approximate somewhere between 0.03 and

366
00:19:47,823 --> 00:19:49,370
0.05, let's call it 0.04.

367
00:19:49,370 --> 00:19:50,860
And of course, this is negative.

368
00:19:50,860 --> 00:19:54,560
I should emphasize here that this is a
downward slope.

369
00:19:54,560 --> 00:19:58,300
We've gone down 1.3 or down 1.8 in y.

370
00:19:58,300 --> 00:20:00,080
So, both of these slopes are negative.

371
00:20:00,080 --> 00:20:03,200
So, on average, the slopes are somewhere
around negative 0.04.

372
00:20:03,200 --> 00:20:06,250
This is just a graphical approximation,
but

373
00:20:06,250 --> 00:20:09,000
that's what we're doing on the district
regression, that's the line we're fitting.

374
00:20:09,000 --> 00:20:12,381
Now, I'm actually going to put the data in
a computer, and

375
00:20:12,381 --> 00:20:17,241
let the computer use the beautiful
calculus to estimate this perfectly,

376
00:20:17,241 --> 00:20:22,101
to get the actual best intercept, and best
beta slope that fits these data.

377
00:20:22,101 --> 00:20:25,810
And I'll show you that our approximation
wasn't bad.

378
00:20:25,810 --> 00:20:27,830
Here's what the computer comes out with
when it does,

379
00:20:27,830 --> 00:20:29,950
what's called maximum likelihood
estimation.

380
00:20:29,950 --> 00:20:31,820
This involves calculus.

381
00:20:31,820 --> 00:20:33,880
It gives us some maximum likelihood
estimates.

382
00:20:33,880 --> 00:20:36,660
I'm not going to go into the details of
maximum likelihood estimation.

383
00:20:36,660 --> 00:20:40,550
That's for another course, but this is
just the way of using some calculus that

384
00:20:40,550 --> 00:20:45,070
we're getting the best alpha and the best
beta here, the best fit line.

385
00:20:45,070 --> 00:20:48,321
So, when I do this on the computer, the
intercept comes out to be just over two.

386
00:20:48,321 --> 00:20:51,924
So, just slightly different than what we
did, got in our approximation.

387
00:20:51,924 --> 00:20:56,560
The slope came out to be right around
negative negative 0.04, negative 0.039.

388
00:20:56,560 --> 00:20:58,122
So, and where did I get those in the
output?

389
00:20:58,122 --> 00:21:00,114
Here's the estimate for the intercept.

390
00:21:00,114 --> 00:21:01,530
That's the alpha estimate.

391
00:21:01,530 --> 00:21:03,110
Here's the estimate for the beta.

392
00:21:03,110 --> 00:21:05,890
That's the estimate for the slope.

393
00:21:05,890 --> 00:21:10,230
Along with the alpha and beta, we also get
a p value to tell us whether that alpha or

394
00:21:10,230 --> 00:21:12,851
beta is statistically different than zero.

395
00:21:12,851 --> 00:21:17,410
A beta of zero would mean no association,
same as it did with linear regression.

396
00:21:17,410 --> 00:21:21,500
So, the intercept is clearly significantly
different than zero.

397
00:21:21,500 --> 00:21:24,580
Again that's not all that meaningful here
because we wouldn't

398
00:21:24,580 --> 00:21:26,328
expect the intercept to be zero.

399
00:21:26,328 --> 00:21:31,150
however, the, the p value for homework is
not statistically significant.

400
00:21:31,150 --> 00:21:32,020
So, as you might have guessed,

401
00:21:32,020 --> 00:21:35,170
just looking at the picture, there isn't a
strong association here.

402
00:21:35,170 --> 00:21:37,890
So, it's not statistically different than
zero.

403
00:21:37,890 --> 00:21:42,040
Probably meaning, it doesn't look like
there's a lot of evidence that homework

404
00:21:42,040 --> 00:21:46,252
and whether or not you consider yourself
book, yourself book smart or related.

405
00:21:46,252 --> 00:21:48,023
But here's the final model.

406
00:21:48,023 --> 00:21:52,181
The log odds of your chance, the log odds
of your being book smart, or

407
00:21:52,181 --> 00:21:54,220
considering yourself book smart,

408
00:21:54,220 --> 00:21:58,580
is equal to the intercept 2.12 minus the
slope point 0.039.

409
00:21:58,580 --> 00:22:02,440
And we will of course, multiply that beta
times the amount of homework.

410
00:22:02,440 --> 00:22:04,470
So, this is a continuous predictor.

411
00:22:04,470 --> 00:22:07,600
So, we would be multiplying the amount of
homework for

412
00:22:07,600 --> 00:22:12,140
a particular person times that slope.

413
00:22:12,140 --> 00:22:14,480
What's this logistic regression equation
useful for?

414
00:22:14,480 --> 00:22:17,965
So, just like when we talked about linear
regression, there was many,

415
00:22:17,965 --> 00:22:20,540
a couple different potential functions
here.

416
00:22:20,540 --> 00:22:24,550
So, one thing is we could use the logistic
regression equation model to try to

417
00:22:24,550 --> 00:22:26,360
do some kind of prediction.

418
00:22:26,360 --> 00:22:29,155
This is very useful in clinical
applications because often we

419
00:22:29,155 --> 00:22:33,565
want to classify people as having a
condition or not having a condition.

420
00:22:33,565 --> 00:22:35,090
That's a binary outcome.

421
00:22:35,090 --> 00:22:37,680
So, if we're doing that kind of
prediction,

422
00:22:37,680 --> 00:22:40,260
we would likely be using logistic
regression.

423
00:22:40,260 --> 00:22:44,280
So, we can try to come up with set of

424
00:22:44,280 --> 00:22:50,030
predictors that predicts who's going to
have a disease or not have a disease.

425
00:22:50,030 --> 00:22:52,616
So, that's one of the functions of
logistic regression, prediction.

426
00:22:52,616 --> 00:22:58,110
A second function of logistic regression
which is probably more,

427
00:22:58,110 --> 00:23:03,280
used even more often, is just to try to
say is a particular predictor,

428
00:23:03,280 --> 00:23:05,751
like homework, related to the outcome.

429
00:23:05,751 --> 00:23:08,407
In this case, book smart or street smart.

430
00:23:08,407 --> 00:23:10,399
Are those variables related?

431
00:23:10,399 --> 00:23:12,889
Is a treatment related to an outcome?

432
00:23:12,889 --> 00:23:15,877
And that's very useful in an, in an, of
itself.

433
00:23:15,877 --> 00:23:18,699
And of course, we can say, is that
predictor,

434
00:23:18,699 --> 00:23:23,870
that treatment, related to the outcome
after adjusting for potential confounder.

435
00:23:23,870 --> 00:23:27,800
So, that's another very important use of
the logistic regression model.

436
00:23:27,800 --> 00:23:32,030
I'm going to start by talking just briefly
about the first of

437
00:23:32,030 --> 00:23:34,622
these aims which is the prediction end of
things.

438
00:23:34,622 --> 00:23:38,098
Again, here's the logistic regression
model.

439
00:23:38,098 --> 00:23:41,694
We've modeled the outcome variable, is the
log odds of the outcome.

440
00:23:41,694 --> 00:23:43,920
The, we're modeling that as a line.

441
00:23:43,920 --> 00:23:48,325
Let me take that model that we just fit,

442
00:23:48,325 --> 00:23:54,942
and how would we actually apply it to make
some predictions.

443
00:23:54,942 --> 00:24:01,767
So, if you're somebody who

444
00:24:01,767 --> 00:24:08,282
does no homework per week,

445
00:24:08,282 --> 00:24:11,695
zero hours of

446
00:24:11,695 --> 00:24:17,474
homework per week, you could plug in that
value for homework of zero into the model.

447
00:24:17,474 --> 00:24:18,124
You get that your predicted logit is 2.12.
As I mentioned,

448
00:24:18,124 --> 00:24:18,787
most of us don't think in logits.
So, in a minute we're going to

449
00:24:18,787 --> 00:24:19,463
do something with that number to make it
more interpretable.

450
00:24:19,463 --> 00:24:19,983
But if you did 10 hours per week of
homework, you

451
00:24:19,983 --> 00:24:20,594
could plug in 10, the predicted logit
there is 1.73.

452
00:24:20,594 --> 00:24:24,050
If you did 50 hours per week, the
predicted logit is 0.17.

453
00:24:24,050 --> 00:24:27,427
So, the more homework you do, the more the
logit goes down.

454
00:24:27,427 --> 00:24:30,500
Well, I don't want to deal with a logit
because a logit, I,

455
00:24:30,500 --> 00:24:31,910
I can't think in logit.

456
00:24:31,910 --> 00:24:34,720
However, we can get this notice the hot.

457
00:24:34,720 --> 00:24:36,030
There's a probability in there.

458
00:24:36,030 --> 00:24:39,450
What we want to get out is the predicted
probability of the outcome,

459
00:24:39,450 --> 00:24:43,510
the predicted probability of considering
yourself book smart in this case.

460
00:24:43,510 --> 00:24:46,940
I want to isolate this predicted
probability.

461
00:24:46,940 --> 00:24:48,470
Well, it's actually not hard to do, right?

462
00:24:48,470 --> 00:24:52,250
We've got one equation, one unknown, we
can isolate that probability.

463
00:24:52,250 --> 00:24:54,550
So, how do we do it.

464
00:24:54,550 --> 00:24:56,138
First thing that we're going to want to do
is we're

465
00:24:56,138 --> 00:24:59,320
going to want to exponentiate both sides.

466
00:24:59,320 --> 00:25:03,340
So, for example, the zero hours per week,
that person would predicted to have

467
00:25:03,340 --> 00:25:08,990
a logit of 2.12, I can then exponentiate
both sides of those, of that equation.

468
00:25:08,990 --> 00:25:13,570
So, that would tell me that the odds for
somebody who does zero

469
00:25:13,570 --> 00:25:19,570
homework per week is the exponential
raised to 2.12.

470
00:25:19,570 --> 00:25:23,320
That value is 8.33, if you actually
calculated it out.

471
00:25:23,320 --> 00:25:31,862
So, e raised to 2.2 is 8.33.

472
00:25:31,862 --> 00:25:36,472
That's the odds for a person who does,

473
00:25:36,472 --> 00:25:38,716
the odds of being book smart, for a person
who does zero hours per week of homework.

474
00:25:38,716 --> 00:25:40,696
Then, we could exponentiate this 1.73,
that would give me the odds for

475
00:25:40,696 --> 00:25:42,181
somebody who does ten hours per week,
that's 5.64.

476
00:25:42,181 --> 00:25:43,864
We could exponentiate the 0.17, that gives
me the odds for

477
00:25:43,864 --> 00:25:45,646
someone who does 50 hours per week, that
comes out to be 1.19.

478
00:25:45,646 --> 00:25:47,050
So, your odds of being book smart,

479
00:25:47,050 --> 00:25:50,265
again, goes down as you do more homework
per week.

480
00:25:50,265 --> 00:25:52,549
So, the opposite of what you might expect.

481
00:25:52,549 --> 00:25:55,020
But of course, this is not statistically
significant.
