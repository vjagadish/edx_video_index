1
00:00:06,080 --> 00:00:09,260
Now we're going to apply maximum
likelihood estimation to

2
00:00:09,260 --> 00:00:11,900
the specific case of logistic regression.

3
00:00:11,900 --> 00:00:14,480
So again, what's our logistic regression
model look like?

4
00:00:14,480 --> 00:00:16,160
Let's write that out to start.

5
00:00:16,160 --> 00:00:23,310
So we have this logit function is equal to
alpha plus beta and

6
00:00:23,310 --> 00:00:27,730
times x, and there might be more than one
beta in the model.

7
00:00:27,730 --> 00:00:31,540
For simplicity I'm just going to stick to
one beta for the moment, but

8
00:00:31,540 --> 00:00:33,920
just realize that there may be more than
one beta in the model.

9
00:00:33,920 --> 00:00:37,200
What we're going to be trying to do now is
to

10
00:00:37,200 --> 00:00:42,329
write out what is the likelihood function
for a logistic regression.

11
00:00:43,720 --> 00:00:48,030
As a function of the unknowns that we want
to estimate, alpha and beta in this case.

12
00:00:48,030 --> 00:00:53,100
So what is the likelihood function going
to be for logistic regression?

13
00:00:53,100 --> 00:00:56,410
So it may help to put a simple example
here.

14
00:00:56,410 --> 00:01:00,000
So hearkening back to the example I talked
about earlier,

15
00:01:00,000 --> 00:01:04,209
where our outcome was a doctor being
disciplined by their state medical board.

16
00:01:05,320 --> 00:01:09,840
I'm just going to give you two
individuals, the likelihood function by

17
00:01:09,840 --> 00:01:12,420
the way for logistic regression is going
to have a term for

18
00:01:12,420 --> 00:01:16,810
every person in our study, so I'm going to
start with two individuals as examples.

19
00:01:16,810 --> 00:01:20,440
So imagine we've got person one from our
data set.

20
00:01:20,440 --> 00:01:24,550
This person is somebody who indeed was
disciplined.

21
00:01:24,550 --> 00:01:28,950
So the outcome of discipline by the
medical board,

22
00:01:28,950 --> 00:01:31,220
that value is equal to one for this
person.

23
00:01:31,220 --> 00:01:32,890
So they actually have the outcome.

24
00:01:32,890 --> 00:01:34,450
They are a case in this case.

25
00:01:35,450 --> 00:01:39,050
And let's say that our predictor, let's
use a simple predictor here, so

26
00:01:39,050 --> 00:01:41,190
let's say we wanted to know whether or

27
00:01:41,190 --> 00:01:46,230
not GPA as an undergraduate predicts
discipline by the medical board later on.

28
00:01:46,230 --> 00:01:49,090
And so let's say their GPA, that person's
GPA happen to be,

29
00:01:49,090 --> 00:01:51,340
we'll make it really abysmal, 1.0.

30
00:01:51,340 --> 00:01:52,450
Make it really bad.

31
00:01:53,460 --> 00:01:55,135
Here's the second person who's in our data
set.

32
00:01:57,840 --> 00:01:59,710
That person was not disciplined.

33
00:01:59,710 --> 00:02:02,330
So they are control, not a case.

34
00:02:02,330 --> 00:02:05,850
They have a value of 0 for that variable
discipline.

35
00:02:05,850 --> 00:02:07,110
The outcome variable.

36
00:02:07,110 --> 00:02:10,230
And, let's say they had a great GPA, we'll
give them, you know, I don't know if

37
00:02:10,230 --> 00:02:14,140
you can get over 4.0, we'll give them 4.0,
saying that's the highest you can get.

38
00:02:15,180 --> 00:02:17,810
Alright, so those are just two examples
that I can have some numbers to

39
00:02:17,810 --> 00:02:19,230
plug in here.

40
00:02:19,230 --> 00:02:21,390
So now, imagine we fit a logistic
regression model.

41
00:02:21,390 --> 00:02:23,150
So we fit a logistic regression model.

42
00:02:23,150 --> 00:02:26,610
Our outcome is this disciplinary action by
the medical, state medical board.

43
00:02:26,610 --> 00:02:29,590
And we're going to have some alpha that
we're trying to solve for

44
00:02:29,590 --> 00:02:31,960
plus some beta that relates to GPA.

45
00:02:33,090 --> 00:02:38,020
If I want to know, if I was to write a, an
equation for

46
00:02:38,020 --> 00:02:40,599
the likelihood here, just for these two
individuals.

47
00:02:42,140 --> 00:02:44,620
So again, the likelihood is going to be a
function of alpha and

48
00:02:44,620 --> 00:02:46,900
beta, because those are the unknowns we
have to solve for later.

49
00:02:46,900 --> 00:02:47,980
So what's the likelihood function?

50
00:02:47,980 --> 00:02:51,690
If, if for this particular logistical
regression model, for these two people.

51
00:02:51,690 --> 00:02:56,770
So the first person here actually got the
disease, or

52
00:02:56,770 --> 00:02:59,440
had the outcome, they got disciplined.

53
00:02:59,440 --> 00:03:01,440
So, that's what we saw in our data.

54
00:03:01,440 --> 00:03:03,890
This is a person who was disciplined.

55
00:03:03,890 --> 00:03:06,640
Therefore, what they're going to get in
the likelihood model

56
00:03:06,640 --> 00:03:11,390
is the probability that, that particular
person would actually be disciplined.

57
00:03:11,390 --> 00:03:13,610
That they would have a value of 1.

58
00:03:13,610 --> 00:03:16,930
The second person did not get disciplined.

59
00:03:16,930 --> 00:03:19,630
So we're going to plug into the likelyhood
the probability that they

60
00:03:19,630 --> 00:03:20,845
wouldn't have gotten disciplined.

61
00:03:20,845 --> 00:03:22,510
because that's what we saw, they were not
disciplined.

62
00:03:22,510 --> 00:03:27,120
And then we're going to plug in a new term
for every person in our data set.

63
00:03:27,120 --> 00:03:29,240
But I'm just going to focus on two to keep
it simple here.

64
00:03:30,330 --> 00:03:33,690
Now, what is that actual, what is that
probability?

65
00:03:33,690 --> 00:03:37,260
Well that probability, remember what we
get out of a logistic regression model is

66
00:03:37,260 --> 00:03:39,920
we can calculate the predicted probability
for

67
00:03:39,920 --> 00:03:44,120
a person as a function of their predictors
such as GPA in this case.

68
00:03:44,120 --> 00:03:47,720
And we showed earlier that the probability
from a logistic regression model,

69
00:03:47,720 --> 00:03:52,240
the probability of having the outcome, is
equal to e raised to alpha plus beta,

70
00:03:52,240 --> 00:03:53,010
whatever the betas are.

71
00:03:53,010 --> 00:03:54,200
In this case, it's beta GPA.

72
00:03:54,200 --> 00:03:58,380
And you're going to plug in the person's
actual value for GPA and

73
00:03:58,380 --> 00:04:03,950
then 1 plus e alpha plus beta in the
denominator.

74
00:04:03,950 --> 00:04:07,480
I showed you in an earlier module that
that gives you the predicted probability

75
00:04:07,480 --> 00:04:08,260
for person i.

76
00:04:09,870 --> 00:04:11,720
That's the probability of having the
event.

77
00:04:11,720 --> 00:04:14,710
You can also calculate for a person the
probability of not having the event.

78
00:04:14,710 --> 00:04:16,285
We're going to need that for the controls.

79
00:04:16,285 --> 00:04:18,460
That's just going to be 1 minus what you
see here.

80
00:04:18,460 --> 00:04:24,220
Well 1 minus all of that, will just give
me, 1 over 1 plus e

81
00:04:24,220 --> 00:04:30,670
raise to the alpha plus beta times the xi,
for the person who is not disciplined.

82
00:04:30,670 --> 00:04:33,225
So, this is how we calculate the
predictive probability and

83
00:04:33,225 --> 00:04:34,850
1 minus that predictive probability.

84
00:04:34,850 --> 00:04:39,510
Well now we're going to do that just
plugging in the actual data from this

85
00:04:39,510 --> 00:04:40,510
particular dataset.

86
00:04:40,510 --> 00:04:45,066
So the person who, got disciplined, their
probability of being disciplined as

87
00:04:45,066 --> 00:04:48,215
a function of alpha, the unknown alphas
and betas, and

88
00:04:48,215 --> 00:04:51,900
their actual GPA is just going to equal e
raised to alpha plus beta GPA

89
00:04:51,900 --> 00:04:56,970
times in this case they have a GPA of 1.0,
so we're just going to plug that GPA in.

90
00:04:57,980 --> 00:05:00,349
And the denominator is going to be 1 plus
that same term.

91
00:05:04,020 --> 00:05:06,540
So that's the probability for the first
person.

92
00:05:06,540 --> 00:05:10,350
The probability for the second person, the
probability that that, the second

93
00:05:10,350 --> 00:05:13,210
person would not have been disciplined,
it's going to be one minus that, so

94
00:05:13,210 --> 00:05:19,140
it's going to be 1 over 1 plus e raised to
alpha plus the beta GPA.

95
00:05:19,140 --> 00:05:22,660
And that person had a value of GPA of 4.0.

96
00:05:22,660 --> 00:05:25,640
And then you're going to continue on this
likelihood function.

97
00:05:25,640 --> 00:05:28,075
You can see this likelihood function is
going to become rather complex.

98
00:05:28,075 --> 00:05:31,310
You're going to continue that on for all
the other, one term for

99
00:05:31,310 --> 00:05:33,400
every person in the study.

100
00:05:33,400 --> 00:05:36,890
That's the formula of likelihood function
for an unconditional logistic regression.

101
00:05:37,980 --> 00:05:39,410
Alright, so what does this do?

102
00:05:39,410 --> 00:05:42,820
This is going to be a, you can see quite a
complicated likelihood function although

103
00:05:42,820 --> 00:05:44,360
I'm going to use the simple example and

104
00:05:44,360 --> 00:05:47,620
actually maximizes simple example later in
this Module.

105
00:05:47,620 --> 00:05:50,870
But what, what conceptually are we doing?

106
00:05:50,870 --> 00:05:55,000
We are saying, I want to find, I'm
going to then maximize this long, long

107
00:05:55,000 --> 00:06:00,990
function to find the values of alpha and
beta that make my data the most likely.

108
00:06:00,990 --> 00:06:02,830
So what's that going to look like?

109
00:06:02,830 --> 00:06:06,330
Well, let's say that GPA is indeed related
to discipline.

110
00:06:06,330 --> 00:06:09,680
So we get a lot of people who are
disciplined having low GPAs, and

111
00:06:09,680 --> 00:06:13,590
a lot of people who are controls, who
weren't disciplined having high GPAs.

112
00:06:13,590 --> 00:06:16,748
So what that's going to mean is that we're
going to end up finding out

113
00:06:16,748 --> 00:06:22,490
that beta all the cases where the beta
appears in the numerator.

114
00:06:22,490 --> 00:06:25,890
Those are going to be the cases where the
GPAs are low, and where it's only in

115
00:06:25,890 --> 00:06:28,690
the denominator, it's going to be the
cases where the GPA is high.

116
00:06:28,690 --> 00:06:31,600
So mathematically, when we go to maximize
that function,

117
00:06:31,600 --> 00:06:37,560
it's going to turn out that we're going to
see a connection between GPA and

118
00:06:37,560 --> 00:06:39,220
who got disciplined and who didn't.

119
00:06:39,220 --> 00:06:41,590
So we're going to find a beta that
connects those two.

120
00:06:41,590 --> 00:06:45,300
So again, we're going to take this
complicated likelihood function,

121
00:06:45,300 --> 00:06:47,150
we're going to maximize it to solve for
alpha and

122
00:06:47,150 --> 00:06:52,570
beta, and that's going to define the
relationship between GPA and the outcome.

123
00:06:52,570 --> 00:06:54,850
Just written out in a slightly different
way.

124
00:06:54,850 --> 00:06:57,990
We're going to be multiplying for all the
cases everybody who had

125
00:06:57,990 --> 00:07:00,890
the outcome the probability, the predicted
probability that they would have a,

126
00:07:00,890 --> 00:07:04,330
an out the outcome using the logistic
regression model.

127
00:07:04,330 --> 00:07:05,130
And for all the controls,

128
00:07:05,130 --> 00:07:07,450
all the people who didn't have the outcome
we'll be multiplying their probability of

129
00:07:07,450 --> 00:07:09,610
not having the outcome because that's
actually what we saw.

130
00:07:09,610 --> 00:07:11,420
We saw exactly these people have the
outcome and

131
00:07:11,420 --> 00:07:13,520
exactly these other people did not.

132
00:07:13,520 --> 00:07:15,360
Now that's one of many possible outcomes.

133
00:07:15,360 --> 00:07:18,520
Imagine you have 100 people in your data
set.

134
00:07:18,520 --> 00:07:22,670
These particular 50, where the cases in
these particular 50 were the controls.

135
00:07:22,670 --> 00:07:24,350
That's one possible outcome.

136
00:07:24,350 --> 00:07:25,350
That's the outcome we saw.

137
00:07:25,350 --> 00:07:27,040
But imagine all the other possible
outcomes.

138
00:07:27,040 --> 00:07:30,630
The person with the GPA of 1, could have
actually not been disciplined.

139
00:07:30,630 --> 00:07:32,630
And so on.
You can imagine that there's a huge,

140
00:07:32,630 --> 00:07:34,630
huge number of possible outcomes here.

141
00:07:34,630 --> 00:07:36,340
We saw one particular outcome.

142
00:07:36,340 --> 00:07:39,640
The likelihood function defines the
probability of that outcome.

143
00:07:39,640 --> 00:07:42,300
So again, just kind of plugging in here
for

144
00:07:42,300 --> 00:07:46,280
all the cases we have some logistic
regression model, the probability from

145
00:07:46,280 --> 00:07:49,150
the logistic regression model of being the
case is that.

146
00:07:49,150 --> 00:07:51,360
The probability of being a control is that
and we'll try and

147
00:07:51,360 --> 00:07:53,470
solve for these betas and alphas.

148
00:07:53,470 --> 00:07:54,800
And of course we're going to be plugging
in for

149
00:07:54,800 --> 00:07:58,300
every person their individual values of
these x's.

150
00:07:58,300 --> 00:08:02,540
You can see, it would be really tedious to
maximize this function by hand, but

151
00:08:02,540 --> 00:08:06,480
there's ways that the computer has of some
iterative methods that will

152
00:08:06,480 --> 00:08:10,090
actually essentially do this maximum
likelihood problem.

153
00:08:10,090 --> 00:08:10,700
What are we going to do?

154
00:08:10,700 --> 00:08:12,550
Well, it's just like we did with the coin
problem.

155
00:08:12,550 --> 00:08:14,440
We're going to take the log of the
likelihood function so

156
00:08:14,440 --> 00:08:17,080
that the product becomes a sum, because
we've got this huge product,

157
00:08:17,080 --> 00:08:19,920
we need to make it a sum to make it easier
to take derivatives.

158
00:08:19,920 --> 00:08:22,810
We're going to take the derivative of that
log likelihood function,

159
00:08:22,810 --> 00:08:25,200
we're going to set that derivative equal
to zero, and we're going to solve for

160
00:08:25,200 --> 00:08:26,090
the alphas and betas.

161
00:08:26,090 --> 00:08:29,530
So in the exact same way we did with the
coin problem, there is a way to get,

162
00:08:29,530 --> 00:08:33,460
from that likelihood function, to get the
to maximize it to

163
00:08:33,460 --> 00:08:37,260
get the most likely alpha and, alphas and
betas given our data.

164
00:08:37,260 --> 00:08:39,540
Now, what actually goes on behind the
scenes in the computer,

165
00:08:39,540 --> 00:08:41,900
it doesn't actually really take the
derivatives.

166
00:08:41,900 --> 00:08:45,340
It uses an algorithm that kind of
approximates that.

167
00:08:45,340 --> 00:08:49,120
But that turns out to be very good and
it's computationally feasible.

168
00:08:49,120 --> 00:08:51,950
But I'm going to actually walk you through
doing out the,

169
00:08:51,950 --> 00:08:56,460
the calculus for a very simple logistic
regression example.

170
00:08:56,460 --> 00:08:59,080
We're going to take the example where we
have just a 2 by 2 table,

171
00:08:59,080 --> 00:09:02,560
binary outcome, binary predictor, and
that's it.

172
00:09:02,560 --> 00:09:07,440
We're going to translate this into the
logistic regression picture.

173
00:09:07,440 --> 00:09:12,950
So, if we have just a 2 by 2 table, what
goes in cell A?

174
00:09:12,950 --> 00:09:17,700
So, cell A is all the people who did have
the outcome and were exposed.

175
00:09:17,700 --> 00:09:19,840
And we have in my logistic regression
model here,

176
00:09:19,840 --> 00:09:21,710
remember, it's going to be something very
simple.

177
00:09:21,710 --> 00:09:25,590
It's just going to have an alpha and it's
going to have a beta.

178
00:09:25,590 --> 00:09:27,270
And that beta's for a binary variable.

179
00:09:27,270 --> 00:09:31,540
So, X is going to be equal to either one
or X is equal to 0 and that's it.

180
00:09:31,540 --> 00:09:33,020
So, it's a very simple model.

181
00:09:33,020 --> 00:09:33,570
Call this beta 1.

182
00:09:34,630 --> 00:09:38,630
So if you have the exposure, you're
going to be multiplying beta 1 times 1,

183
00:09:38,630 --> 00:09:41,440
and so you're, you will have beta 1 in
your equation.

184
00:09:41,440 --> 00:09:46,226
If you have the disease, you get e over 1
plus e rather than 1 over 1 + e,

185
00:09:46,226 --> 00:09:46,850
so it gives you again,

186
00:09:46,850 --> 00:09:52,060
the probability of having the disease is
equal to this e over 1 plus e.

187
00:09:52,060 --> 00:09:56,221
The probability of not having the disease
is equal to 1 minus that,

188
00:09:56,221 --> 00:09:59,640
which is just turns out to be 1 over 1
plus [INAUDIBLE].

189
00:09:59,640 --> 00:10:03,660
So if you are not exposed and you have the
disease, what happens?

190
00:10:03,660 --> 00:10:06,280
Well your predicted probability of having
the disease based on

191
00:10:06,280 --> 00:10:09,050
the logistic regression model is your,
you, your x value is 0.

192
00:10:09,050 --> 00:10:13,100
You do not have the exposure, so the beta
term goes away, so your probability of

193
00:10:13,100 --> 00:10:17,820
having the disease is just e raised alpha
over 1 plus e raised to alpha.

194
00:10:17,820 --> 00:10:21,430
What about somebody who doesn't get the
disease, but was exposed?

195
00:10:21,430 --> 00:10:26,350
So that person's logistic regression
equation will be alpha plus beta times 1,

196
00:10:26,350 --> 00:10:28,840
so the beta will remain in, because
they're exposed.

197
00:10:28,840 --> 00:10:30,440
However, they did not get the disease.

198
00:10:30,440 --> 00:10:32,580
They're a control, so we calculate for

199
00:10:32,580 --> 00:10:35,250
them the probability of not getting the
disease, because that's what we saw.

200
00:10:35,250 --> 00:10:37,110
So that's this 1 over 1 plus e.

201
00:10:38,630 --> 00:10:40,980
And then for somebody who has neither the
exposure, nor

202
00:10:40,980 --> 00:10:45,700
the disease, the beta drops out, so it's
just e over, it's e, e raised to alpha.

203
00:10:45,700 --> 00:10:48,970
And they didn't get the disease, so it's
just 1 over 1 plus e.

204
00:10:48,970 --> 00:10:54,010
So those are the probabilities for what we
saw in this experiment for

205
00:10:54,010 --> 00:10:57,340
every person who either, have the exposure
or it, or you don't.

206
00:10:57,340 --> 00:11:00,970
You either have the disease or you don't
and that translates to different terms,

207
00:11:00,970 --> 00:11:04,150
these four different terms that will go in
the likelihood function.

208
00:11:04,150 --> 00:11:08,160
And just to show you if I actually
multiply in this case a times d divided by

209
00:11:08,160 --> 00:11:12,270
b times c, when I do that, you notice a
lot of things cancel here.

210
00:11:13,420 --> 00:11:14,380
Cancel.

211
00:11:14,380 --> 00:11:15,510
And we end up with.

212
00:11:19,340 --> 00:11:22,500
This also cancels, we end up with just e
raised to beta.

213
00:11:22,500 --> 00:11:25,745
Well, guess what, the odds ratio is a
times d divided by b times c, and

214
00:11:25,745 --> 00:11:27,220
it's also e raised to beta.

215
00:11:27,220 --> 00:11:30,520
So that assures you that it comes out as
we would expect.

216
00:11:30,520 --> 00:11:32,210
So now, just to plug in some real numbers
so

217
00:11:32,210 --> 00:11:35,390
we can actually do a maximum likelihood
estimation problem here.

218
00:11:35,390 --> 00:11:36,880
So let's, simple example.

219
00:11:36,880 --> 00:11:38,490
You are either older or you are younger.

220
00:11:38,490 --> 00:11:40,900
Are you more likely to have heart disease
or not?

221
00:11:40,900 --> 00:11:42,070
Very simple example.

222
00:11:42,070 --> 00:11:43,939
Some numbers thrown in this example.

223
00:11:45,090 --> 00:11:49,950
There were 21 people who were over 55 who
also had cardiovascular disease.

224
00:11:49,950 --> 00:11:53,190
22 people younger than 55 who did and so
on.

225
00:11:53,190 --> 00:11:55,840
So we're going to take these data and

226
00:11:55,840 --> 00:11:57,570
we're going to put them into the
likelihood function.

227
00:11:57,570 --> 00:12:00,340
So what's the likelihood function going to
look like here?

228
00:12:00,340 --> 00:12:02,370
So this is again the like, the model here,

229
00:12:02,370 --> 00:12:03,720
the logistic regression model is really
simple.

230
00:12:03,720 --> 00:12:05,790
It's just alpha plus beta 1 times x 1.

231
00:12:05,790 --> 00:12:08,250
X 1 is either 1 or 0, it's a binary
predictor.

232
00:12:08,250 --> 00:12:11,658
So you're only going to have beta 1 in the
model if you're older, if you are exposed,

233
00:12:11,658 --> 00:12:14,346
you're not going to have beta in the model
if you're unexposed because it

234
00:12:14,346 --> 00:12:15,730
will just be multiplied by 0.

235
00:12:15,730 --> 00:12:20,860
So going, using those four terms that I
already showed you for the predicted

236
00:12:20,860 --> 00:12:25,970
probabilities for each of the different
types of people in my 2 by 2 table, so

237
00:12:25,970 --> 00:12:29,000
this was the predicted probability for
those who got the disease and

238
00:12:29,000 --> 00:12:31,910
had the exposure this is the probability
of being a control for

239
00:12:31,910 --> 00:12:34,780
those who were controls but had the
exposure and so on.

240
00:12:34,780 --> 00:12:40,210
Well every person, now in that cell a, for
example, is going to get that same

241
00:12:40,210 --> 00:12:43,810
likelihood because this is a very simple
example and everybody shares exposures.

242
00:12:43,810 --> 00:12:45,690
There's only one binary exposure.

243
00:12:45,690 --> 00:12:47,980
There were 21 people who get that term in
the likelihood.

244
00:12:47,980 --> 00:12:50,000
We're going to multiply that 21 times.

245
00:12:50,000 --> 00:12:53,490
There were six people who were exposed but
did not have the disease.

246
00:12:53,490 --> 00:12:57,010
And they are each going to get this term,
we are going to multiple that 6 times.

247
00:12:57,010 --> 00:13:00,260
There were 22 people who get this term and
there were 51 who

248
00:13:00,260 --> 00:13:03,530
have neither the exposure nor the disease
and they're going to get this term.

249
00:13:03,530 --> 00:13:07,650
So even though the likelihood function for
a big study like this is, you know,

250
00:13:07,650 --> 00:13:11,370
there's lots of people that we have to
multiply one term in the likelihood for

251
00:13:11,370 --> 00:13:11,940
each person.

252
00:13:11,940 --> 00:13:15,450
There's a lot of people that look the same
for a simple 2 by 2 example, so

253
00:13:15,450 --> 00:13:17,020
we can simplify it.

254
00:13:17,020 --> 00:13:19,470
This represents a, a very complicated
function,

255
00:13:19,470 --> 00:13:23,930
however, because we have a lot of things
multiplied together.

256
00:13:23,930 --> 00:13:27,840
But in fact, for this very simple case, we
can maximize this function by hand, so

257
00:13:27,840 --> 00:13:29,630
we're going to do that now.

258
00:13:29,630 --> 00:13:32,450
So that's the likelihood function, this is
the probability of our data of seeing

259
00:13:32,450 --> 00:13:37,430
exactly 21 in cell A, and exactly 6 in
cell C etcetera, as a function of

260
00:13:37,430 --> 00:13:40,270
our unknowns alpha and beta, so now we're
going to try to solve for alpha and beta.

261
00:13:41,950 --> 00:13:46,190
So as I mentioned before, you want to
first take a log, because that

262
00:13:46,190 --> 00:13:50,360
will help to linearize the function and
make it easier take, to take derivatives.

263
00:13:50,360 --> 00:13:54,530
So I've gone ahead and taken a log here.

264
00:13:54,530 --> 00:13:57,930
So, just some rules to remind you about
how logs work,

265
00:13:57,930 --> 00:14:01,530
I've put those little rules in yellow up
in on the top in case you've forgotten.

266
00:14:01,530 --> 00:14:03,410
But if I'm going to take the log of the
likelihood function,

267
00:14:03,410 --> 00:14:06,500
I've already done it out here just to save
time just,

268
00:14:06,500 --> 00:14:08,150
I just want you to follow what I've done.

269
00:14:08,150 --> 00:14:09,950
So I'm taking the log of this function.

270
00:14:09,950 --> 00:14:15,450
So remember that with if you take the log,
the powers come down in front, and

271
00:14:15,450 --> 00:14:18,060
things that are multiplied together become
added together.

272
00:14:18,060 --> 00:14:22,600
Things that are in the denominator when
you take a log become subtracted.

273
00:14:22,600 --> 00:14:25,210
So hopefully you can follow the logic
here.

274
00:14:25,210 --> 00:14:25,900
I'm not going to go through,

275
00:14:25,900 --> 00:14:29,600
just to save time here, of how to take the
log of this com, complicated function.

276
00:14:29,600 --> 00:14:32,670
I've done it out here again for this first
one.

277
00:14:32,670 --> 00:14:35,640
You take the log, the 21 comes in front
because that's a power.

278
00:14:35,640 --> 00:14:39,350
You'll be adding the log of e raised to
alpha plus beta,

279
00:14:39,350 --> 00:14:43,410
I guess I'll just do the first one out
here, so, you get the idea.

280
00:14:47,360 --> 00:14:50,820
That was raised to 21, when we take the
log, the 21 comes down in front.

281
00:14:53,150 --> 00:14:57,140
We end up with the natural log of e raised
to alpha plus beta, but

282
00:14:57,140 --> 00:15:02,460
remember that the natural log in e are the
inverses, so

283
00:15:02,460 --> 00:15:06,520
in fact those cancel and we just end up
with 21 times alpha plus beta.

284
00:15:06,520 --> 00:15:11,650
For the denominator we end up with the
negative, the 21 remains in front,

285
00:15:11,650 --> 00:15:17,230
21 and then we just end up with the log of
1 plus e alpha plus beta.

286
00:15:17,230 --> 00:15:20,460
There is no way to simplify that because
the log is applied across the 1, e and

287
00:15:20,460 --> 00:15:22,150
d, and so on and so forth.

288
00:15:22,150 --> 00:15:23,040
So you take the log here.

289
00:15:23,040 --> 00:15:25,540
You did this whole function out and you
take the log.

290
00:15:25,540 --> 00:15:28,320
For fun, if you want to do it on your own
feel free to pause the video and

291
00:15:28,320 --> 00:15:29,550
try to, to do that out on your own.

292
00:15:29,550 --> 00:15:30,990
Make sure you get the same answer as me.

293
00:15:32,400 --> 00:15:33,990
All right, so here's the log likelihood
function.

294
00:15:33,990 --> 00:15:38,690
Next thing we have to do to, to find the
maximum point is to take the derivative.

295
00:15:38,690 --> 00:15:41,710
So again just to kind of save time here
I've gone through and

296
00:15:41,710 --> 00:15:43,990
done out the derivatives already.

297
00:15:43,990 --> 00:15:46,850
For a fun challenge for those of you who
are interested you could

298
00:15:46,850 --> 00:15:49,020
pause the video and try to take the
derivatives on your own.

299
00:15:49,020 --> 00:15:52,000
But I'll just walk you through the general
logic here.

300
00:15:52,000 --> 00:15:55,560
So there's two unknowns so we have to take
the derivative with respect to

301
00:15:55,560 --> 00:15:59,180
alpha separately from the derivative with
respect to beta.

302
00:15:59,180 --> 00:16:01,590
Now where did the various things come from
in the derivative?

303
00:16:01,590 --> 00:16:04,980
So, let me talk about the derivative with
respect to beta first.

304
00:16:04,980 --> 00:16:08,010
So the first term is 21 alpha plus beta.

305
00:16:09,170 --> 00:16:13,230
Well if I multiply that out, I get 21
alpha plus 21 beta.

306
00:16:14,410 --> 00:16:15,636
Alright.

307
00:16:15,636 --> 00:16:18,100
Alpha, when we're taking the derivative
with respect to beta,

308
00:16:18,100 --> 00:16:21,420
the alpha becomes a constant, so the 21
alpha actually will,

309
00:16:21,420 --> 00:16:24,800
when we take the derivative it's just a
constant, will go to 0.

310
00:16:24,800 --> 00:16:26,620
21 beta will become 21, so

311
00:16:26,620 --> 00:16:30,610
hopefully you remember your rules of
derivative, you have 21x,

312
00:16:30,610 --> 00:16:35,280
if you take the derivative of that that's
just going to be 21, you lose the x.

313
00:16:35,280 --> 00:16:36,620
Okay, so that's the first.

314
00:16:36,620 --> 00:16:40,240
Second term here requires you to remember
little bit more calculus but

315
00:16:40,240 --> 00:16:42,600
basically again I got a beta in this term,
so

316
00:16:42,600 --> 00:16:45,500
this is not a constant since my beta is my
unknown.

317
00:16:45,500 --> 00:16:47,010
The negative will be preserved.

318
00:16:47,010 --> 00:16:47,850
The 21 will be preserved.

319
00:16:47,850 --> 00:16:49,750
So we'll still have the negative 21, and

320
00:16:49,750 --> 00:16:54,200
then you have to remember, how do you take
the derivative of a log.

321
00:16:54,200 --> 00:16:58,470
So the derivative of a log is, the
derivative of the log x is 1 over x.

322
00:16:58,470 --> 00:17:02,010
So in this case the derivative of log of
everything that's in that parentheses,

323
00:17:02,010 --> 00:17:03,990
that just goes in the denomenator.

324
00:17:03,990 --> 00:17:09,430
Kay, so that, the derivative of log x is 1
over x.

325
00:17:09,430 --> 00:17:14,800
So we put here everything that's inside
these parentheses, 1 over that.

326
00:17:14,800 --> 00:17:16,150
Then we have to use chain rule.

327
00:17:16,150 --> 00:17:17,310
So what's the chain rule?

328
00:17:17,310 --> 00:17:20,210
So the chain rule says that we then have
to take the derivative of everything

329
00:17:20,210 --> 00:17:22,200
that's inside the parenthesis.

330
00:17:22,200 --> 00:17:25,660
So again the derivative of 1 is 0 because
that's a constant.

331
00:17:25,660 --> 00:17:28,590
But what's the derivative of e raised to
alpha plus beta?

332
00:17:28,590 --> 00:17:32,170
Well it might help you to remember that
the derivative of you know,

333
00:17:32,170 --> 00:17:35,840
e raised to x is just itself e raised to
x.

334
00:17:35,840 --> 00:17:39,160
the, the x, so that is easy,

335
00:17:39,160 --> 00:17:42,570
the derivative of the exponential function
is just the exponential function.

336
00:17:42,570 --> 00:17:46,755
So in fact when we take the derivative of
e raised to alpha plus beta,

337
00:17:46,755 --> 00:17:49,610
it just becomes e raised to alpha plus
beta.

338
00:17:49,610 --> 00:17:52,630
We then have to take the derivative of
beta but that's just 1.

339
00:17:52,630 --> 00:17:54,170
So we don't end up with anything extra
there.

340
00:17:54,170 --> 00:17:59,900
So that defines this term here and we do
the same logic for the next term.

341
00:17:59,900 --> 00:18:02,660
And then we take the derivative with
respect to alpha, so

342
00:18:02,660 --> 00:18:05,520
we get a 22 alpha becomes 22 and so on and
so forth.

343
00:18:05,520 --> 00:18:07,310
So that's how you take the derivative, and

344
00:18:07,310 --> 00:18:11,230
again I don't want to over emphasize the
actual mechanics of doing the derivative.

345
00:18:11,230 --> 00:18:14,310
It's most important that you just believe
me that those are the derivatives and

346
00:18:14,310 --> 00:18:16,490
if you want and you're interested and it's
fun for

347
00:18:16,490 --> 00:18:18,770
you to do calculus, feel free to, to go
through that on your own.

348
00:18:20,600 --> 00:18:23,570
All right, so next we're going to maximize
the function.

349
00:18:23,570 --> 00:18:25,370
So we'll start with the derivative of
alpha function.

350
00:18:25,370 --> 00:18:26,140
We actually need to solve for

351
00:18:26,140 --> 00:18:30,820
alpha first so that we can then plug alpha
into the function for beta.

352
00:18:30,820 --> 00:18:33,940
So, here's what we get out when we set
that function that

353
00:18:33,940 --> 00:18:37,150
I just showed on the previous slide, the
derivative with respect to alpha.

354
00:18:37,150 --> 00:18:40,224
When we set that equal to zero, we get out
this function here and

355
00:18:40,224 --> 00:18:42,840
then it's just algebraic rearrangement.

356
00:18:42,840 --> 00:18:43,740
So, you know, I,

357
00:18:43,740 --> 00:18:49,390
I'm just algebraically arranging here in
order to be able to solve this function.

358
00:18:50,540 --> 00:18:54,350
So I'm going to add up these two because
22 e over alpha plus 51 or

359
00:18:54,350 --> 00:18:58,730
e over alpha this gets me the 73 here, the
denominators are the same

360
00:18:58,730 --> 00:19:03,560
cross multiplying doing you know, some
subtraction the division here.

361
00:19:03,560 --> 00:19:05,580
So I end up with if I solved all the
algebra here.

362
00:19:05,580 --> 00:19:08,100
And again, feel free to kind of pause the
video and walk through that on your own.

363
00:19:08,100 --> 00:19:12,020
Make sure you get the same answer, but I'm
going to go over it quickly to save time.

364
00:19:12,020 --> 00:19:18,110
The we end up that e alpha is equal to 22
over 51.

365
00:19:18,110 --> 00:19:21,280
So e alpha, e raised to alpha is 22 over
51.

366
00:19:21,280 --> 00:19:23,100
Notice what that is.

367
00:19:23,100 --> 00:19:30,910
So think back to our 2 by 2 table, we had
21 in cell A,

368
00:19:30,910 --> 00:19:34,589
we had 6 in cell C, 22 and 51, so what
was, what's the 22 over 51?

369
00:19:34,589 --> 00:19:42,320
Well that was among the unexposed, that is
the odds of disease in the unexposed.

370
00:19:42,320 --> 00:19:46,640
Well that's what alpha represents, alpha
represents the odds of

371
00:19:46,640 --> 00:19:50,260
getting the disease in the unexposed
group, because that's the group you have,

372
00:19:50,260 --> 00:19:54,530
you get alpha if you're, if you are
unexposed and the beta drops out.

373
00:19:54,530 --> 00:19:56,200
And only the intercept applies to you,

374
00:19:56,200 --> 00:19:58,160
so that's the odds of disease in the
unexposed.

375
00:19:58,160 --> 00:20:02,320
Now we can also take the natural log of
this 22 over 51 and

376
00:20:02,320 --> 00:20:07,750
we will get a value of about negative
0.84, so

377
00:20:07,750 --> 00:20:13,526
the, so alpha, e alpha is 22 over 51,
alpha is equal to -0.84.

378
00:20:13,526 --> 00:20:18,860
That represents the log odds of the
disease in the unexposed group.

379
00:20:18,860 --> 00:20:20,380
So that's maximizing alpha.

380
00:20:20,380 --> 00:20:24,310
We can then take that alpha and plug it in
to solve for beta.

381
00:20:24,310 --> 00:20:26,730
So now we're going to maximize the beta
function.

382
00:20:26,730 --> 00:20:31,050
So again, take the equation, the log

383
00:20:31,050 --> 00:20:35,300
likelihood equation that I showed you in
the earlier slides, set that equal to 0.

384
00:20:35,300 --> 00:20:37,100
Do some algebraic rearrangement and

385
00:20:37,100 --> 00:20:39,920
notice that I've I'm going to have to plug
in for alpha.

386
00:20:39,920 --> 00:20:45,550
So later on I'm going to plug in the 22
divided by 51 for alpha.

387
00:20:45,550 --> 00:20:48,390
When I've solved down then I can just plug
that in for e alpha.

388
00:20:48,390 --> 00:20:50,310
So I do some algebraic rearrangement here.

389
00:20:50,310 --> 00:20:51,100
Set the derivatives equal here.

390
00:20:51,100 --> 00:20:54,360
Do that algebraic rearrangement, solve for
e raised to beta.

391
00:20:54,360 --> 00:20:58,240
That becomes 21 over 6 divided by e raised
to alpha, then I plug in we

392
00:20:58,240 --> 00:21:03,010
already solved for alpha, that was 22 over
51, and when I solve this out,

393
00:21:03,010 --> 00:21:06,880
I solved it out in such a way that I can
show you that the e raised to beta just

394
00:21:06,880 --> 00:21:12,450
turns out to be our normal odds ratio, so
21 times 51,

395
00:21:12,450 --> 00:21:19,060
6 times 22, so A times B, divided by B
times, A times D divided by B times C.

396
00:21:19,060 --> 00:21:21,902
That gives us our odds ratio that we're
expecting there so

397
00:21:21,902 --> 00:21:25,788
e raised to beta, we've proven, then just
comes out to be the odds ratio that we

398
00:21:25,788 --> 00:21:27,990
normally see from a two by two table.

399
00:21:27,990 --> 00:21:33,660
We can also solve all the way for beta, so
if I take the log of this value,

400
00:21:33,660 --> 00:21:35,960
21 times 51 divided by 6 times 22.

401
00:21:35,960 --> 00:21:40,740
That's going to give me that beta is equal
to about 2.09,

402
00:21:40,740 --> 00:21:44,180
and of course if you exponentiate that
you'll get the odds ratio.

403
00:21:44,180 --> 00:21:46,550
The odds ratio here comes out to be about
8.

404
00:21:46,550 --> 00:21:50,310
So now we solve for beta using maximum
likelihood estimation.

405
00:21:50,310 --> 00:21:55,240
And I'll just show you that I threw these
data also into a computer, and

406
00:21:55,240 --> 00:21:56,980
I ran them in a logistic regression.

407
00:21:56,980 --> 00:21:59,150
It's a very simple problem, just a two by
two table, but

408
00:21:59,150 --> 00:22:00,770
you can run a logistic regression on it.

409
00:22:00,770 --> 00:22:02,530
The outcome will be you get heart disease
or

410
00:22:02,530 --> 00:22:05,000
not, and older age is the binary
predictor.

411
00:22:05,000 --> 00:22:08,800
And indeed, just to show you where you
find these things in computer output.

412
00:22:08,800 --> 00:22:12,410
We get the estimates that we just
estimated by hand,

413
00:22:12,410 --> 00:22:14,670
from maximum likelihood estimation.

414
00:22:14,670 --> 00:22:19,490
So we get that negative 0.84 for the
intercept, we get the 2.09 for the beta.

415
00:22:19,490 --> 00:22:22,440
Those estimates, you know the computer
does easily for you, so

416
00:22:22,440 --> 00:22:23,870
we don't have to do these by hand, but

417
00:22:23,870 --> 00:22:28,250
it's good just to walk through that whole
logic one time so that you've seen it.

418
00:22:28,250 --> 00:22:30,160
I'll just point out that not only do we
get the alpha and

419
00:22:30,160 --> 00:22:33,150
the beta, we also get the standard error
here.

420
00:22:33,150 --> 00:22:35,290
I'll talk more about that in the next
module.

421
00:22:35,290 --> 00:22:38,580
But as I mentioned before, maximum
likelihood estimation spits you

422
00:22:38,580 --> 00:22:41,139
out the standard error, which is a
function of the second derivative.

423
00:22:42,640 --> 00:22:45,540
Notice that both of these came out to be
significant in these,

424
00:22:45,540 --> 00:22:48,050
in this case, the odds ratio here came out
to be 8.

425
00:22:48,050 --> 00:22:49,760
I get that just by exponentiating the
2.09.

426
00:22:49,760 --> 00:22:54,960
I'm just going to throw in one more
example, just for fun,

427
00:22:54,960 --> 00:22:59,700
if you've had enough, you can, you can
stop here, but just for one more example

428
00:22:59,700 --> 00:23:03,870
to see the likelihood again done out for a
slightly more complicated data set.

429
00:23:03,870 --> 00:23:07,056
This is a two by four data set rather than
a two by two data set, but

430
00:23:07,056 --> 00:23:10,832
imagine again our, our outcome here is
heart disease, you either have it or

431
00:23:10,832 --> 00:23:13,490
you don't, and then there's various races.

432
00:23:13,490 --> 00:23:16,320
There's usually a categorical variable in
this data set.

433
00:23:16,320 --> 00:23:18,110
You see this written down in the
contingency table,

434
00:23:18,110 --> 00:23:19,680
we could apply a chi square here.

435
00:23:19,680 --> 00:23:22,510
We could also run a logistic regression.

436
00:23:22,510 --> 00:23:24,820
And what would the likelihood function be
here?

437
00:23:24,820 --> 00:23:26,280
It's only slightly more complicated, but

438
00:23:26,280 --> 00:23:29,280
just let me remind you what the, this is
going to involve dummy coding.

439
00:23:29,280 --> 00:23:32,310
So what the logistic model will look like,
there will be an alpha.

440
00:23:32,310 --> 00:23:35,880
I'm going to make the white group the
reference group here, and

441
00:23:35,880 --> 00:23:38,640
then we're going to have a beta for the
other races.

442
00:23:38,640 --> 00:23:42,544
So a beta for black, and a beta for
Hispanic, and a beta for other, and these

443
00:23:42,544 --> 00:23:46,814
are dummy code of variables, so you're in
one of those four groups, you get a 1 for

444
00:23:46,814 --> 00:23:50,474
one in those and a 0 for the rest, if you
get a 0 for all three you end up,

445
00:23:50,474 --> 00:23:54,630
you'd just be in the reference group, that
means you're in the white group.

446
00:23:54,630 --> 00:23:56,010
We can code this any way you want.

447
00:23:56,010 --> 00:23:57,810
Any one of those can be the reference
group.

448
00:23:57,810 --> 00:24:02,600
I coded in this way because the reference,
given the, the reference group,

449
00:24:02,600 --> 00:24:05,790
the one that has the lowest risk of heart
disease.

450
00:24:05,790 --> 00:24:08,170
So that's what a logistic model would look
like.

451
00:24:08,170 --> 00:24:09,570
What does the likelihood function look
like?

452
00:24:09,570 --> 00:24:10,810
Again, this is for fun, but

453
00:24:10,810 --> 00:24:13,730
just to show what the likelihood function
would look like, we have group data again.

454
00:24:13,730 --> 00:24:16,580
So there's five people that look the same,
20 people that look the same.

455
00:24:16,580 --> 00:24:21,580
But the terms, so for, if you're white and
you do have heart disease, the predicted

456
00:24:21,580 --> 00:24:24,730
probability from that logistic regression
model would look like this.

457
00:24:24,730 --> 00:24:26,460
There's five of those people.

458
00:24:26,460 --> 00:24:29,270
And everybody's going to get the
intercept, and

459
00:24:29,270 --> 00:24:30,890
if you're in a different racial group,

460
00:24:30,890 --> 00:24:34,920
you'll also get the additional beta for
the race group that you're in, and

461
00:24:34,920 --> 00:24:38,560
I've just raised all of these to the
numbers from that contingency table.

462
00:24:38,560 --> 00:24:40,680
So that's what the likelihood function
would look like here.

463
00:24:40,680 --> 00:24:44,460
Now actually applying calculus, here to
take the derivative and solve for the,

464
00:24:44,460 --> 00:24:48,832
you know, for each one of those betas is
beyond what I would ever want you to do,

465
00:24:48,832 --> 00:24:52,860
but you could do it if you had to, or even
let the computer do it for you.

466
00:24:52,860 --> 00:24:55,310
So I just plug these data into a computer.

467
00:24:55,310 --> 00:24:58,410
The computer maximizes that likelihood
function for me.

468
00:24:58,410 --> 00:25:00,350
Here is the results that we get out at the
end of the day.

469
00:25:00,350 --> 00:25:01,840
Again, I get some estimates.

470
00:25:01,840 --> 00:25:03,890
Those come out of maximum likelihood
estimation.

471
00:25:03,890 --> 00:25:05,230
They come with the standard error.

472
00:25:05,230 --> 00:25:10,300
We also get to see whether or not
individual betas are significant or not.

473
00:25:10,300 --> 00:25:13,590
We also get something out, up here called
the likelihood ratio test,

474
00:25:13,590 --> 00:25:15,080
which I'm going to talk about in the next
module.

475
00:25:15,080 --> 00:25:16,284
Just point it out here.

476
00:25:16,284 --> 00:25:20,080
So you get, it's a chi squared value, in
this case it has, it's a chi square with

477
00:25:20,080 --> 00:25:24,747
three degrees of freedom, and that's
because there's three betas in the model.

478
00:25:24,747 --> 00:25:26,720
We so we get three degrees of freedom.

479
00:25:26,720 --> 00:25:28,050
So that comes out to be significant and

480
00:25:28,050 --> 00:25:30,940
what the interpretation there is that
tells you that race,

481
00:25:30,940 --> 00:25:36,130
that categorical variable, overall, has
some predictive power here.

482
00:25:36,130 --> 00:25:39,100
That's different than looking at the
individual p values for

483
00:25:39,100 --> 00:25:40,640
the individual racial groups.

484
00:25:40,640 --> 00:25:43,960
This gives you a total over the
categorical variable of race,

485
00:25:43,960 --> 00:25:45,520
it's more similar to a chi-square.

486
00:25:45,520 --> 00:25:49,460
Overall, there's some difference in heart
disease, somewhere among those races.

487
00:25:49,460 --> 00:25:51,760
And that's what the likelihood ratio test
tells you.

488
00:25:51,760 --> 00:25:53,980
It looks at all those three betas at once,

489
00:25:53,980 --> 00:25:57,536
rather than giving you individual betas
for, p values for the individual betas.

490
00:25:57,536 --> 00:25:59,270
So I just allude to that, because we're
going to be

491
00:25:59,270 --> 00:26:01,640
talking about the likelihood ratio test in
the next module.
