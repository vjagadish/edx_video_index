1
00:00:05,240 --> 00:00:10,660
In this next module, we're going to dive
right in to the logistic regression model.

2
00:00:10,660 --> 00:00:15,020
We've talked about, when you have binary
or categorical outcomes that, that is when

3
00:00:15,020 --> 00:00:19,020
you're looking at proportions, we've
talked about some statistical tests,

4
00:00:19,020 --> 00:00:21,640
when you don't want to adjust for anything
that you can use.

5
00:00:21,640 --> 00:00:24,740
So this is like the risk difference,
relative risk, the chi square test,

6
00:00:24,740 --> 00:00:26,050
the McNemar's chi-square test.

7
00:00:26,050 --> 00:00:29,130
So we talked about those, but what about
when you want to

8
00:00:29,130 --> 00:00:33,810
do a regression analysis when you have a
binary or categorical outcome?

9
00:00:33,810 --> 00:00:37,390
In that case, you're going to be using
logistic regression,

10
00:00:37,390 --> 00:00:39,580
which we're going to talk about in this
module.

11
00:00:41,330 --> 00:00:43,940
I want to just give you a little overview
of logistic regression.

12
00:00:43,940 --> 00:00:44,900
So to remind you,

13
00:00:44,900 --> 00:00:48,550
when we did linear regression, the model
looked like the following.

14
00:00:48,550 --> 00:00:52,670
We were predicting the value of some
continuous outcome variable,

15
00:00:52,670 --> 00:00:56,710
call it y, as a function of x.

16
00:00:56,710 --> 00:00:58,390
Here's the simple linear regression model.

17
00:00:58,390 --> 00:01:01,800
We have the intercept alpha, and beta is
our slope.

18
00:01:01,800 --> 00:01:05,320
So that was the simple linear regression
model; in the multivariate regression

19
00:01:05,320 --> 00:01:10,900
model, we can add some additional betas
and exes, additional predictors.

20
00:01:10,900 --> 00:01:15,950
That makes it then a plane or a higher
dimension thing.

21
00:01:15,950 --> 00:01:19,090
But, essentially we're fitting lines,
linear regression.

22
00:01:19,090 --> 00:01:23,110
Logistic regression actually isn't that
different; the right-hand side of

23
00:01:23,110 --> 00:01:26,550
the equation is identical in logistic
regression.

24
00:01:26,550 --> 00:01:28,520
So it's going to look something like this.

25
00:01:28,520 --> 00:01:31,646
There's a linear combination of betas and,

26
00:01:31,646 --> 00:01:35,720
and exes, risk factors; it looks exactly
the same.

27
00:01:35,720 --> 00:01:38,900
And everything's going to be set up
exactly the same in terms of

28
00:01:38,900 --> 00:01:42,540
defining the predictors of the, of the
model.

29
00:01:42,540 --> 00:01:44,900
You can put binary predictors in; you can
put continuation,

30
00:01:44,900 --> 00:01:46,930
you could put categorical, you can do
dummy coding.

31
00:01:46,930 --> 00:01:49,910
All of that is the same as with linear
regression.

32
00:01:49,910 --> 00:01:54,000
What's different in logistic regression is
the left-hand side of the equation.

33
00:01:54,000 --> 00:01:59,240
We are no longer modeling Yi, the outcome
variable, directly,

34
00:01:59,240 --> 00:02:04,160
rather, we're going to be modeling a
transformation of the outcome variable.

35
00:02:04,160 --> 00:02:05,960
In logist-, the case of logistic
regression,

36
00:02:05,960 --> 00:02:09,700
we're going to be modelling the log odds
of the outcome.

37
00:02:09,700 --> 00:02:13,370
And I know that sounds terrible, that's
called a logit function, but

38
00:02:13,370 --> 00:02:16,070
don't worry I'll explain why in a minute.

39
00:02:16,070 --> 00:02:19,750
We're going to be modelling this logit
function, the log odds of the outcome,

40
00:02:19,750 --> 00:02:21,290
that's our outcome variable.

41
00:02:22,400 --> 00:02:25,100
We're going to be modelling that as a line
though,

42
00:02:25,100 --> 00:02:30,160
as an alpha plus a beta x, a intercept and
a slope.

43
00:02:30,160 --> 00:02:34,780
Because we've made this transformation,
the beta is just slightly more

44
00:02:34,780 --> 00:02:38,080
complicated to interpret than the beta
from linear regression.

45
00:02:38,080 --> 00:02:40,880
So for linear regression, the beta was
just the slope.

46
00:02:40,880 --> 00:02:44,590
It's, as x goes up by one unit, how much
does y go up?

47
00:02:44,590 --> 00:02:46,570
So there was a direct interpretation.

48
00:02:46,570 --> 00:02:50,490
It's just slightly more tricky to
interpret the betas from

49
00:02:50,490 --> 00:02:51,450
logistic regression.

50
00:02:51,450 --> 00:02:56,300
But actually, it's not terrible, because
the betas from logistic regression,

51
00:02:56,300 --> 00:03:01,020
if you exponentiate them, what it gives
you out is the odds ratio.

52
00:03:01,020 --> 00:03:03,590
And I referred to, in the second week of
the course,

53
00:03:03,590 --> 00:03:06,280
we talked a lot about odds ratios, and I
told you,

54
00:03:06,280 --> 00:03:10,470
hey, logistic regression spits out odds
ratios, and this is where it comes from.

55
00:03:10,470 --> 00:03:16,230
The betas can be directly interpreted as
odds ratios just by exponentiating,

56
00:03:16,230 --> 00:03:19,410
and the reason there's an exponential
involved is because our outcome variable

57
00:03:19,410 --> 00:03:21,000
involves a natural log.

58
00:03:21,000 --> 00:03:22,850
So this is just the overview of where
we're going this.

59
00:03:22,850 --> 00:03:25,760
Just keep in mind that the right hand side
of

60
00:03:25,760 --> 00:03:28,600
the equation's the same as with linear
regression.

61
00:03:28,600 --> 00:03:31,550
So we're still fitting a line its just
we're fitting the outcome variable,

62
00:03:31,550 --> 00:03:34,390
there's something trickier going on.

63
00:03:34,390 --> 00:03:40,600
Let me start here with a simple example
using that example data set,

64
00:03:40,600 --> 00:03:44,050
data set on my Stanford students.

65
00:03:44,050 --> 00:03:47,890
One of the questions I asked my Stanford
students is whether or

66
00:03:47,890 --> 00:03:52,250
not they considered themselves to be
book-smart or street-smart.

67
00:03:52,250 --> 00:03:54,250
This is a binary variable.

68
00:03:54,250 --> 00:03:57,560
Book smart the, the variable is equal to
one,

69
00:03:57,560 --> 00:03:59,520
that means that they consider themselves
book smart.

70
00:03:59,520 --> 00:04:03,340
If the variable is equal to zero, the way
I've coded it,

71
00:04:03,340 --> 00:04:06,160
that means that they consider themselves
to be street smart.

72
00:04:06,160 --> 00:04:11,530
So we can see that this is the book smart
group over here, and

73
00:04:11,530 --> 00:04:15,220
this is the street smart book group over
here.

74
00:04:15,220 --> 00:04:18,770
And I, just as an aside, I've been asking
this question to lots of

75
00:04:18,770 --> 00:04:21,040
Stanford students just for fun over the
years.

76
00:04:21,040 --> 00:04:25,570
And it generally comes out about 80% of my
Stanford students consider themselves to

77
00:04:25,570 --> 00:04:28,930
be book smart, and about 20% consider
themselves to be street smart.

78
00:04:28,930 --> 00:04:31,850
And that was indeed the case in this
little practice data set,

79
00:04:31,850 --> 00:04:33,160
it's right around 80%.

80
00:04:33,160 --> 00:04:38,200
I wondered if book smart students did
more,

81
00:04:38,200 --> 00:04:43,770
spent more time on homework, were more
studious, than street smart students.

82
00:04:43,770 --> 00:04:47,100
And so I wanted to look at the variable
homework in the number of

83
00:04:47,100 --> 00:04:48,630
hours per week that they spent on
homework.

84
00:04:48,630 --> 00:04:51,320
That's another question that I had asked
these students.

85
00:04:51,320 --> 00:04:52,630
And we can look at that here and

86
00:04:52,630 --> 00:04:54,620
I've done this, I've looked at this
graphically here.

87
00:04:54,620 --> 00:04:59,600
So, I'm graphing my predictor variable is
just the binary a yes, no, book smart or

88
00:04:59,600 --> 00:05:04,260
not and my outcome variable here is
homework and the number of hours per week.

89
00:05:04,260 --> 00:05:07,430
And now surprisingly the number of
students who consider themselves book

90
00:05:07,430 --> 00:05:10,640
smart as you can see from this graphic
here, actually on average, do

91
00:05:10,640 --> 00:05:14,430
slightly less homework than the students
who consider themselves street smart for

92
00:05:14,430 --> 00:05:16,310
whatever reason.

93
00:05:16,310 --> 00:05:19,440
So it went kind of in the opposite
direction from what I might have expected.

94
00:05:21,030 --> 00:05:25,870
This analysis, this graphic that I'm
showing you here actually coincides with

95
00:05:25,870 --> 00:05:27,660
a linear regression.

96
00:05:27,660 --> 00:05:32,190
So the way I've set up the problem now is
I've got a binary predictor variable, and

97
00:05:32,190 --> 00:05:35,430
a continuous outcome variable, homework.

98
00:05:35,430 --> 00:05:38,860
And we could model this with a linear
regression using what we

99
00:05:38,860 --> 00:05:40,540
talked about last week.

100
00:05:40,540 --> 00:05:44,100
When we have a binary predictor, it's a
very simple linear regression.

101
00:05:44,100 --> 00:05:47,130
Right, that line is just, you connect to
the mean for

102
00:05:47,130 --> 00:05:49,910
the street smart group with the mean for
the book smarts group and when,

103
00:05:49,910 --> 00:05:53,160
if you connect two points with a line
that, that defines a line.

104
00:05:53,160 --> 00:05:55,420
And so we end up with a very simple line.

105
00:05:55,420 --> 00:05:57,660
So we could answer this question,

106
00:05:57,660 --> 00:06:01,480
look at this question in this data set
with a linear regression or

107
00:06:01,480 --> 00:06:05,239
even a T-test, which of course is a
special case of a linear regression.

108
00:06:06,700 --> 00:06:10,180
This is cross sectional data, however, so
I've happened to pick, just for

109
00:06:10,180 --> 00:06:16,220
the purposes of illustration in, for this
slide to put book smart as the x variable,

110
00:06:16,220 --> 00:06:19,190
as the predictor and homework as the
outcome variable.

111
00:06:19,190 --> 00:06:23,740
But because this is cross sectional data,
I could ask, I could flip that.

112
00:06:23,740 --> 00:06:28,290
I could ask not whether or not whether you
classify yourself as book smart,

113
00:06:28,290 --> 00:06:32,510
predicts the amount of homework you would
do, but I could, ask it the other way,

114
00:06:32,510 --> 00:06:36,410
does the amount of homework you do, the
amount of time you spend on homework,

115
00:06:36,410 --> 00:06:39,084
predict whether or not you consider
yourself book smart?

116
00:06:39,084 --> 00:06:41,950
So, I'm just going to flip the x and y
variables.

117
00:06:41,950 --> 00:06:44,890
So here's the graphic when I flip the x
and y variables.

118
00:06:44,890 --> 00:06:48,700
So, now my x, my predictor variable is
continuous: the number of

119
00:06:48,700 --> 00:06:51,230
hours spent on homework per week.

120
00:06:51,230 --> 00:06:54,345
And my outcome variable is this binary,
this yes,

121
00:06:54,345 --> 00:06:58,220
no: zero is street smart, one is
booksmart.

122
00:06:58,220 --> 00:07:04,580
We've got a new graphic and we could try
to fit a line to these data.

123
00:07:04,580 --> 00:07:07,300
But look what happens when I try to fit a
line through these data; notice that

124
00:07:07,300 --> 00:07:10,260
all of my my outcomes are either all one
or

125
00:07:10,260 --> 00:07:12,370
all zero, so you just get these two
stripes.

126
00:07:12,370 --> 00:07:15,419
And it doesn't, it doesn't seem optimal to
try to fit a line.

127
00:07:16,460 --> 00:07:18,110
I did fit a line to these data.

128
00:07:18,110 --> 00:07:20,400
You can do it, you, this is a linear
regression line so

129
00:07:20,400 --> 00:07:23,590
I fit the linear, the best fit line to
these data.

130
00:07:23,590 --> 00:07:26,210
But you can see, that if you look at

131
00:07:26,210 --> 00:07:30,410
this picture it doesn't really look like
that line fits the data very well, right?

132
00:07:30,410 --> 00:07:33,490
I mean when we were doing linear
regression last week,

133
00:07:33,490 --> 00:07:34,920
we had these scatter plots and

134
00:07:34,920 --> 00:07:37,790
we were trying to find the line that was
kind of very close to all those dots.

135
00:07:37,790 --> 00:07:40,120
Well, this line does, I mean, you can fit
it,

136
00:07:40,120 --> 00:07:42,190
but it doesn't look very close to all
those dots.

137
00:07:42,190 --> 00:07:44,750
It doesn't look like a very good fit.

138
00:07:44,750 --> 00:07:48,010
So a linear regression model was not
optimal here.

139
00:07:49,630 --> 00:07:50,430
Well, what do we do?

140
00:07:50,430 --> 00:07:53,520
Again, I told you, we're still going to
want to fit lines here, so

141
00:07:53,520 --> 00:07:59,350
how can we still fit a line but do
something other than linear regression?

142
00:08:02,300 --> 00:08:04,080
How could we, what could we do?

143
00:08:04,080 --> 00:08:08,680
So what we could do here is to transform
the outcome variable.

144
00:08:08,680 --> 00:08:12,790
Again, I still want to fit a line
ultimately, but I don't want a zero,

145
00:08:12,790 --> 00:08:15,882
one variable as my outcome, because a
zero,

146
00:08:15,882 --> 00:08:18,910
one variable just doesn't fit well to a
line.

147
00:08:18,910 --> 00:08:20,620
It has only two possible values.

148
00:08:21,630 --> 00:08:23,400
So what could I do?

149
00:08:23,400 --> 00:08:26,230
How could I transform that binary, that
zero,

150
00:08:26,230 --> 00:08:29,410
one variable into something that better
fits a line?

151
00:08:30,540 --> 00:08:33,270
Well here is one thing; this is where just
a starting point because we're

152
00:08:33,270 --> 00:08:34,350
going to go further with this,

153
00:08:34,350 --> 00:08:37,810
but let's just start with with some simple
thing that we can do.

154
00:08:37,810 --> 00:08:41,597
So recognize that instead of modeling it
as a yes, no,

155
00:08:41,597 --> 00:08:46,020
a zero, one, what if we could somehow
model it as a probability?

156
00:08:47,160 --> 00:08:52,350
A probability, as you know, is a value
that ranges from zero to one,

157
00:08:52,350 --> 00:08:55,320
but it can take on any value between zero
and one.

158
00:08:55,320 --> 00:08:56,940
So we would do a little bit better,

159
00:08:56,940 --> 00:09:01,800
we would put in all these values between
Between zero and one back into play.

160
00:09:01,800 --> 00:09:05,480
If somehow we could model this as we could
make the outcome variable

161
00:09:05,480 --> 00:09:09,140
the probability of being booksmart, rather
than yes, no you are booksmart.

162
00:09:10,820 --> 00:09:12,570
Just looking at the data here, you can
see,

163
00:09:12,570 --> 00:09:17,400
well, that's not immediate obvious,
immediately obvious how you would do it.

164
00:09:19,438 --> 00:09:22,160
In fact, we're going to do something
called the logit,

165
00:09:22,160 --> 00:09:23,980
which is the function of the probability
and

166
00:09:23,980 --> 00:09:27,930
in fact, what the computer is going to do
is do a little algorithm involving some

167
00:09:27,930 --> 00:09:30,870
calculus to calculate the probability,
here, the logit.

168
00:09:30,870 --> 00:09:35,060
But we could kind of approximate it just
actually looking at the data, and

169
00:09:35,060 --> 00:09:38,020
I think this is the best way to, to
understand it.

170
00:09:38,020 --> 00:09:40,690
So, if I wanted to get a probability here
rather than a zero,

171
00:09:40,690 --> 00:09:42,340
one variable, imagine I did the following.

172
00:09:42,340 --> 00:09:45,100
What if I just, to get a probability, I'm
going to have to

173
00:09:45,100 --> 00:09:47,850
look at more then one person, I'm going to
have to look at a group of people.

174
00:09:47,850 --> 00:09:53,310
So what if I kind of arbitrarily looked at
you know, a group of people let's

175
00:09:53,310 --> 00:10:00,110
just pick say the, the 10 people with the
lowest amount of homework time per week.

176
00:10:00,110 --> 00:10:03,670
Notice that these little red dots here,
actually some of these dots represent more

177
00:10:03,670 --> 00:10:07,850
than one person, because there were a lot
of people who had tied values.

178
00:10:07,850 --> 00:10:11,500
They selected the same number of homework
and they also called themselves book smart

179
00:10:11,500 --> 00:10:14,970
for example, so that, well, each of these
dots may represent more than one person.

180
00:10:14,970 --> 00:10:21,870
So imagine that this and this what I put
in this box here, represents ten people.

181
00:10:21,870 --> 00:10:25,850
I could look at the, say the ten people
with the lowest amount of homework time,

182
00:10:25,850 --> 00:10:27,950
putting in the lowest amount of homework
time, and

183
00:10:27,950 --> 00:10:31,480
I could count up how many of them called
themselves book smart and

184
00:10:31,480 --> 00:10:35,070
how many of them called themselves street
smart.

185
00:10:35,070 --> 00:10:38,780
And I could calculate a proportion, a
percentage, a probability.

186
00:10:38,780 --> 00:10:40,490
So let's say, I'm just making this up, but

187
00:10:40,490 --> 00:10:43,570
let's say when I calculate that, again
since some of these dots are overlapping,

188
00:10:43,570 --> 00:10:47,970
I can't do it exactly, but from the
graphic, but let's say I count up and

189
00:10:47,970 --> 00:10:54,425
I find that among those doing the least
Amount of homework, let's say that

190
00:10:54,425 --> 00:10:59,785
90% of them, again I'm just making this
up, let's say 90% of that 10,

191
00:10:59,785 --> 00:11:04,170
nine out of 10, consider themselves book
smart rather than street smart.

192
00:11:04,170 --> 00:11:07,615
Now I've got a probability, rather than a
zero, one.

193
00:11:07,615 --> 00:11:08,930
And I could do this for

194
00:11:08,930 --> 00:11:12,450
the next next group of ten, the next
highest in homework.

195
00:11:13,490 --> 00:11:19,410
Maybe in the next group, the probability
is again I'm just making this up,

196
00:11:19,410 --> 00:11:24,090
is 85% and I can't tell, because again
these dots are overlapping.

197
00:11:24,090 --> 00:11:28,810
Maybe that comes out to be 85% but I could
go along and I could group people,

198
00:11:28,810 --> 00:11:32,320
I have to group people in order to be able
to get a percentage, a probability, but

199
00:11:32,320 --> 00:11:34,670
I could come up with some probabilities.

200
00:11:34,670 --> 00:11:38,480
And of course the probabilities I come up
with are going to depend exactly on how I

201
00:11:38,480 --> 00:11:41,820
group them, but just realize that the
grouping is just for

202
00:11:41,820 --> 00:11:43,930
the purposes of illustration.

203
00:11:43,930 --> 00:11:45,790
When you actually do this with calculus,

204
00:11:45,790 --> 00:11:48,140
you don't have to make any arbitrary
groupings.

205
00:11:48,140 --> 00:11:52,810
Of course we won't go through the calculus
here, but the computer does that for you.

206
00:11:52,810 --> 00:11:58,050
So if I could get that probability, now I
can make a graphic which is

207
00:11:58,050 --> 00:12:01,370
my predictor variable is the number of
hours of homework per week,

208
00:12:01,370 --> 00:12:03,140
the continuous variable.

209
00:12:03,140 --> 00:12:08,010
Now my outcome variable is a probability,
a number that ranges between zero and one.

210
00:12:09,030 --> 00:12:11,400
I have fewer dots on this graphic.

211
00:12:11,400 --> 00:12:15,020
I'm going to just for now, talk about the
graphic on the left side, this 10 groups.

212
00:12:16,180 --> 00:12:17,940
Notice I have fewer dots on that graphic,

213
00:12:17,940 --> 00:12:19,700
none of these dots are overlapping
anymore.

214
00:12:19,700 --> 00:12:24,830
So, I actually have a, I had, my previous
graphic represented 48 people,

215
00:12:24,830 --> 00:12:27,500
this graphic only represents 10
observations.

216
00:12:27,500 --> 00:12:33,450
I divided those 48 people up into roughly
even, groups of 10 groups of,

217
00:12:33,450 --> 00:12:36,840
you know, somewhere between about four or
five per group.

218
00:12:36,840 --> 00:12:39,560
And I got those percentages and I plotted
them here.

219
00:12:39,560 --> 00:12:40,852
What you notice is,

220
00:12:40,852 --> 00:12:46,030
this is much more amenable, this, this
scatter plot, to fitting a line.

221
00:12:46,030 --> 00:12:48,640
Right now we've got dots all around the
range, and

222
00:12:48,640 --> 00:12:51,880
you can imagine actually fitting a
meaningful line to those dots.

223
00:12:51,880 --> 00:12:55,020
As I mentioned, this is just for

224
00:12:55,020 --> 00:12:59,230
the purposes of graphing, just to show you
the concept.

225
00:12:59,230 --> 00:13:02,410
And, so it's arbitrary how I group people,
so of course,

226
00:13:02,410 --> 00:13:05,320
the graphic's going to look a little bit
different depending on how I group people.

227
00:13:05,320 --> 00:13:08,860
So just to show you that I also divided it
into five groups, there's 48 people, so

228
00:13:08,860 --> 00:13:12,140
this is roughly groups of, you know,
eight, nine, or 10, and

229
00:13:12,140 --> 00:13:15,250
I calculated the proportion who considered
themselves book smart,

230
00:13:15,250 --> 00:13:19,240
that probability in each of those five
groups and I plotted it here.

231
00:13:19,240 --> 00:13:24,140
By the way when I'm, each of these groups
of course has a range of homework values.

232
00:13:24,140 --> 00:13:28,710
I'm using the mean homework time for each
group,

233
00:13:28,710 --> 00:13:30,020
that's what I'm putting in the plot here.

234
00:13:30,020 --> 00:13:34,530
So for example, the lowest homework group
on this plot here, they have

235
00:13:34,530 --> 00:13:39,400
a mean homework time of just, you know,
and maybe, thirty minutes to an hour, and

236
00:13:39,400 --> 00:13:42,680
there, the proportion that considered
themselves book smart was right about 78%.

237
00:13:42,680 --> 00:13:47,420
So I'm using the mean, just for graphical
purposes.

238
00:13:48,500 --> 00:13:52,000
But the picture looks a little different
depending on how you group it, but

239
00:13:52,000 --> 00:13:53,170
you can get the idea.

240
00:13:53,170 --> 00:13:56,534
Now we've put back all the numbers between
zero and

241
00:13:56,534 --> 00:13:59,240
one into play, it makes much more sense to
fit a line.

242
00:13:59,240 --> 00:14:03,050
But this is still limited, okay, so this
is not, we're going to go further than

243
00:14:03,050 --> 00:14:07,772
that, with this, because a probability is
a number that's bounded between zero and

244
00:14:07,772 --> 00:14:10,880
one, so it's still a bounded quantity.

245
00:14:10,880 --> 00:14:13,680
You can't go lower than zero, you can't go
higher than one.

246
00:14:13,680 --> 00:14:17,450
The problem with that is, imagine when we
fit a line, when you fit a line,

247
00:14:17,450 --> 00:14:20,386
theoretically, that line can go anywhere
from negative in-,

248
00:14:20,386 --> 00:14:24,160
the y-value can go anywhere from negative
infinity to positive infinity.

249
00:14:24,160 --> 00:14:25,750
So, well this, this is still not perfect.

250
00:14:27,640 --> 00:14:33,140
So, what we're going to do is, instead of
directly having the probability

251
00:14:33,140 --> 00:14:35,970
be the outcome variable, we're going to
make a little transformation.

252
00:14:35,970 --> 00:14:38,920
So, probability is bounded from zero to
one.

253
00:14:38,920 --> 00:14:44,550
If I divide the probability however, by 1
minus the probability,

254
00:14:44,550 --> 00:14:48,290
that of course is what we had learned
earlier is the odd.

255
00:14:48,290 --> 00:14:52,240
So divide the probability by the
probability of something not happened, so

256
00:14:52,240 --> 00:14:55,020
the probability divided by 1 minus the
probability.

257
00:14:55,020 --> 00:14:57,990
That's what we learned in Week two of the
course is in odds.

258
00:14:59,000 --> 00:15:00,910
Why would I want to do that?

259
00:15:00,910 --> 00:15:03,830
Well, a probability can't be bigger than
one,

260
00:15:03,830 --> 00:15:09,260
but an odds can go from anywhere, anywhere
from zero to positive infinity.

261
00:15:09,260 --> 00:15:11,500
The odds has no right-hand bound.

262
00:15:11,500 --> 00:15:15,280
So now we've put in all the positive real
numbers back into play.

263
00:15:16,820 --> 00:15:17,890
We're still missing the negatives.

264
00:15:17,890 --> 00:15:19,400
How can we get the negatives back in?

265
00:15:19,400 --> 00:15:21,430
So here's where the natural log comes in.

266
00:15:21,430 --> 00:15:27,660
So if I then take the natural log of the
odds, it turns out that when you take

267
00:15:27,660 --> 00:15:33,660
a natural log, any value that was below
one now becomes a negative number.

268
00:15:33,660 --> 00:15:38,170
So this, the natural log of the odds,
actually ranges from negative,

269
00:15:38,170 --> 00:15:41,690
can theoretically range from negative
infinity to positive infinity.

270
00:15:41,690 --> 00:15:46,278
So now we've put all the numbers back into
play, so now this is the most optimal for

271
00:15:46,278 --> 00:15:47,410
fitting a line.

272
00:15:47,410 --> 00:15:51,690
So the outcome variable for logistic
regression, as I've mentioned earlier,

273
00:15:51,690 --> 00:15:55,810
is actually going to be the natural log of
the odds of the outcome; a little

274
00:15:55,810 --> 00:16:00,170
complicated but it's for mathematical
reasons that we, we choose that.

275
00:16:01,580 --> 00:16:07,260
And just to illustrate, I took those, the
same groupings, ten groups and

276
00:16:07,260 --> 00:16:11,510
five groups, where I had calculated the
percent, the probability of

277
00:16:11,510 --> 00:16:15,890
being book smart in each of those groups
and I transformed that into a log odd.

278
00:16:15,890 --> 00:16:19,420
So that is I took the percentage divided
by 1 minus the percentage and

279
00:16:19,420 --> 00:16:22,330
then took the natural log of that number.

280
00:16:22,330 --> 00:16:26,420
This gives me some numbers like 0.5, 1.0,
1.5, 2.0.

281
00:16:26,420 --> 00:16:31,280
The, the log odds has no inherent meaning
to me, but that's okay 'because when we,

282
00:16:31,280 --> 00:16:33,740
eventually we'll translate it back into
something meaningful.

283
00:16:33,740 --> 00:16:38,340
But now, this outcome variable is a number
that can theoretically go anywhere from

284
00:16:38,340 --> 00:16:42,910
negative infinity to positive infinity,
makes a very fit, nice for fitting a line.

285
00:16:42,910 --> 00:16:47,450
I'm graphing here the logit for each of
these groups against again, the mean

286
00:16:47,450 --> 00:16:53,340
homework time in each of those groups, and
you could fit a line to that.

287
00:16:53,340 --> 00:16:54,220
And that's what we're doing in

288
00:16:54,220 --> 00:16:56,700
logistic regression; that's the line that
we're fitting.

289
00:16:56,700 --> 00:16:59,650
So we're going to estimate an alpha and
beta, an intercept and

290
00:16:59,650 --> 00:17:01,280
a slope for that particular line.

291
00:17:02,600 --> 00:17:05,930
As I mentioned, the groupings here are
completely arbitrary.

292
00:17:05,930 --> 00:17:09,760
When you actually do this in the computer,
there's calculus going on, and so

293
00:17:09,760 --> 00:17:12,740
you're going to get the best alpha and the
best beta.

294
00:17:12,740 --> 00:17:16,950
When I do this graphically, it's a little
bit of an approximation.

295
00:17:16,950 --> 00:17:20,570
You can see that the line is slightly
different when I group it into ten groups,

296
00:17:20,570 --> 00:17:23,950
than when I group it into five groups;
these lines are slightly different.

297
00:17:23,950 --> 00:17:28,250
But, we can actually kind of look at these
graphics and just kind of

298
00:17:28,250 --> 00:17:31,650
visually approximate the equation of that
line, so I'm going to do that now.

299
00:17:33,280 --> 00:17:36,070
This is not as good as doing the calculus
in the computer, but

300
00:17:36,070 --> 00:17:38,280
it is, it'll give us a little bit of an
approximation.

301
00:17:38,280 --> 00:17:39,650
And it'll, I think it's also good for

302
00:17:39,650 --> 00:17:41,640
illustrating what's actually going on
here.

303
00:17:41,640 --> 00:17:43,520
So what is the equation of this line?

304
00:17:43,520 --> 00:17:46,510
There's two lines here and we can kind of
say the,

305
00:17:46,510 --> 00:17:49,540
the real line is going to be somewhere
around the these two.

306
00:17:49,540 --> 00:17:51,500
So first of all what's the intercept
value?

307
00:17:51,500 --> 00:17:55,120
So if I just look at the graphic, my
intercept for this line over on the left

308
00:17:55,120 --> 00:18:01,100
side is oh, I'm going to guess about 1.6,
you know I'm just approximating here.

309
00:18:01,100 --> 00:18:03,110
And the intercept over for

310
00:18:03,110 --> 00:18:07,640
the line on the right hand side of the
page is maybe about 1.8.

311
00:18:07,640 --> 00:18:11,940
So lets just kind of call it that the, the
equation for

312
00:18:11,940 --> 00:18:15,310
this line again if we're kind of saying
there's one line here that we're trying to

313
00:18:15,310 --> 00:18:18,282
approximate, maybe it's around 1.7; I'm
just making,

314
00:18:18,282 --> 00:18:24,180
it's somewhere in that ballpark, maybe
around 1.7 is the intercept.

315
00:18:24,180 --> 00:18:26,050
How would I get the slopes of these lines?

316
00:18:26,050 --> 00:18:29,430
Just visually here, how could I
approximate the slopes of these lines?

317
00:18:29,430 --> 00:18:30,210
So what's a slope?

318
00:18:30,210 --> 00:18:31,230
It's rise over run.

319
00:18:31,230 --> 00:18:37,120
So, I could say well, how much have I gone
up in the y variable and

320
00:18:37,120 --> 00:18:39,219
how much have I gone up in the x variable?

321
00:18:40,260 --> 00:18:43,200
So I can just kind of look at this as a
triangle here and

322
00:18:43,200 --> 00:18:47,190
say, well, what's the, how far did I go
on, on the x-axis?

323
00:18:47,190 --> 00:18:52,470
I went from about zero to 40, so the x
changes about 40 here for this triangle.

324
00:18:52,470 --> 00:18:54,590
On the y end of things, yeah,

325
00:18:54,590 --> 00:19:00,332
if you kind of approximate it's maybe
going from about 0.3 maybe to 1.6.

326
00:19:00,332 --> 00:19:05,675
I'm going to just call it 1.3, so sorry
the,

327
00:19:05,675 --> 00:19:10,570
you the, we go up 1.3 in the log odds.

328
00:19:10,570 --> 00:19:14,580
So the slope here is going to be rise over
run so 1.3 divided by 40 or

329
00:19:14,580 --> 00:19:17,250
approximately 0.03 so the slope here is
about 0.03.

330
00:19:17,250 --> 00:19:20,389
For this line we can use the same kind of
logic.

331
00:19:22,150 --> 00:19:26,460
So, for the x we went over about 35 here,
that's about zero to 35.

332
00:19:26,460 --> 00:19:27,285
On the y end,

333
00:19:27,285 --> 00:19:33,170
we've gone up a little bit bigger, maybe
about 1.8, from one to about 1.8.

334
00:19:33,170 --> 00:19:37,800
The slope then would be 1.8 divided by 35,
which is about 0.05.

335
00:19:37,800 --> 00:19:41,337
So, you know, this somewhere, I'm going to
approximate somewhere between 0.03 and

336
00:19:41,337 --> 00:19:42,320
0.05, let's call it 0.04.

337
00:19:42,320 --> 00:19:44,130
And of course this is negative,

338
00:19:44,130 --> 00:19:47,840
I should emphasize here that this is a
downward slope.

339
00:19:47,840 --> 00:19:51,920
We've gone down, 1.3 or down 1.8 in y.

340
00:19:51,920 --> 00:19:53,660
So both of these slopes are negative so

341
00:19:53,660 --> 00:19:56,180
on average, the slope is going to be
somewhere around negative .04.

342
00:19:56,180 --> 00:19:59,520
This is just my graphical approximation,
but

343
00:19:59,520 --> 00:20:02,270
that's what we're doing on logistic
regression, that's the line we're fitting.

344
00:20:03,770 --> 00:20:06,180
Now I'm actually going to put the data in
the computer and

345
00:20:06,180 --> 00:20:10,920
let the computer use the beautiful
calculus to estimate this perfectly to get

346
00:20:10,920 --> 00:20:16,620
the actual best intercept and best beta
slope that fits these data.

347
00:20:16,620 --> 00:20:19,130
And I'll show you that our approximation
wasn't bad.

348
00:20:19,130 --> 00:20:22,400
Here's what the computer comes out with
when it does what's called maximum

349
00:20:22,400 --> 00:20:25,050
likelihood estimation, this involves
calculus.

350
00:20:25,050 --> 00:20:28,420
And, this is just the way that using some
calculus that we're getting

351
00:20:28,420 --> 00:20:31,970
the best alpha and the best beta here, the
best fit line.

352
00:20:31,970 --> 00:20:35,530
So, when I do this on the computer, the
intercept comes out to be just over two.

353
00:20:35,530 --> 00:20:38,564
So it's just slightly different than what
we did, got in our approximation.

354
00:20:38,564 --> 00:20:43,370
The slope came out to be right around
negative 04, negative .039.

355
00:20:43,370 --> 00:20:45,000
So, and where did I get those in the
output?

356
00:20:45,000 --> 00:20:48,420
Here's the estimate for the intercept,
that's the alpha estimate.

357
00:20:48,420 --> 00:20:51,780
Here's the estimate for the beta, that's
the estimate for the slope.

358
00:20:52,780 --> 00:20:56,350
Along with the alpha and the beta, we also
get a p-value to tell us whether that

359
00:20:56,350 --> 00:20:59,990
alpha or beta is statistically different
than zero.

360
00:20:59,990 --> 00:21:04,820
A beta of zero would mean no association,
same as it did with linear regression.

361
00:21:04,820 --> 00:21:08,370
So the intercept is clearly significantly
different than zero.

362
00:21:08,370 --> 00:21:10,560
again, that's not all that meaningful
here,

363
00:21:10,560 --> 00:21:13,210
because we wouldn't expect the intercept
to be zero.

364
00:21:13,210 --> 00:21:17,170
However the, the p value for homework is
not statistically

365
00:21:17,170 --> 00:21:19,960
significant, so as you might have guessed
just looking at the picture, there

366
00:21:19,960 --> 00:21:25,050
isn't a strong association here, so it's
not statistically different than zero.

367
00:21:25,050 --> 00:21:27,790
Probably meaning it doesn't look like
there's a lot of evidence that

368
00:21:27,790 --> 00:21:32,000
homework and whether or not you consider
yourself book smart are related.

369
00:21:33,160 --> 00:21:34,490
But here's the final model.

370
00:21:34,490 --> 00:21:39,420
The log odds of your, the log odds of
being book smart or

371
00:21:39,420 --> 00:21:44,235
considering yourself book smart is equal
to the intercept 2.12 minus the slope

372
00:21:44,235 --> 00:21:49,200
0.039, and we would of course multiply
that beta times the amount of homework.

373
00:21:49,200 --> 00:21:51,350
So this is a continuous predictor.

374
00:21:51,350 --> 00:21:54,500
So we would be multiplying the amount of
homework for

375
00:21:54,500 --> 00:21:56,850
a particular person, times that slope.

376
00:21:59,040 --> 00:22:01,310
What's this logistic regression equation
useful for?

377
00:22:01,310 --> 00:22:04,796
So, just like when we talked about linear
regression, there's many,

378
00:22:04,796 --> 00:22:07,770
a couple different pote-, potential
functions here.

379
00:22:07,770 --> 00:22:11,410
So one thing is we could use the logistic
regression equation model to try to

380
00:22:11,410 --> 00:22:13,230
do some kind of prediction.

381
00:22:13,230 --> 00:22:15,300
This is very useful in clinical
applications,

382
00:22:15,300 --> 00:22:19,220
because often we want to classify people
as having a condition or

383
00:22:19,220 --> 00:22:22,390
not having a condition, that's a binary
outcome.

384
00:22:22,390 --> 00:22:24,560
So if we're doing that kind of prediction,

385
00:22:24,560 --> 00:22:27,570
we would likely be using logistic
regression.

386
00:22:27,570 --> 00:22:31,150
So we can try to come up with a set of

387
00:22:31,150 --> 00:22:36,920
predictors that predicts who's going to
have a disease or not have a disease.

388
00:22:36,920 --> 00:22:39,310
So that's one of the functions of logistic
regression prediction.

389
00:22:40,480 --> 00:22:46,780
A second function of logistic regression
which is probably used even more often,

390
00:22:46,780 --> 00:22:51,810
is just to try to say, is a particular
predictor like homework,

391
00:22:51,810 --> 00:22:56,450
related to the outcome, in this current
case, book smart or street smart?

392
00:22:56,450 --> 00:22:59,890
Are those variables related, is a
treatment related to an outcome?

393
00:22:59,890 --> 00:23:03,300
And that's very useful in, in and of
itself and

394
00:23:03,300 --> 00:23:05,840
of course we can say is that predictor,

395
00:23:05,840 --> 00:23:10,810
that treatment, related to the outcome
after adjusting for potential confounders?

396
00:23:10,810 --> 00:23:15,520
So that's another very Important use of
the logistic regression model.

397
00:23:15,520 --> 00:23:19,770
I'm going to start by talking just briefly
about the first of these aims,

398
00:23:19,770 --> 00:23:21,260
which is the prediction end of things.

399
00:23:23,018 --> 00:23:26,580
Again, here's the logistic regression
model, we've modeled the outcome

400
00:23:26,580 --> 00:23:30,790
variable that is the log odds of the
outcome, we were modelling that as a line.

401
00:23:32,210 --> 00:23:35,880
Let me take that model that we just fit.

402
00:23:35,880 --> 00:23:38,890
And, how would we actually apply it to
make some predictions?

403
00:23:40,440 --> 00:23:44,300
So, if you're somebody who does no
homework per week, zero hours of homework

404
00:23:44,300 --> 00:23:50,820
per week, you could plug in that value for
homework, of zero into the model.

405
00:23:50,820 --> 00:23:54,500
You get that your predicted logit is 2.12.

406
00:23:54,500 --> 00:23:57,390
As I mentioned most of us don't think in
logits, so in

407
00:23:57,390 --> 00:24:01,330
a minute we're going to do something with
that number to make it more interpretable.

408
00:24:01,330 --> 00:24:03,300
But if you did ten hours per week of
homework,

409
00:24:03,300 --> 00:24:07,020
you could plug in ten, the predicted logit
there is 1.73.

410
00:24:07,020 --> 00:24:10,880
If you did 50 hours per week, the
predicted logit is 0.17.

411
00:24:10,880 --> 00:24:14,710
So the more homework you do, the more the
logit goes down.

412
00:24:14,710 --> 00:24:17,240
Well, I don't want to deal with a logit,
because, a logit, I,

413
00:24:17,240 --> 00:24:18,770
I can't think in logits.

414
00:24:18,770 --> 00:24:22,920
However, we can get this, notice that
there's a probability in there.

415
00:24:22,920 --> 00:24:26,320
What we want to get out is the predicted
probability of the outcome,

416
00:24:26,320 --> 00:24:30,400
the predicted probability of considering
yourself book smart in this case.

417
00:24:30,400 --> 00:24:33,800
I want to isolate this predicted
probability.

418
00:24:33,800 --> 00:24:35,330
Well, it's actually not that hard to do,
right?

419
00:24:35,330 --> 00:24:37,050
We've got one equation, one unknown.

420
00:24:37,050 --> 00:24:39,360
We can isolate that probability.

421
00:24:39,360 --> 00:24:40,250
So how do we do it?

422
00:24:41,440 --> 00:24:44,130
First thing that we're going to want to do
is,

423
00:24:44,130 --> 00:24:46,170
we're going to want to exponentiate both
sides.

424
00:24:46,170 --> 00:24:50,200
So, for example, the zero hours per week,
that person would be predicted to have

425
00:24:50,200 --> 00:24:56,210
a logit of 2.12; I can then exponentiate
both sides of those, of that equation.

426
00:24:56,210 --> 00:25:00,750
So that would tell me that the odds for
somebody who does zero,

427
00:25:00,750 --> 00:25:06,394
homework per week is the exponential
raised to 2.12.

428
00:25:06,394 --> 00:25:09,944
That value is 8.33 if you actually
calculate it out; so

429
00:25:09,944 --> 00:25:12,250
e raised to 2.2 is 8.33.

430
00:25:12,250 --> 00:25:15,080
That's the odds for a person who does, the
odds of

431
00:25:15,080 --> 00:25:19,210
being book smart per person who does zero
hours a per week of homework.

432
00:25:19,210 --> 00:25:22,830
Then we could exponentiate this 1.73, that
will give me the odds for

433
00:25:22,830 --> 00:25:28,060
somebody who does 10 hours per week that's
5.64; we could exponentiate the 0.17, that

434
00:25:28,060 --> 00:25:31,940
gives me the odds for somebody who does 50
hours per week that comes out to be 1.19.

435
00:25:31,940 --> 00:25:35,290
So your odds of being book smart, again,
goes down,

436
00:25:35,290 --> 00:25:39,450
as you do more homework per week, sort of
the opposite of what you might expect.

437
00:25:39,450 --> 00:25:41,410
But of course this is not statistically
significant.

