1
00:00:01,300 --> 00:00:06,550
Now let me me just show you with a simple
example here how these two models work.

2
00:00:06,550 --> 00:00:09,910
So recall that we had some data on from a
pregnancy study.

3
00:00:09,910 --> 00:00:12,790
So 38 women, who didn't have great
fertility,

4
00:00:12,790 --> 00:00:14,940
were all given a treatment for fertility.

5
00:00:14,940 --> 00:00:17,700
They were followed for up to two years to
see if they got pregnant.

6
00:00:17,700 --> 00:00:19,260
So the outcome here was pregnancy.

7
00:00:19,260 --> 00:00:23,800
And when we look carefully at that data It
did kind of look like the hazard rates

8
00:00:23,800 --> 00:00:26,790
were pretty similar from month to month
fairly constant.

9
00:00:26,790 --> 00:00:29,400
That would imply that an exponential model
should be a good fit.

10
00:00:29,400 --> 00:00:32,590
And we can actually do a little bit more
diagnostics to

11
00:00:32,590 --> 00:00:34,640
see if indeed that's the case.

12
00:00:34,640 --> 00:00:39,220
It's actually difficult to take from data
and directly plot the hazard function.

13
00:00:39,220 --> 00:00:40,770
There are ways to do it.

14
00:00:40,770 --> 00:00:43,990
But they are all require a little bit of
hand-waving.

15
00:00:43,990 --> 00:00:45,970
A little bit of assumptions to do it.

16
00:00:45,970 --> 00:00:49,460
So rather than trying to directly plug the
hazard function, what I am instead

17
00:00:49,460 --> 00:00:54,550
going to look at something that's simpler,
which is to get accumulative hazard plot.

18
00:00:54,550 --> 00:00:57,510
And the reason that I can use the
accumulative hazard plot,

19
00:00:57,510 --> 00:01:01,240
is that we have a direct relationship
between the hazard function and

20
00:01:01,240 --> 00:01:03,720
the survival function shown over here in
blue.

21
00:01:03,720 --> 00:01:05,270
I showed you this in an earlier module.

22
00:01:05,270 --> 00:01:10,030
So it turns out if I take my Kaplan Meijer
estimates of survival and take negative

23
00:01:10,030 --> 00:01:15,190
log of them, that that represents the
integral under the hazard function.

24
00:01:15,190 --> 00:01:17,810
The area under the hazard function curve.

25
00:01:17,810 --> 00:01:20,690
Well, if you have the area under a curve.

26
00:01:20,690 --> 00:01:23,080
That is a straight line.

27
00:01:23,080 --> 00:01:26,630
That indicates that the underlying hazard
is, a constant.

28
00:01:26,630 --> 00:01:30,640
So, if the cumulative hazard function,
follows a straight line,

29
00:01:30,640 --> 00:01:33,680
is linear, that would indicate a constant
hazard.

30
00:01:33,680 --> 00:01:36,150
And, if you look at here, these are real
data so

31
00:01:36,150 --> 00:01:38,840
it's not perfectly linear, but this looks
pretty close to a straight line.

32
00:01:38,840 --> 00:01:42,340
So that's indicating to me that probably
the underlying hazard function is

33
00:01:42,340 --> 00:01:43,750
indeed a constant.

34
00:01:43,750 --> 00:01:47,180
So probably an exponential model is
going to be a good fit here.

35
00:01:47,180 --> 00:01:50,000
So I went ahead and put these data, into
my computer and

36
00:01:50,000 --> 00:01:52,190
ran an exponential regression and here are
the results.

37
00:01:53,490 --> 00:01:54,980
So, again, the exponential model,

38
00:01:54,980 --> 00:01:59,990
we're modeling the laog, hazard, as alpha
plus some betas.

39
00:01:59,990 --> 00:02:01,580
In this case I have no predictors in the
model.

40
00:02:01,580 --> 00:02:03,390
I'm just estimating the intercepts.

41
00:02:03,390 --> 00:02:07,130
So, I get that the intercept is equal to
2.2858.

42
00:02:07,130 --> 00:02:07,630
Here.

43
00:02:08,630 --> 00:02:10,697
And this is not a terribly interesting
model,

44
00:02:10,697 --> 00:02:13,082
because again I don't have any predictors
in it, but

45
00:02:13,082 --> 00:02:16,643
I just modeled the underlying function
here, the baseline hazard function.

46
00:02:16,643 --> 00:02:21,668
And because I'm working in SAS, I have to
know that to get the hazard function,

47
00:02:21,668 --> 00:02:25,343
of course I'm dealing, I've modeled the
log hazard, so

48
00:02:25,343 --> 00:02:28,200
I'd have to exponentiate both sides here.

49
00:02:28,200 --> 00:02:32,010
>> So I would have to exponentiate my
alpha to get the hazard function, but

50
00:02:32,010 --> 00:02:33,710
because I'm dealing, I'm working in SAS.

51
00:02:33,710 --> 00:02:37,150
>> I also first have to know that I need
to negate that alpha before I

52
00:02:37,150 --> 00:02:39,670
exponentiate To get the correct hazard
function.

53
00:02:39,670 --> 00:02:43,160
And it would be clear if you exponentiated
the positive number here,

54
00:02:43,160 --> 00:02:44,370
the answer would make no sense,

55
00:02:44,370 --> 00:02:48,480
so just something to keep in mind to keep
track of those negative signs.

56
00:02:48,480 --> 00:02:52,740
If I exponentiate negative 2.2858, I will
get the hazard rate,

57
00:02:52,740 --> 00:02:57,462
the constant hazard rate, and when I do
that I get a value of about ten percent.

58
00:02:57,462 --> 00:03:01,769
If you recall back to the pregnancy study
when we estimated the average

59
00:03:01,769 --> 00:03:06,006
incidence rate across the whole study, it
was right about 10%.

60
00:03:06,006 --> 00:03:08,020
About 10% chance of pregnancy per month,

61
00:03:08,020 --> 00:03:11,610
given that you hadn't got pregnant in
previous months.

62
00:03:11,610 --> 00:03:13,410
So that seems about right.

63
00:03:13,410 --> 00:03:16,000
Now I want to point out a few things
about, over here about the,

64
00:03:16,000 --> 00:03:20,240
what are called the fits statistics, that
we can get for the overall model.

65
00:03:20,240 --> 00:03:23,500
So because we're doing maximum likelihood
estimation,

66
00:03:23,500 --> 00:03:26,330
we can get a negative two log likelihood.

67
00:03:26,330 --> 00:03:30,470
Hopefully you're familiar with that
already, but basically all we're doing to

68
00:03:30,470 --> 00:03:33,530
get this value is we're plugging back in
the fitted value for

69
00:03:33,530 --> 00:03:35,499
the intercept here, 2.2858.

70
00:03:35,499 --> 00:03:39,740
And any other fitted values, fitted
parameters, we're plugging those back in.

71
00:03:39,740 --> 00:03:43,940
We're actually getting a, a value for the
probability of our data.

72
00:03:43,940 --> 00:03:45,270
That number might come out to be very,

73
00:03:45,270 --> 00:03:48,300
very small cause there's lots of different
configurations that could have

74
00:03:48,300 --> 00:03:52,630
happened in our data, but we're getting
that probability back out.

75
00:03:52,630 --> 00:03:57,570
We take negative two log of it which makes
it, you know, some kind of whole, positive

76
00:03:57,570 --> 00:04:00,710
number, something we can deal with better
than a teeny, tiny probability.

77
00:04:02,000 --> 00:04:04,800
Because we're taking the likelihood as a
probability, so

78
00:04:04,800 --> 00:04:09,210
it's something as a fraction and then we
take negative two log of that.

79
00:04:09,210 --> 00:04:13,340
That's going to mean that the higher the
probability the smaller the negative 2

80
00:04:13,340 --> 00:04:13,910
log likelihood.

81
00:04:13,910 --> 00:04:16,750
So, making our data have a higher
probability is

82
00:04:16,750 --> 00:04:19,660
going to give us a smaller negative 2 log
likelihood.

83
00:04:19,660 --> 00:04:22,450
I want to point out some other statistics
that are directly related to

84
00:04:22,450 --> 00:04:25,960
the negative log likelihood, that we
haven't talked about before but

85
00:04:25,960 --> 00:04:29,930
that can be used to compare different
models, even unnested models.

86
00:04:29,930 --> 00:04:34,960
And that is the AIC criteria, and the
AICC.

87
00:04:34,960 --> 00:04:41,080
So the AIC criteria is something called,
and I'm going to pronounce this wrong,

88
00:04:41,080 --> 00:04:46,320
probably, but something like Akaike, the
Akaike information criteria.

89
00:04:46,320 --> 00:04:49,824
Excuse if my pronunciation is not quite
right there, but

90
00:04:49,824 --> 00:04:53,328
this is another way of telling us how good
our model is, so

91
00:04:53,328 --> 00:04:56,659
that we can again compare between
different models.

92
00:04:56,659 --> 00:04:58,459
Say a model that is an exponential or

93
00:04:58,459 --> 00:05:02,070
a model that has an additional parameter
for the weigel.

94
00:05:02,070 --> 00:05:04,820
What is this AIC criteria.

95
00:05:04,820 --> 00:05:09,630
That it's nothing too, different than the
negative 2 Log Likelihood.

96
00:05:09,630 --> 00:05:14,180
It's just the negative 2 Log Likelihood
plus a penalty for

97
00:05:14,180 --> 00:05:16,650
the number of parameters that you have in
your model.

98
00:05:16,650 --> 00:05:20,610
So every time you put more parameters in
the model, your likelihood always go up,

99
00:05:20,610 --> 00:05:23,180
meaning your negative 2 Log Likelihood
will always go down.

100
00:05:23,180 --> 00:05:24,640
It will always look better.

101
00:05:24,640 --> 00:05:26,680
But, of course, you can stuff your
parameter with,

102
00:05:26,680 --> 00:05:29,740
your model with too many parameters and
just over fit it.

103
00:05:29,740 --> 00:05:33,930
So the AIC give an explicit penalty for
the number of parameters in your model.

104
00:05:33,930 --> 00:05:36,070
So you're just going to take the negative
two log likelihood and

105
00:05:36,070 --> 00:05:38,350
you're going to add to that 2 times p.

106
00:05:38,350 --> 00:05:40,610
And p is the number of parameters in the
model.

107
00:05:40,610 --> 00:05:43,720
In this case we only have one parameter
that we fit in this model,

108
00:05:43,720 --> 00:05:46,040
just one intercept, so we're just going to
be adding two.

109
00:05:46,040 --> 00:05:47,320
To the negative 2 log likelihood.

110
00:05:47,320 --> 00:05:49,570
So, the negative 2 log likelihood was
101.632.

111
00:05:49,570 --> 00:05:51,565
We add 2 to that.

112
00:05:51,565 --> 00:05:53,500
103.632 is the AIC.

113
00:05:54,710 --> 00:05:57,860
The nice thing about the AIC is because
we've controlled for

114
00:05:57,860 --> 00:06:00,170
the number of parameters, you can take
that and

115
00:06:00,170 --> 00:06:02,810
compare two models that don't necessarily
have to be nested,

116
00:06:02,810 --> 00:06:07,490
and whichever one has the smaller AIC,
would be considered the better model.

117
00:06:07,490 --> 00:06:08,705
And you can see that, you know,

118
00:06:08,705 --> 00:06:10,801
in the SAS output it even tells you
smaller is better.

119
00:06:10,801 --> 00:06:12,244
So it's very helpful so you won't forget.

120
00:06:12,244 --> 00:06:13,974
You want, you actually want the smaller
number here.

121
00:06:15,210 --> 00:06:19,770
And in fact what we tend to use is
something called the AICC

122
00:06:19,770 --> 00:06:21,490
that's a little bit preferred.

123
00:06:21,490 --> 00:06:24,050
You can see they're not very different
here but

124
00:06:24,050 --> 00:06:26,866
the AICC has even an additional penalty
filled in for

125
00:06:26,866 --> 00:06:30,918
the fact that we have a finite sample
size, sometimes a small sample size.

126
00:06:30,918 --> 00:06:35,406
And so you can see that there's a teeny
tiny additional penalty here that the AICC

127
00:06:35,406 --> 00:06:39,180
is slightly bigger then the AIC an
additional penalty here.

128
00:06:39,180 --> 00:06:42,830
So we're going to use these fit statistics
to compare different models.

129
00:06:42,830 --> 00:06:47,260
So I went ahead with those data and then
tit, took the same data and

130
00:06:47,260 --> 00:06:48,320
fit a Weibull model.

131
00:06:48,320 --> 00:06:53,780
And so we're estimating one additional
parameter here and we get our intercepts,

132
00:06:53,780 --> 00:07:01,150
so again the Weibull model As the
intercept plus this additional

133
00:07:03,650 --> 00:07:08,690
parameter for telling us how our hazard is
changing over time.

134
00:07:08,690 --> 00:07:10,190
And you actually get two things in the
output.

135
00:07:10,190 --> 00:07:13,290
First, you actually get something called
the scale and something called a shape.

136
00:07:13,290 --> 00:07:16,830
The shape is just the power that we've
raised t to.

137
00:07:16,830 --> 00:07:19,810
But they also give you the scale, which is
just the reciprocal of the shape.

138
00:07:19,810 --> 00:07:23,380
So what you're seeing in the SAS output,
one is the reciprocal of the other so

139
00:07:23,380 --> 00:07:25,060
it doesn't really matter which one you
look at.

140
00:07:25,060 --> 00:07:27,140
We're probably talk mostly about the scale
parameter, but

141
00:07:27,140 --> 00:07:29,970
that's just the reciprocal of the shape
parameter.

142
00:07:29,970 --> 00:07:31,050
If the scale parameter or

143
00:07:31,050 --> 00:07:34,300
the shape parameter is one, remember this
is like the k I was talking about before,

144
00:07:34,300 --> 00:07:37,890
if that's equal to one then we're back on
an exponential distribution.

145
00:07:37,890 --> 00:07:42,560
You can see that they're very close to one
here, so we're probably, there's probably

146
00:07:42,560 --> 00:07:46,750
nothing to gained by adding an additional
parameter for the Weibull model here.

147
00:07:47,850 --> 00:07:50,600
And you can see that also with the fit
statistics.

148
00:07:50,600 --> 00:07:53,400
So recall that the negative 2 Log
Likelihood for

149
00:07:53,400 --> 00:07:57,930
the exponential model was equal to, I
think, 101.632.

150
00:07:57,930 --> 00:08:01,440
And it got just a teeny bit smaller when
we

151
00:08:01,440 --> 00:08:03,440
added the additional Weibull parameter in
there.

152
00:08:03,440 --> 00:08:04,880
But it's hardly very different.

153
00:08:04,880 --> 00:08:07,710
Remember to, compare two models,

154
00:08:07,710 --> 00:08:12,410
two nested models You can subtract the
negative 2 log likelihood for the,

155
00:08:12,410 --> 00:08:16,270
from the smaller model minus the negative
2 log likelihood for the bigger model.

156
00:08:16,270 --> 00:08:17,890
That should follow a pi square.

157
00:08:17,890 --> 00:08:19,870
Here, that pi square value's going to be
extremely small.

158
00:08:19,870 --> 00:08:22,480
You know that's not going to be
statistically significant, so

159
00:08:22,480 --> 00:08:26,160
we have no evidence that that additional
parameter adds anything to the model.

160
00:08:26,160 --> 00:08:29,000
Or, even easier, you can just look at the
AIC.

161
00:08:29,000 --> 00:08:30,850
Remember, smaller is better.

162
00:08:30,850 --> 00:08:35,050
Well the AIC was 103 point something for
the exponential model, and

163
00:08:35,050 --> 00:08:38,300
same with the AICC was about 103 point
something.

164
00:08:38,300 --> 00:08:40,120
Well here, clearly they're bigger,

165
00:08:40,120 --> 00:08:42,910
105, because we're putting an additional
parameter in the model,

166
00:08:42,910 --> 00:08:46,090
we get that penalty without much gain in
the likelihood.

167
00:08:46,090 --> 00:08:49,850
So this is telling us that probably the
exponential is the best fit here.

168
00:08:49,850 --> 00:08:52,940
And just to show you what we've actually
predicted.

169
00:08:52,940 --> 00:08:54,810
[COUGH] When we do parametric regression,

170
00:08:54,810 --> 00:08:57,270
we actually specify the entire survival
function.

171
00:08:57,270 --> 00:08:58,920
So I've actually gone ahead and plotted
them here.

172
00:08:58,920 --> 00:09:03,708
So here is the survival probability on the
Y-axis.

173
00:09:03,708 --> 00:09:08,340
[NOISE]
And here's time on the X-axis.

174
00:09:08,340 --> 00:09:09,390
And, on the left,

175
00:09:09,390 --> 00:09:13,460
I'm showing you the Weibull model, as used
in the Weibull model.

176
00:09:13,460 --> 00:09:15,600
And on the right, I'm showing you the
exponential model.

177
00:09:15,600 --> 00:09:17,400
You can see that they look very, very
close.

178
00:09:17,400 --> 00:09:19,560
There's hardly any difference between the
two.

179
00:09:19,560 --> 00:09:21,960
and compare those two to the Kaplan Meyer
curve.

180
00:09:21,960 --> 00:09:25,260
They look, they're basically a smooth
version of the Kaplan Meyer curve.

181
00:09:25,260 --> 00:09:27,920
We've assumed a nice, smooth mathematical
function there.

182
00:09:29,460 --> 00:09:31,670
Just for fun, let me show you one more
example.

183
00:09:31,670 --> 00:09:36,060
So remember that we had this data about
longevity in 1669, it wasn't so

184
00:09:36,060 --> 00:09:38,230
good people were dying pretty early.

185
00:09:38,230 --> 00:09:39,220
Well I, just have the,

186
00:09:39,220 --> 00:09:43,090
the curve here I don't have the exact data
of exactly when people died.

187
00:09:43,090 --> 00:09:46,860
But just based on this curve I made up a
data set so,

188
00:09:46,860 --> 00:09:50,720
I know that 36 people died between zero
years and 60 years, so

189
00:09:50,720 --> 00:09:54,090
I just kind of put them somewhere in
between those time points.

190
00:09:54,090 --> 00:09:58,620
So I made up a data set that's based
roughly on this curve here for

191
00:09:58,620 --> 00:10:00,900
100 people time to death in 1669.

192
00:10:00,900 --> 00:10:04,990
So using this little data set, I drew a
Kaplan-Meier curve.

193
00:10:04,990 --> 00:10:08,970
You can see that that looks very similar
to the actual curve that was drawn on

194
00:10:08,970 --> 00:10:09,520
these data.

195
00:10:09,520 --> 00:10:12,510
So here's the Kaplan-Meier curve from
these data I then

196
00:10:12,510 --> 00:10:15,730
plotted the cumulative hazard function to
see if that can give

197
00:10:15,730 --> 00:10:19,730
me some clue about what the hazard
function looks like, and indeed,

198
00:10:19,730 --> 00:10:23,620
the cumulative hazard function here looks
like a pretty solid straight line.

199
00:10:23,620 --> 00:10:25,250
It looks nice and linear, so

200
00:10:25,250 --> 00:10:29,920
that's indicating that probably that this
is well modeled by constant hazard.

201
00:10:29,920 --> 00:10:32,410
That's kind of interesting, because
nowadays, you know,

202
00:10:32,410 --> 00:10:36,260
your hazard of death probably goes up as
you age But back then you

203
00:10:36,260 --> 00:10:39,200
kind of have a constant chance of death
every year, because there were so

204
00:10:39,200 --> 00:10:42,910
many things that could affect young people
as well as old people.

205
00:10:42,910 --> 00:10:45,760
So this appears well modelled by an
exponential model,

206
00:10:45,760 --> 00:10:48,030
because we have a constant hazard.

207
00:10:48,030 --> 00:10:51,580
So if we're to had it fit these data into
an exponential model,

208
00:10:51,580 --> 00:10:55,170
here the result again, there is only one
parameter here I'm just estimating that,

209
00:10:55,170 --> 00:10:57,910
single survival curve for everybody.

210
00:10:57,910 --> 00:11:01,526
And the intercept here came out to be
2.8785.

211
00:11:01,526 --> 00:11:04,480
If I want to say, well what is that
constant hazard?

212
00:11:04,480 --> 00:11:08,070
What was your chance of dying every year
in 1669?

213
00:11:08,070 --> 00:11:14,080
Well, I would just exponentiate negative
of the intercept, so negative 2.8785.

214
00:11:14,080 --> 00:11:18,970
The constant hazard rate was 0.056, so
people

215
00:11:18,970 --> 00:11:24,390
living around that time had a 5% or 6%
chance of dying every year, not so great.

216
00:11:24,390 --> 00:11:27,020
But that's the exponential model that I
fit, and then I went ahead and

217
00:11:27,020 --> 00:11:29,610
since I've, I've, again, set the entire
model.

218
00:11:29,610 --> 00:11:33,130
I can dry out what does that smooth
survival curve look like.

219
00:11:33,130 --> 00:11:33,790
And here it is.

220
00:11:33,790 --> 00:11:36,410
This is the survival curve that comes from
that model.

221
00:11:36,410 --> 00:11:38,310
I get this nice smooth mathematical curve.

222
00:11:38,310 --> 00:11:41,620
And, again, you can see survival was not
very good at that time,

223
00:11:41,620 --> 00:11:43,940
because that curve is dropping off pretty
steeply.

224
00:11:43,940 --> 00:11:50,380
[BLANK_AUDIO]
