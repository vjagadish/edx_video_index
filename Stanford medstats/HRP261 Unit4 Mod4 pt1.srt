1
00:00:05,360 --> 00:00:09,820
In this next module, I'm going to show you
how to evaluate confounding in

2
00:00:09,820 --> 00:00:12,500
the framework of regression analysis.

3
00:00:12,500 --> 00:00:17,270
I'm also going to tell you a little bit
about residual confounding.

4
00:00:17,270 --> 00:00:18,020
So first of all,

5
00:00:18,020 --> 00:00:22,390
it's important to understand that when we
evaluate confounding within regression

6
00:00:22,390 --> 00:00:28,572
models, we need to focus on the effect
sizes, the betas, and not on the p-values.

7
00:00:28,572 --> 00:00:32,730
You almost want to exclusively focus on
those effect sizes.

8
00:00:32,730 --> 00:00:34,910
When we talk about interaction, for

9
00:00:34,910 --> 00:00:36,900
interaction we're actually going to focus
on p-values.

10
00:00:36,900 --> 00:00:39,640
But for confounding, the effect sizes are
what matter.

11
00:00:39,640 --> 00:00:41,340
And here's the rule of thumb.

12
00:00:41,340 --> 00:00:44,396
It's a rule of thumb, so it's not you
know, it's not set in stone.

13
00:00:44,396 --> 00:00:48,390
But a general good rule of thumb is that
you would identify a variable as

14
00:00:48,390 --> 00:00:54,530
a confounder if it changed the beta, the
effect size relating the predictor and

15
00:00:54,530 --> 00:00:57,380
the outcome, the coefficient, by more than
10%.

16
00:00:57,380 --> 00:01:03,110
So if you have a model, where there's, you
do the,

17
00:01:03,110 --> 00:01:05,620
there's a beta for the predictor in the
model.

18
00:01:05,620 --> 00:01:08,860
And then you add the confounder to the
model.

19
00:01:10,950 --> 00:01:16,560
And the beta for the predictor changes by
more than 10% when you add the confounder,

20
00:01:16,560 --> 00:01:19,750
that would be considered, that then this
would be considered a confounder.

21
00:01:19,750 --> 00:01:22,040
If adding the confounder doesn't really
affect the beta for

22
00:01:22,040 --> 00:01:25,700
the predictor, then this third variable
would not be considered a confounder.

23
00:01:25,700 --> 00:01:28,300
Now just know that this rule of thumb
doesn't really work very well if

24
00:01:28,300 --> 00:01:29,560
you're talking about betas close to 0.

25
00:01:29,560 --> 00:01:32,695
So if the beta between the predictor and
the outcome is already close to

26
00:01:32,695 --> 00:01:37,950
0 it's very easy to change by 10% if
you're already close to 0.

27
00:01:37,950 --> 00:01:40,830
So if you're at 0.01, 10% of 0.01 is very
small.

28
00:01:40,830 --> 00:01:42,640
So this really works for

29
00:01:42,640 --> 00:01:46,970
talking about things that have a beta
that's not the null value.

30
00:01:46,970 --> 00:01:48,990
When you get to betas close to 0,

31
00:01:48,990 --> 00:01:50,520
you're going to have to use your judgment
a little bit.

32
00:01:50,520 --> 00:01:53,200
It's not, the 10% rule is not going to
really work there.

33
00:01:56,206 --> 00:02:00,180
The example that I'm going to use to start
with here is this example of

34
00:02:00,180 --> 00:02:03,070
gender bias in admissions at Berkeley.

35
00:02:03,070 --> 00:02:06,520
So this example was from some data, this
was

36
00:02:06,520 --> 00:02:11,070
collected in the early 1970s at Berkeley
and this was looking at whether or

37
00:02:11,070 --> 00:02:15,384
not there was bias against females in
graduate school admissions.

38
00:02:15,384 --> 00:02:19,110
And somebody, just looking at the crude
data overall,

39
00:02:19,110 --> 00:02:22,020
concluded that there was indeed a bias
against women.

40
00:02:22,020 --> 00:02:25,530
And here are just the data, presented in a
two by two table.

41
00:02:25,530 --> 00:02:28,290
Fewer women were applying than men but

42
00:02:28,290 --> 00:02:31,220
women were also getting denied admissions
at a higher rate.

43
00:02:31,220 --> 00:02:35,150
If you calculate the risk ratio, women
were 25% more likely to

44
00:02:35,150 --> 00:02:38,460
be denied admissions to Berkeley Graduate
School than men.

45
00:02:38,460 --> 00:02:42,178
And that was indeed statistically
significant.

46
00:02:42,178 --> 00:02:44,610
However, that's the crude data, but

47
00:02:44,610 --> 00:02:47,500
there is an important confounder lurking
in the background here.

48
00:02:47,500 --> 00:02:50,300
So let's start by just running the model
where we put

49
00:02:50,300 --> 00:02:54,480
gender into the logistic regression model
as the only predictor.

50
00:02:54,480 --> 00:02:57,079
So here is the model with only age in the
model.

51
00:02:58,300 --> 00:03:02,410
So if we just say, does gender predict
whether or not you're going to

52
00:03:02,410 --> 00:03:07,880
be rejected from Berkeley Graduate School,
indeed you can see that indeed,

53
00:03:07,880 --> 00:03:13,760
there's a very strong relationship between
your gender and being rejected.

54
00:03:13,760 --> 00:03:16,160
And the p-value is very strong here.

55
00:03:16,160 --> 00:03:19,895
We have a very positive association gender
here.

56
00:03:19,895 --> 00:03:22,920
1 is equal to female, I should clarify.

57
00:03:22,920 --> 00:03:25,980
So a positive relationship means that
women are more likely to be rejected.

58
00:03:25,980 --> 00:03:28,330
The outcome here is being rejected.

59
00:03:28,330 --> 00:03:31,830
The risk ratio as we saw in the last slide
was 1.25.

60
00:03:31,830 --> 00:03:33,150
We're doing logistic regression.

61
00:03:33,150 --> 00:03:35,460
So we get out an odds ratio, not a risk
ratio.

62
00:03:35,460 --> 00:03:37,800
So the odds ratio comes out to be 1.84.

63
00:03:37,800 --> 00:03:38,720
Of course,

64
00:03:38,720 --> 00:03:42,040
that's the increase in your odds of your
being rejected, not your risk.

65
00:03:43,240 --> 00:03:45,130
But anyway, with nothing else in the
model,

66
00:03:45,130 --> 00:03:49,310
age looks very predictive, but there's a
confounder here.

67
00:03:49,310 --> 00:03:53,710
So the confounder here is that, especially
back in the 1970's, women and

68
00:03:53,710 --> 00:03:56,670
men applied to very different departments.

69
00:03:56,670 --> 00:04:02,060
So women, it turned out, were applying To
departments that were very hard

70
00:04:02,060 --> 00:04:07,770
to get into in high numbers, whereas men
tended to apply to departments that for

71
00:04:07,770 --> 00:04:10,370
whatever reason were easier to get into.

72
00:04:10,370 --> 00:04:14,770
If you looked within a particular
department, the admissions rates for

73
00:04:14,770 --> 00:04:17,170
men and women are actually very similar.

74
00:04:17,170 --> 00:04:20,250
It's just that there's a confounder here,
women are applying to things,

75
00:04:20,250 --> 00:04:23,410
to departments that happen to be harder to
get into.

76
00:04:23,410 --> 00:04:25,500
Men are finding easier ones to get into.

77
00:04:25,500 --> 00:04:26,550
So what we're going to do is,

78
00:04:26,550 --> 00:04:30,140
we're going to adjust for program in our
logistic regression model.

79
00:04:30,140 --> 00:04:32,090
Program or department.

80
00:04:32,090 --> 00:04:36,240
And the, the programs are labeled as A, B,
C, D, E, and F.

81
00:04:36,240 --> 00:04:38,628
There were six programs in the data set,

82
00:04:38,628 --> 00:04:42,440
six programs were represented, essentially
departments.

83
00:04:42,440 --> 00:04:47,670
This is a categorical variable, so there's
program A, B, C, D, E, and F.

84
00:04:47,670 --> 00:04:51,150
How do we represent categorical predictors
in regression?

85
00:04:51,150 --> 00:04:54,800
We're going to have to dummy code them, so
that's what's represented here.

86
00:04:54,800 --> 00:04:58,600
There are, there is six categories, six
programs.

87
00:04:58,600 --> 00:05:00,550
Then we're going to get five betas in the
model.

88
00:05:00,550 --> 00:05:01,750
Why five betas?

89
00:05:01,750 --> 00:05:06,860
Because the reference group is going to be
absorbed into the intercept.

90
00:05:06,860 --> 00:05:10,210
So the reference group here turned out to
be program F.

91
00:05:10,210 --> 00:05:14,450
That just happens to be because the
default in my the computer program I

92
00:05:14,450 --> 00:05:17,880
was using, SAS, the default is to use the
alphabetically last

93
00:05:19,600 --> 00:05:21,690
value as the reference group.

94
00:05:21,690 --> 00:05:22,710
Of course I could change that,

95
00:05:22,710 --> 00:05:26,020
but it's kind of arbitrary which one I
pick to be the reference group here.

96
00:05:26,020 --> 00:05:28,780
So come, this is going to tell me that
indeed there

97
00:05:28,780 --> 00:05:30,742
is a strong effect for program.

98
00:05:30,742 --> 00:05:35,120
These p-values represent the comparison
between these individual programs with

99
00:05:35,120 --> 00:05:37,930
program f, but certainly at least compared
to program f,

100
00:05:37,930 --> 00:05:41,280
these other programs all have a very
strong difference from program f.

101
00:05:42,300 --> 00:05:45,840
There seems to be an important effect here
of program.

102
00:05:45,840 --> 00:05:50,240
But here's what we want to focus on in the
cost of adjusting for confounding.

103
00:05:50,240 --> 00:05:53,670
So notice what happened to the estimate
from, for gender.

104
00:05:53,670 --> 00:05:57,600
The estimate for gender, the beta for
gender was about .60,

105
00:05:57,600 --> 00:06:03,260
maybe .61 before we adjusted for program.

106
00:06:03,260 --> 00:06:05,640
After we adjust for program,

107
00:06:05,640 --> 00:06:10,070
that beta coefficient actually becomes
slightly negative, it goes down to 0.099.

108
00:06:10,070 --> 00:06:14,680
That is clearly more than a 10% change.

109
00:06:14,680 --> 00:06:20,410
So program would absolutely be a
confounder in this context, and

110
00:06:20,410 --> 00:06:24,860
it, clearly we need to keep program in the
model, because when we adjust for program,

111
00:06:24,860 --> 00:06:29,300
it completely changes the relationship
between gender and

112
00:06:29,300 --> 00:06:31,510
admissions to graduate school at Berkeley.

113
00:06:32,530 --> 00:06:36,030
So the, again our general rule of thumb
for confounding is,

114
00:06:36,030 --> 00:06:37,670
if we change the beta by more than 10%,

115
00:06:37,670 --> 00:06:41,139
this, this clearly does that, then we're
going to consider it a confounder.

116
00:06:42,280 --> 00:06:44,248
So program is a confounder in this
context.

117
00:06:44,248 --> 00:06:47,400
Now, I'm going to go one step further
here.

118
00:06:47,400 --> 00:06:51,320
It also happens to be that program is not
only a confounder, but

119
00:06:51,320 --> 00:06:53,910
it's also a effect modifier.

120
00:06:53,910 --> 00:06:55,750
So it happens to be in this data set that,

121
00:06:55,750 --> 00:06:59,110
program, you can have variables that are
both confounders and effect modifiers.

122
00:06:59,110 --> 00:07:01,420
In fact, program is also an effect
modifier, and

123
00:07:01,420 --> 00:07:03,250
I just want to show you that here.

124
00:07:03,250 --> 00:07:06,190
We're going to talk about interactions
very soon.

125
00:07:06,190 --> 00:07:07,720
So I want to just kind of set up that
idea.

126
00:07:07,720 --> 00:07:11,670
How would I test for an interaction
between gender and program?

127
00:07:11,670 --> 00:07:15,550
Well, you can see the model is getting
rather complex, but

128
00:07:15,550 --> 00:07:20,850
all we have to do to test for interaction
is to multiply gender times program.

129
00:07:20,850 --> 00:07:22,250
That's how we test for interaction.

130
00:07:22,250 --> 00:07:24,400
That's called an interacting variable.

131
00:07:24,400 --> 00:07:27,820
Now, because there are five programs,
well,

132
00:07:27,820 --> 00:07:31,880
there's six programs and five programs
represented as betas in the model.

133
00:07:31,880 --> 00:07:34,720
What we end up with is that, when we add
an interaction term here,

134
00:07:34,720 --> 00:07:37,630
because we are interacting with a
categorical variable,

135
00:07:37,630 --> 00:07:40,130
we actually end up with five betas for
interaction.

136
00:07:40,130 --> 00:07:43,370
So now we've got five betas for the main
effect for program, and

137
00:07:43,370 --> 00:07:45,670
five betas for interaction.

138
00:07:45,670 --> 00:07:49,550
So what you can see here is that, in fact,
some of the interaction terms,

139
00:07:49,550 --> 00:07:53,430
specifically term, the, the interaction
between gender and

140
00:07:53,430 --> 00:07:58,370
program A, that one came out to be
statistically significant.

141
00:07:58,370 --> 00:08:00,180
So it turns out that, in this case,

142
00:08:00,180 --> 00:08:06,580
that program A actually is more likely to
admit women than they are to admit men.

143
00:08:06,580 --> 00:08:09,590
So the other programs were all pretty
even, but

144
00:08:09,590 --> 00:08:13,870
program A in particular had a higher admit
rate for, for women.

145
00:08:13,870 --> 00:08:15,770
So we have some interaction going on here.

146
00:08:15,770 --> 00:08:18,750
So I just want to point out that you can
also have a,

147
00:08:18,750 --> 00:08:22,750
something that's both a confounder and an
interaction, an interacting variable.

148
00:08:22,750 --> 00:08:27,520
And I want to show you how you might want
to use the likelihood ratio test here

149
00:08:27,520 --> 00:08:32,310
when talking in particular about the
interaction between gender and program.

150
00:08:32,310 --> 00:08:34,200
You might just want to report this p-value
and

151
00:08:34,200 --> 00:08:37,180
say, hey, program A is different from the
rest.

152
00:08:37,180 --> 00:08:41,290
But sometimes we also like to just report
an overall p-value for

153
00:08:41,290 --> 00:08:42,890
the interaction between gender and

154
00:08:42,890 --> 00:08:47,610
program and not get into the fine level of
detail of individual categories.

155
00:08:47,610 --> 00:08:50,410
So sometimes with a categorical variable,
we want to just report what's called a p

156
00:08:50,410 --> 00:08:53,160
for interaction, which is just to say,
overall,

157
00:08:53,160 --> 00:08:57,090
is there any interaction between gender
and the categorical variable of program.

158
00:08:57,090 --> 00:09:00,270
And this is where we can use a likelihood
ratio test,

159
00:09:00,270 --> 00:09:01,850
which we've talked about already.

160
00:09:01,850 --> 00:09:04,570
This is a good use of the likelihood ratio
test.

161
00:09:04,570 --> 00:09:09,350
If you wanted overall p for interaction,
what you can do is compare a model that

162
00:09:09,350 --> 00:09:13,570
contains the interactions between gender
and

163
00:09:13,570 --> 00:09:16,489
program to a model that doesn't have those
interactions.

164
00:09:17,820 --> 00:09:20,540
And, that you can compare with the
likelihood ratio test,

165
00:09:20,540 --> 00:09:24,820
because it's actually a group of betas,
that you have to compare it once.

166
00:09:24,820 --> 00:09:28,580
So I ran the model with the interaction
term, that's the model I just showed you,

167
00:09:28,580 --> 00:09:33,845
my negative 2 log likelihood here turned
out to be 4856.

168
00:09:34,880 --> 00:09:38,080
When I then take the interaction term out
and I run the model

169
00:09:38,080 --> 00:09:41,810
without the interaction, my likelihood, my
negative 2 log likelihood goes up.

170
00:09:41,810 --> 00:09:44,370
Remember, this is the reduced model.

171
00:09:44,370 --> 00:09:46,660
And this is the full model.

172
00:09:46,660 --> 00:09:50,360
The reduced model will have a smaller
likelihood, but

173
00:09:50,360 --> 00:09:53,160
therefore a bigger negative 2 log
likelihood, and

174
00:09:53,160 --> 00:09:56,410
the full model will have a smaller value
for negative 2 log likelihood.

175
00:09:57,630 --> 00:10:01,370
Remember that if we wanted to say whether
or not adding that interaction term,

176
00:10:01,370 --> 00:10:04,700
which comprises five variables, whether or
not that

177
00:10:04,700 --> 00:10:10,250
adds anything significantly to the model,
we can do that by using a chi square.

178
00:10:10,250 --> 00:10:12,580
This is a chi square with five degrees of
freedom,

179
00:10:12,580 --> 00:10:16,820
because we have five interaction terms in
the model.

180
00:10:16,820 --> 00:10:20,340
It's a six level categorical variable, we
end up with five terms in the model.

181
00:10:20,340 --> 00:10:23,880
So we have a chi square with five degrees
of freedom, the negative 2

182
00:10:23,880 --> 00:10:26,910
log likelihood for the reduced model is
this, for the full model is this.

183
00:10:26,910 --> 00:10:28,630
We just subtract those,

184
00:10:28,630 --> 00:10:33,570
we get that we have a chi square with five
degrees of freedom with a value of 20.

185
00:10:33,570 --> 00:10:37,140
We can then look to see whether or not
that comes out to be significant.

186
00:10:37,140 --> 00:10:40,154
Indeed, I'll show you in a minute where
got this from.

187
00:10:40,154 --> 00:10:43,020
But the p-value here comes out to be
significant.

188
00:10:43,020 --> 00:10:44,780
So the expected value here is 5, and

189
00:10:44,780 --> 00:10:47,380
the p value does indeed come out to be
significant.

190
00:10:47,380 --> 00:10:50,420
So that's telling us that the p value for
interaction overall,

191
00:10:50,420 --> 00:10:54,740
adding the interacting term to our model
improves our model.

192
00:10:54,740 --> 00:10:59,310
So, in case you don't want to get into the
fine level of details of which particular

193
00:10:59,310 --> 00:11:03,300
interacting terms came out significant
when you have a categorical variable,

194
00:11:03,300 --> 00:11:05,320
this is a way to just report an overall p
for

195
00:11:05,320 --> 00:11:08,510
interaction for that entire categorical
variable.

196
00:11:08,510 --> 00:11:13,090
Just as a quick reminder, to get the, the
p-values that go with a chi square test,

197
00:11:13,090 --> 00:11:15,740
you can plug in to one of these online
calculators.

198
00:11:15,740 --> 00:11:16,920
I did that here.

199
00:11:16,920 --> 00:11:20,350
The computer told me that my p-value here
is 0.0013.

200
00:11:20,350 --> 00:11:24,540
I did have to do this legwork on my own.

201
00:11:24,540 --> 00:11:29,210
I actually had to run the model in my
computer program, get out the negative 2

202
00:11:29,210 --> 00:11:34,360
log likelihood for one model, and then run
the model with or the interaction term,

203
00:11:34,360 --> 00:11:37,570
get the negative 2 log likelihoods head
out for that, and then I had to

204
00:11:37,570 --> 00:11:42,560
subtract those negative 2 log likelihoods
myself, and go and get the p-value myself.

205
00:11:42,560 --> 00:11:45,930
So sometimes you actually have to do that
legwork by hand, it's not automatically

206
00:11:45,930 --> 00:11:51,410
built into your computer package to do the
comparison of a reduced and full model.

207
00:11:51,410 --> 00:11:54,210
The automatic one that the computer
package will give you is

208
00:11:54,210 --> 00:11:57,120
the intercept-only model compared with a
full model.

209
00:11:57,120 --> 00:11:59,620
This is comparing something slightly
different, so

210
00:11:59,620 --> 00:12:01,850
again, I had to do that legwork on my own.

211
00:12:01,850 --> 00:12:06,059
'Kay, just want to give you a second
example of confounding here just to

212
00:12:06,059 --> 00:12:09,578
throw out another example, so this was
from a study, well,

213
00:12:09,578 --> 00:12:13,235
that was looking at whether or not
maternal smoking was going to

214
00:12:13,235 --> 00:12:16,870
change your risk of having a baby with
Down's Syndrome.

215
00:12:16,870 --> 00:12:20,420
This was the American Journal of
Epidemiology a number of years ago, but

216
00:12:20,420 --> 00:12:24,430
I really like this example, because it's
going to also illustrate the concept of

217
00:12:24,430 --> 00:12:27,490
residual confounding, which we'll get to
in a minute.

218
00:12:27,490 --> 00:12:32,100
But the crude odds ratio here, so when you
just look at,

219
00:12:32,100 --> 00:12:35,120
the outcome here is Down Syndrome, the
baby has Down Syndrome or not.

220
00:12:35,120 --> 00:12:38,110
The predictor here is whether or not the
mother smoked during pregnancy.

221
00:12:38,110 --> 00:12:41,910
And you just look at the crude odds ratio,
It indeed comes out to be significant.

222
00:12:41,910 --> 00:12:43,350
This confidence interval does not
correspond.

223
00:12:43,350 --> 00:12:45,860
It's actually significant in the
protective direction.

224
00:12:45,860 --> 00:12:49,240
So it looks like, if you don't adjust for
any confounders,

225
00:12:49,240 --> 00:12:54,410
that maternal smoking reduces your risk of
getting Down Syndrome.

226
00:12:54,410 --> 00:12:59,150
Now, it's very hard to come up with the
biological story here that that

227
00:12:59,150 --> 00:13:00,390
could somehow be causal.

228
00:13:00,390 --> 00:13:04,450
We know that in general, smoking tends to
cause DNA damage.

229
00:13:04,450 --> 00:13:07,320
DNA damage would relate to Down syndrome.

230
00:13:07,320 --> 00:13:09,320
Perhaps you could come up with a causal
story here,

231
00:13:09,320 --> 00:13:12,890
but it's more likely that this is due to
confounding, and indeed it was.

232
00:13:12,890 --> 00:13:14,520
So that's the crude odds ratio.

233
00:13:14,520 --> 00:13:16,750
But then, think about potential
confounders here.

234
00:13:16,750 --> 00:13:20,890
The big confounder here, the most
important one, turns out to be age.

235
00:13:20,890 --> 00:13:24,330
And that's I like this paper, because they
did a lot of nice graphics, and

236
00:13:24,330 --> 00:13:26,470
that's well pictured here.

237
00:13:26,470 --> 00:13:30,970
So here it's showing you, on the left hand
figure, this is showing you

238
00:13:30,970 --> 00:13:35,820
the relationship between maternal age and
the rates of Down syndrome.

239
00:13:35,820 --> 00:13:39,680
So as you all probably know, as maternal
age goes up, especially as you

240
00:13:39,680 --> 00:13:44,390
get to 40 and above, there's a really big
increase in the risk of Down syndrome.

241
00:13:44,390 --> 00:13:47,220
So you can see that there's a strong
relationship clearly between age

242
00:13:47,220 --> 00:13:48,550
and Down syndrome.

243
00:13:48,550 --> 00:13:52,000
You turn over to age and smoking though,
the predictor here,

244
00:13:52,000 --> 00:13:55,250
age is also related to the predictor, and
it's a pretty strong relationship, so

245
00:13:55,250 --> 00:14:00,420
younger women smoke much more, are much
more likely to smoke than older women.

246
00:14:00,420 --> 00:14:05,800
So as you get older you're also more,
you're like, more likely to have

247
00:14:05,800 --> 00:14:08,420
a baby with Down syndrome, but you're also
less likely to smoke.

248
00:14:08,420 --> 00:14:14,732
Hence, we end up with a spurious
association between maternal smoking and

249
00:14:14,732 --> 00:14:16,010
Down syndrome.

250
00:14:16,010 --> 00:14:19,930
So, what the authors did here was then to
adjust for various things.

251
00:14:19,930 --> 00:14:21,760
So here they're adjusting for various
things.

252
00:14:21,760 --> 00:14:22,560
I'm just going to focus for

253
00:14:22,560 --> 00:14:26,850
the moment on the most adjusted model,
that's the one at the end.

254
00:14:26,850 --> 00:14:29,760
So when they adjusted for everything they
needed to adjust for, the most

255
00:14:29,760 --> 00:14:34,260
important one being age, the crude odds,
the odds ratio now is exactly 1.0.

256
00:14:34,260 --> 00:14:37,775
So there is no association between
maternal smoking and

257
00:14:37,775 --> 00:14:40,920
Down syndrome once you've accounted for
the confounders here.

258
00:14:40,920 --> 00:14:44,810
And you can see that we started with an
odds ratio of 0.8.

259
00:14:44,810 --> 00:14:48,810
We have changed clearly by more than 10%.

260
00:14:48,810 --> 00:14:50,570
We now are at an odds ratio of 1.0.

261
00:14:50,570 --> 00:14:53,105
And again, I pay attention only to the
effect size and

262
00:14:53,105 --> 00:14:55,100
not to the statistical significance here.

263
00:14:57,220 --> 00:15:00,860
I usually like to apply that 10% rule to
the betas rather then the odds ratio.

264
00:15:00,860 --> 00:15:04,930
So if you wanted to, you could covert the
ln of 0.8 here.

265
00:15:04,930 --> 00:15:08,200
And then, of course, ln of 1 would be 0,
and you could compare whatever this

266
00:15:08,200 --> 00:15:11,650
value turns out to be to 0] rather then
comparing the odds ratios.

267
00:15:11,650 --> 00:15:15,080
But clearly we have a major change in the
effect size here.

268
00:15:15,080 --> 00:15:19,340
So that's how we evaluate confounding in
the context of regression.
