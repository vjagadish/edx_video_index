1
00:00:00,000 --> 00:00:05,360
[BLANK_AUDIO]

2
00:00:05,360 --> 00:00:09,810
In this next module, I'm going to show you
how to evaluate confounding in

3
00:00:09,810 --> 00:00:12,490
the framework of regression analysis.

4
00:00:12,490 --> 00:00:16,360
I'm also going to tell you a little bit
about residual confounding.

5
00:00:17,470 --> 00:00:18,010
So first of all,

6
00:00:18,010 --> 00:00:22,380
it's important to understand that when we
evaluate confounding within regression

7
00:00:22,380 --> 00:00:28,360
models we need to focus on the effect
sizes, the betas and not on the p-values.

8
00:00:28,360 --> 00:00:32,710
You almost want to exclusively focus on
those effect sizes.

9
00:00:32,710 --> 00:00:36,090
When we talk about interaction, for
interaction we are actually going to

10
00:00:36,090 --> 00:00:39,370
focus on p-values but for confounding the
effect sizes are what matter.

11
00:00:39,370 --> 00:00:41,370
And here's the rule of thumb.

12
00:00:41,370 --> 00:00:44,390
The rule of thumb is not set in stone.

13
00:00:44,390 --> 00:00:48,390
But a general good rule of thumb is that
you would identify a variable as

14
00:00:48,390 --> 00:00:53,410
a confounder if it changed the beta, the
effect size,

15
00:00:53,410 --> 00:00:58,250
relating the predictor and the outcome,
the coefficient, by more than 10%.

16
00:00:58,250 --> 00:01:03,110
So if you have a model where there's, you
know,

17
00:01:03,110 --> 00:01:05,600
there's a beta for the predictor in the
model.

18
00:01:05,600 --> 00:01:11,050
And then you add the confounder to the
model, and

19
00:01:11,050 --> 00:01:14,640
the beta for the predictor changes by more
than 10%.

20
00:01:14,640 --> 00:01:18,170
When you add the confounder, that would be
considered that,

21
00:01:18,170 --> 00:01:19,730
then this would be considered a
confounder.

22
00:01:19,730 --> 00:01:22,810
If adding the confounder doesn't really
affect the beta for the predictor.

23
00:01:22,810 --> 00:01:25,680
Then this third variable would not be
considered a confounder.

24
00:01:25,680 --> 00:01:28,286
Now I'll just note that this rule of thumb
doesn't really work very well if

25
00:01:28,286 --> 00:01:29,540
you're talking about betas close to zero.

26
00:01:29,540 --> 00:01:32,700
So if the beta between the predictor and
the outcome is already close to

27
00:01:32,700 --> 00:01:37,210
zero it's very easy to change by 10% if
your already close to zero.

28
00:01:37,210 --> 00:01:40,850
So if your at 0.01 10% of 0.01 is very
small.

29
00:01:40,850 --> 00:01:42,620
So this really works for

30
00:01:42,620 --> 00:01:46,950
talking about things that have a beta
that's not the null value.

31
00:01:46,950 --> 00:01:49,090
When you get to betas close to zero,

32
00:01:49,090 --> 00:01:50,520
you are going to have use your judgement a
little bit.

33
00:01:50,520 --> 00:01:53,140
It's not, the 10% rule is not going to
work there.

34
00:01:53,140 --> 00:01:57,640
The example that I'm going to use to start
with here is

35
00:01:57,640 --> 00:02:03,040
this example of gender bias in admissions
are Berkeley.

36
00:02:03,040 --> 00:02:06,240
So this example was from some data.

37
00:02:06,240 --> 00:02:10,760
This was collected in the early 1970s at
Berkeley and this was looking at

38
00:02:10,760 --> 00:02:15,610
whether or not there was bias against
females in graduate school admissions.

39
00:02:15,610 --> 00:02:19,840
And somebody just looking at the crude
data overall concluded that

40
00:02:19,840 --> 00:02:22,020
there was indeed a bias against women.

41
00:02:22,020 --> 00:02:25,530
And here are just the data presented to
the two by two table.

42
00:02:25,530 --> 00:02:28,110
Fewer women were applying than men.

43
00:02:28,110 --> 00:02:31,220
But women were also getting denied
admissions at a higher rate.

44
00:02:31,220 --> 00:02:35,150
If you calculate the risk ratio, women
were 25% more likely to

45
00:02:35,150 --> 00:02:38,450
be denied admissions to Berkeley gradate
school than men.

46
00:02:38,450 --> 00:02:42,168
And that was, indeed statistically
significant.

47
00:02:42,168 --> 00:02:44,590
However, that's the crude data, but

48
00:02:44,590 --> 00:02:47,480
there is an important confounder lurking
in the background here.

49
00:02:47,480 --> 00:02:51,150
So let's start by just running the model
where we put gender in to

50
00:02:51,150 --> 00:02:54,450
the logistic regression model as the only
predictor.

51
00:02:54,450 --> 00:02:57,059
So here is the model with only age in the
model.

52
00:02:58,240 --> 00:03:01,890
So if we just say, does gender predict
whether or

53
00:03:01,890 --> 00:03:05,290
not you're going to be rejected from
Berkeley graduate school.

54
00:03:05,290 --> 00:03:07,850
Indeed, you can see that indeed,

55
00:03:07,850 --> 00:03:13,730
there's a very strong relationship between
your gender, and being rejected.

56
00:03:13,730 --> 00:03:16,130
And the p-value is very strong here.

57
00:03:16,130 --> 00:03:19,960
We have a very positive association with
gender here.

58
00:03:19,960 --> 00:03:21,490
One is equal to female.

59
00:03:21,490 --> 00:03:24,460
I should clarify, so a positive
relationship means that

60
00:03:24,460 --> 00:03:27,350
women are more likely to be rejected, the
outcome here is being rejected.

61
00:03:28,350 --> 00:03:32,020
The risk ratio as we saw in the last time
was 1.25.

62
00:03:32,020 --> 00:03:35,460
We are doing logistic regression, we get
out an odds ratio, not a risk ratio.

63
00:03:35,460 --> 00:03:37,820
So the odds ratio comes to be 1.84.

64
00:03:37,820 --> 00:03:42,262
Of course that's the increase in your odds
of being rejected, not your risk.

65
00:03:42,262 --> 00:03:46,300
But anyway, with nothing else in the model
age looks very predictive.

66
00:03:47,720 --> 00:03:49,280
But there's a confounder here.

67
00:03:49,280 --> 00:03:53,630
So the confounder here is that especially
back in the 1970s women and

68
00:03:53,630 --> 00:03:56,640
men applied to very different departments.

69
00:03:56,640 --> 00:04:01,350
So women it turned out where applying to
departments that were

70
00:04:01,350 --> 00:04:05,130
very hard to get into in high numbers.

71
00:04:05,130 --> 00:04:07,740
Whereas men tended to apply to departments
that for

72
00:04:07,740 --> 00:04:10,380
whatever reason were easier to get into.

73
00:04:10,380 --> 00:04:14,720
If you looked within a particular
department the admissions rates for

74
00:04:14,720 --> 00:04:17,140
men and women were actually very similar.

75
00:04:17,140 --> 00:04:20,230
It's just that there's a confounder here,
women are applying to things,

76
00:04:20,230 --> 00:04:23,390
to departments that happen to be hard to
get into

77
00:04:23,390 --> 00:04:25,480
Men are applying easier ones to get into.

78
00:04:25,480 --> 00:04:27,470
So what we are going to do is we are going
to adjust for

79
00:04:27,470 --> 00:04:30,145
program in our logistic regression model.

80
00:04:30,145 --> 00:04:36,230
Program or department, and the programs
labeled as A B C D E and F.

81
00:04:36,230 --> 00:04:37,880
There were six programs.

82
00:04:37,880 --> 00:04:40,600
In the data sets six programs were
represented.

83
00:04:40,600 --> 00:04:42,410
Essentially departments.

84
00:04:42,410 --> 00:04:44,470
This is a categorical variable.

85
00:04:44,470 --> 00:04:47,650
So there's program A, B, C, D, E and F.

86
00:04:47,650 --> 00:04:51,130
How do we represent categorical predictors
in regression?

87
00:04:51,130 --> 00:04:54,810
We're going to have to dummy code them, so
that's what's represented here.

88
00:04:54,810 --> 00:04:58,600
There are, there was six categories, six
programs.

89
00:04:58,600 --> 00:05:00,540
Then we're going to get five betas in the
model.

90
00:05:00,540 --> 00:05:01,750
Why five betas?

91
00:05:01,750 --> 00:05:06,880
Because the reference group is going to be
absorbed into the intercept.

92
00:05:06,880 --> 00:05:10,190
So, the reference group here turned out to
be program F.

93
00:05:10,190 --> 00:05:14,430
That just happens to be because the
default in my the computer program I

94
00:05:14,430 --> 00:05:17,860
was using, SAS, the default is to use the
alphabetically last

95
00:05:19,570 --> 00:05:21,670
value as the reference group.

96
00:05:21,670 --> 00:05:22,690
Of course I did change that,

97
00:05:22,690 --> 00:05:26,020
but it's kind of arbitrary, which one I
pick to be the reference group here.

98
00:05:26,020 --> 00:05:29,012
So come, this is going to tell me that
indeed there is

99
00:05:29,012 --> 00:05:30,750
a strong effect for program.

100
00:05:30,750 --> 00:05:35,020
These p-values represent the comparison
between these individual programs with

101
00:05:35,020 --> 00:05:36,030
program F.

102
00:05:36,030 --> 00:05:37,719
But certainly at least compared to program
F,

103
00:05:37,719 --> 00:05:41,245
these other programs both have a very
strong difference from program F.

104
00:05:42,310 --> 00:05:45,840
There seems to be an important effect here
of program.

105
00:05:45,840 --> 00:05:50,240
But here's what we want to focus on in the
concept of adjusting for confounding.

106
00:05:50,240 --> 00:05:53,680
So notice what happened the estimate from,
for gender.

107
00:05:53,680 --> 00:05:57,736
The estimate for gender, the beta for
gender was about 0.60,

108
00:05:57,736 --> 00:06:03,240
maybe 0.61, before we adjusted for
program.

109
00:06:03,240 --> 00:06:07,140
After we adjust for program that data
coefficient actually

110
00:06:07,140 --> 00:06:10,050
becomes slightly negative, it goes down to
0.099.

111
00:06:10,050 --> 00:06:14,970
That is clearly more than a ten percent
change So

112
00:06:14,970 --> 00:06:19,540
program would absolutely be a confounder
in this context.

113
00:06:19,540 --> 00:06:24,350
And, it, clearly we need to keep program
in the model, because when we adjust for

114
00:06:24,350 --> 00:06:29,290
program it completely changes the
relationship between gender and

115
00:06:29,290 --> 00:06:31,490
admissions to graduate school at Berkeley.

116
00:06:32,500 --> 00:06:35,310
So, there are, again our general rule of
thumb for

117
00:06:35,310 --> 00:06:37,650
confounding is if we changed the data by
more than 10%,

118
00:06:37,650 --> 00:06:41,146
this, this clearly does that, then we're
going to consider it a confounder.

119
00:06:42,290 --> 00:06:44,240
So program is a confounder in this
context.

120
00:06:45,420 --> 00:06:47,390
Now, I want to go one step further here.

121
00:06:47,390 --> 00:06:51,330
It also happens to be that program is not
only a confounder, but

122
00:06:51,330 --> 00:06:53,920
it's also an effect modifier.

123
00:06:53,920 --> 00:06:56,230
So it happens to be in this data set that
the program.

124
00:06:56,230 --> 00:06:59,110
You can have variables that are both
confounders and effect modifiers.

125
00:06:59,110 --> 00:07:01,400
In fact, program is also an effect
modifier and

126
00:07:01,400 --> 00:07:02,908
I just want to show you that here.

127
00:07:02,908 --> 00:07:06,314
We're going to talk about interactions
very soon, so

128
00:07:06,314 --> 00:07:07,700
I just want to kind of set up that idea.

129
00:07:07,700 --> 00:07:11,650
How would I test for an interaction
between gender and program?

130
00:07:11,650 --> 00:07:15,530
Well you can see that the model is getting
rather complex, but

131
00:07:15,530 --> 00:07:20,820
all we have to do to test for interaction
is to multiply gender times program.

132
00:07:20,820 --> 00:07:22,230
That's how we test for interaction.

133
00:07:22,230 --> 00:07:24,380
That's called an interacting variable.

134
00:07:24,380 --> 00:07:27,830
Now, because there are five programs.

135
00:07:27,830 --> 00:07:29,450
Well, there's six programs, but

136
00:07:29,450 --> 00:07:31,753
five programs represented as betas in the
model.

137
00:07:31,753 --> 00:07:34,774
What we end up with is that when we add an
interaction term here,

138
00:07:34,774 --> 00:07:37,567
because we are interacting with a
categorical variable,

139
00:07:37,567 --> 00:07:40,120
we actually end up with five betas for
interaction.

140
00:07:40,120 --> 00:07:43,360
So now we've got five betas for the main
effect for program, and

141
00:07:43,360 --> 00:07:45,630
five betas for interaction.

142
00:07:45,630 --> 00:07:49,480
So what you can see here is that, in fact,
some of the interaction terms,

143
00:07:49,480 --> 00:07:53,400
specifically term, the interaction between
gender and

144
00:07:53,400 --> 00:07:58,340
program A, that one came out to be
statistically significant.

145
00:07:58,340 --> 00:08:01,940
So it turns out that in this case that
program A actually is

146
00:08:01,940 --> 00:08:06,580
more likely to admit women than they are
to admit men.

147
00:08:06,580 --> 00:08:09,600
So the other programs were all pretty
even, but

148
00:08:09,600 --> 00:08:13,860
program A in particular had a higher admit
rate for, for women.

149
00:08:13,860 --> 00:08:15,780
So we have some interaction going on here.

150
00:08:15,780 --> 00:08:19,200
So I just want to point out that you can
also have something that's

151
00:08:19,200 --> 00:08:23,070
both a confounder and a interaction and
the interacting variable.

152
00:08:23,070 --> 00:08:27,520
And I want to show you how you might want
to use the likelihood ratio test here

153
00:08:27,520 --> 00:08:32,310
when talking in particular about the
interaction between gender and program.

154
00:08:32,310 --> 00:08:34,210
You might just want to report this p-value
and

155
00:08:34,210 --> 00:08:37,160
say hey, program A is different from the
rest.

156
00:08:37,160 --> 00:08:41,270
But sometimes we also like to just report
an overall p-value for

157
00:08:41,270 --> 00:08:42,880
the interaction between gender and

158
00:08:42,880 --> 00:08:47,580
program, and not get into the fine level
of detail of individual categories.

159
00:08:47,580 --> 00:08:50,400
So sometimes with a categorical variable,
we want to just report what's called the p

160
00:08:50,400 --> 00:08:53,530
for interaction, which is just to say
overall is there

161
00:08:53,530 --> 00:08:57,070
any interaction between gender and the
categorical variable of program.

162
00:08:57,070 --> 00:09:00,250
And this is where we can use a likelihood
ratio test,

163
00:09:00,250 --> 00:09:02,090
which we've talked about already.

164
00:09:02,090 --> 00:09:04,550
This is a good use of the likelihood ratio
test.

165
00:09:04,550 --> 00:09:06,780
If you wanted overall p for interaction,

166
00:09:06,780 --> 00:09:12,910
what you can do is compare a model that
contains the interactions with,

167
00:09:12,910 --> 00:09:16,479
between gender a program and to a model
that, it doesn't have those interactions.

168
00:09:17,830 --> 00:09:20,790
And, that you can compare with the
likelihood ratio test because

169
00:09:20,790 --> 00:09:24,830
it's actually a group of betas that you
have to compare at once.

170
00:09:24,830 --> 00:09:27,630
So, I ran the model with the interaction
term, that's of the model I just

171
00:09:27,630 --> 00:09:34,870
showed you, my negative 2 Log likelihood
here turned out to be four 4856.

172
00:09:34,870 --> 00:09:38,080
When I then take the interaction term out
and I run the model

173
00:09:38,080 --> 00:09:41,810
without the interaction, my likelihood, my
negative 2 Log likelihood goes up.

174
00:09:41,810 --> 00:09:43,925
Remember this is the reduced model.

175
00:09:43,925 --> 00:09:46,640
[SOUND] And this is the full model.

176
00:09:46,640 --> 00:09:50,340
The reduced model will have a smaller
likelihood but

177
00:09:50,340 --> 00:09:53,140
therefore a bigger negative 2 Log
likelihood, and

178
00:09:53,140 --> 00:09:56,379
the full model will have a smaller value
for negative 2 Log likelihood.

179
00:09:57,580 --> 00:10:01,530
Remember that if we wanted to say whether
or not adding that interaction term which

180
00:10:01,530 --> 00:10:06,530
comprises five variables, whether or not
that adds anything significantly to

181
00:10:06,530 --> 00:10:09,844
the model, we can do that by using a
chi-square.

182
00:10:09,844 --> 00:10:12,792
This is a chi-square with five degrees of
freedom,

183
00:10:12,792 --> 00:10:15,880
because we have five interaction terms in
the model.

184
00:10:16,920 --> 00:10:20,340
It's a six level categorical variable, we
end up with five terms in the model.

185
00:10:20,340 --> 00:10:22,640
So we have a chi-square with five degrees
of freedom.

186
00:10:22,640 --> 00:10:26,050
The negative 2 log likelihood for the
reduced model is this, for

187
00:10:26,050 --> 00:10:26,900
the full model is this.

188
00:10:26,900 --> 00:10:28,620
We just subtract those.

189
00:10:28,620 --> 00:10:32,140
We get the, we have a chi square, with
five degrees of freedom,

190
00:10:32,140 --> 00:10:33,720
with a value of 20.

191
00:10:33,720 --> 00:10:37,080
We can then look to see whether or not
that comes out to be significant.

192
00:10:37,080 --> 00:10:40,050
Indeed I'll show you in a minute, where I
got this from.

193
00:10:40,050 --> 00:10:43,000
But the p-value here come out to be
significant.

194
00:10:43,000 --> 00:10:44,640
So, the expected value here is five.

195
00:10:44,640 --> 00:10:47,350
And, the P-value does indeed come out to
be significant.

196
00:10:47,350 --> 00:10:49,730
So, that's telling us that the P-value for
interaction,

197
00:10:49,730 --> 00:10:54,750
over all adding the interacting term to
our model, improves our model.

198
00:10:54,750 --> 00:10:59,250
So, in case you don't want to get into the
fine level of details of which particular

199
00:10:59,250 --> 00:11:03,300
interacting terms came out significant,
when you have have a categorical variable.

200
00:11:03,300 --> 00:11:05,860
This is a way to just report an overall p
for interaction for

201
00:11:05,860 --> 00:11:08,500
that entire categorical variable.

202
00:11:08,500 --> 00:11:13,100
Just as a quick reminder, to get the, the
p-values that go with a chi-squared test,

203
00:11:13,100 --> 00:11:15,730
you can plug in to one of these online
calculators.

204
00:11:15,730 --> 00:11:16,910
I did that here.

205
00:11:16,910 --> 00:11:20,300
The computer told me that my P-value for
here is 0.0013.

206
00:11:20,300 --> 00:11:24,530
I did have to do this leg work on my own.

207
00:11:24,530 --> 00:11:28,290
I actually had to run the model in my
computer program,

208
00:11:28,290 --> 00:11:30,950
get out the negative 2 log likelihood for
one model.

209
00:11:30,950 --> 00:11:34,360
And then run the model with or the
interaction term,

210
00:11:34,360 --> 00:11:36,630
get the negative 2 log likelihood out for
that.

211
00:11:36,630 --> 00:11:40,920
And then I had to subtract those negative
2 log likelihoods myself, and go and

212
00:11:40,920 --> 00:11:42,550
get the p-value myself.

213
00:11:42,550 --> 00:11:44,420
So sometimes you actually have to do that
legwork by hand.

214
00:11:44,420 --> 00:11:47,990
It's not automatically built into your
computer package to do

215
00:11:47,990 --> 00:11:51,410
the comparison of a reduced and full
model.

216
00:11:51,410 --> 00:11:54,520
The automatic one that the computer
package will give you is the intercept

217
00:11:54,520 --> 00:11:57,120
only model compared with a full model.

218
00:11:57,120 --> 00:12:01,770
This is comparing slightly different so
again I had to do that legwork on my own.

219
00:12:01,770 --> 00:12:04,420
I just want to give you

220
00:12:04,420 --> 00:12:08,010
a second example of confounding here just
to throw out another example.

221
00:12:08,010 --> 00:12:12,940
So this was from a study well, it was
looking at whether or not maternal smoking

222
00:12:12,940 --> 00:12:16,860
was going to change your risk of having a
baby with Down's Syndrome.

223
00:12:16,860 --> 00:12:20,420
This was the American Journal of
Epidemiology a number of years ago but

224
00:12:20,420 --> 00:12:24,440
I really like this example because it's
going to also illustrate the concept of

225
00:12:24,440 --> 00:12:27,500
residual confounding, which we'll get to
in a minute.

226
00:12:27,500 --> 00:12:32,080
But the crude odds ratio here, so when you
just look at,

227
00:12:32,080 --> 00:12:35,110
the outcome here is Down's Syndrome, the
baby has Down's Syndrome or not.

228
00:12:35,110 --> 00:12:37,970
The predictor here is whether or not the
mother smoked during pregnancy.

229
00:12:37,970 --> 00:12:39,630
And if you just look at the crude odds
ratio,

230
00:12:39,630 --> 00:12:41,880
it indeed comes out to be significant.

231
00:12:41,880 --> 00:12:43,913
This confidence interval does not cross 1.

232
00:12:43,913 --> 00:12:47,136
It's actually significant in the
protective direction.

233
00:12:47,136 --> 00:12:50,296
So it looks like, if you don't adjust for
any confounders,

234
00:12:50,296 --> 00:12:54,410
that maternal smoking reduces your risk of
getting Down's Syndrome.

235
00:12:54,410 --> 00:12:58,720
Now, it's very hard to come up with a
biological story here,

236
00:12:58,720 --> 00:13:00,370
that that could somehow be causal.

237
00:13:00,370 --> 00:13:01,530
We know that in general,

238
00:13:01,530 --> 00:13:07,330
smoking tends to cause DNA damage, DNA
damage, would relate to Down syndrome.

239
00:13:07,330 --> 00:13:09,706
Perhaps you can come up with a causal
story here, but

240
00:13:09,706 --> 00:13:12,924
it's more likely that this is due to in
confounding and indeed it was.

241
00:13:12,924 --> 00:13:14,500
So that's the crude odds ratio.

242
00:13:14,500 --> 00:13:16,730
But then, think about potential
confounders here.

243
00:13:16,730 --> 00:13:20,870
The big confounder here, the most
important one turns out to be age.

244
00:13:20,870 --> 00:13:24,320
And that's I like this paper because they
did a lot of nice graphics and

245
00:13:24,320 --> 00:13:26,450
it's well pictured here.

246
00:13:26,450 --> 00:13:31,320
So here is showing you on the left hand
figure, this is showing you

247
00:13:31,320 --> 00:13:35,810
the relationship between maternal age and
the rates of Down's Syndrome.

248
00:13:35,810 --> 00:13:39,690
So as you all probably know, as maternal
age goes up especially as you

249
00:13:39,690 --> 00:13:44,390
get to 40 and above, there's a really big
increase in the risk of Down's Syndrome.

250
00:13:44,390 --> 00:13:47,330
So you can see that there's a strong
relationship clearly between age and

251
00:13:47,330 --> 00:13:48,560
Down's Syndrome.

252
00:13:48,560 --> 00:13:49,570
You turn over to age and

253
00:13:49,570 --> 00:13:53,760
smoking though, the predictor here, age is
also related to the predictor and

254
00:13:53,760 --> 00:13:58,010
it's a pretty strong relationship so
younger women smoke much more.

255
00:13:58,010 --> 00:14:00,390
Are much more likely to smoke than older
women.

256
00:14:00,390 --> 00:14:05,160
So as you get older, you're also more,
you're more likely to

257
00:14:05,160 --> 00:14:08,400
have a baby with Down's Syndrome, but
you're also less likely to smoke.

258
00:14:08,400 --> 00:14:15,160
Hence we end up with this spurious
association between maternal smoking and

259
00:14:15,160 --> 00:14:15,990
Down's Syndrome.

260
00:14:15,990 --> 00:14:19,850
So what the authors did here was then to
adjust for various things.

261
00:14:19,850 --> 00:14:21,760
So here they're adjusting for various
things.

262
00:14:21,760 --> 00:14:25,770
I'm just going to focus, for the moment,
on the, the most adjusted model.

263
00:14:25,770 --> 00:14:26,860
That's the one at the end.

264
00:14:26,860 --> 00:14:29,760
So when they adjusted for everything that
they needed to adjust for, the most

265
00:14:29,760 --> 00:14:34,620
important one being age, the crude odds
ratio, the odds ratio now is exactly 1.0.

266
00:14:34,620 --> 00:14:37,740
So there is no association between
maternal smoking and

267
00:14:37,740 --> 00:14:40,910
Down's Syndrome once you've accounted for
the confounders here.

268
00:14:40,910 --> 00:14:45,150
And you can see that we started with an
odds ratio of 0.8.

269
00:14:45,150 --> 00:14:50,490
We have changed clearly by more than 10%,
we now are at an odds ratio of 1.0.

270
00:14:50,490 --> 00:14:52,990
And again I pay attention only to these
effects then and

271
00:14:52,990 --> 00:14:55,080
not to the statistical significance here.

272
00:14:57,200 --> 00:15:00,840
I usually like to apply that 10% rule to
the betas rather than the odds ratio.

273
00:15:00,840 --> 00:15:05,040
So if you wanted to you could convert the
ln of 0.8 here and

274
00:15:05,040 --> 00:15:06,660
then of course ln of 1 would be 0.

275
00:15:06,660 --> 00:15:11,200
And you can compare whatever this value
turns out to be to 0, rather comparing

276
00:15:11,200 --> 00:15:15,070
the odds ratios, but clearly we have a
major change in the effects size here.

277
00:15:15,070 --> 00:15:19,810
So that's how we ach, evaluate confounding
in, in the context of regression.

278
00:15:19,810 --> 00:15:21,690
What's interesting about this particular
example,

279
00:15:21,690 --> 00:15:27,000
one of the reasons I like to actually use
this example, is that the authors made

280
00:15:27,000 --> 00:15:31,210
of point of illustrating something called
residual confounding.

281
00:15:31,210 --> 00:15:35,070
And it's very important as we start
talking a lot about statistical adjustment

282
00:15:35,070 --> 00:15:38,040
for confounding, it's really important to
keep in the back of

283
00:15:38,040 --> 00:15:41,570
your mind that adjustment for confounding
is not a panacea.

284
00:15:41,570 --> 00:15:45,330
You can not adjust away all confounding.

285
00:15:45,330 --> 00:15:47,970
And that's kind of illustrated here.

286
00:15:47,970 --> 00:15:50,860
So this first regression analysis was

287
00:15:50,860 --> 00:15:54,360
actually included in the paper just as an
illustration of residual confounding.

288
00:15:54,360 --> 00:15:58,010
What they did in that very first
regression analysis is they

289
00:15:58,010 --> 00:15:59,800
modeled age crudely.

290
00:15:59,800 --> 00:16:03,080
They put into their regression, as a
predictor,

291
00:16:03,080 --> 00:16:06,580
you're either younger than 35 or you're 35
and older.

292
00:16:06,580 --> 00:16:09,720
So they modeled age as a binary predictor.

293
00:16:11,230 --> 00:16:14,570
When they do that, we are indeed
attenuating the odds ratio.

294
00:16:14,570 --> 00:16:15,980
It's going from point 0.80 to 0.87.

295
00:16:15,980 --> 00:16:22,750
But, we are not completely wiping out the
effect of confounding by maternal age.

296
00:16:22,750 --> 00:16:26,320
And that's illustrated as well by this
second regression analysis.

297
00:16:26,320 --> 00:16:30,540
Because this also includes age as a binary
predictor.

298
00:16:30,540 --> 00:16:31,560
But it also adjusts for

299
00:16:31,560 --> 00:16:34,480
all the other things that we're eventually
going to adjust for, race and parity.

300
00:16:35,640 --> 00:16:38,820
What this shows you is that just putting
age in

301
00:16:38,820 --> 00:16:43,820
as a binary predictor does not take away
all of the confounding.

302
00:16:43,820 --> 00:16:49,490
When in the last model the authors finally
put in the exact year of maternal age,

303
00:16:49,490 --> 00:16:52,480
then all of the confounding is removed.

304
00:16:53,640 --> 00:16:57,110
But, this is illustrating the point that
when you put in predictors that

305
00:16:57,110 --> 00:16:59,020
are imprecisely measured,

306
00:16:59,020 --> 00:17:03,100
which is often times we are not measureing
things precisely.

307
00:17:04,700 --> 00:17:07,600
then, there was going to be some leftover
confounding.

308
00:17:07,600 --> 00:17:10,180
Finally, at the end here, they were
actually, I mean,

309
00:17:10,180 --> 00:17:12,540
age is one of the things that we are able
to measure precisely.

310
00:17:12,540 --> 00:17:13,320
You can get birth dates.

311
00:17:13,320 --> 00:17:17,590
So once they finally put in age as an
exact birth date,

312
00:17:17,590 --> 00:17:22,550
the exact age, then finally they were able
to adjust away for all the counfounding.

313
00:17:22,550 --> 00:17:23,870
But for most predictors, for

314
00:17:23,870 --> 00:17:27,110
most confounders we don't measure them
precisely and we don't

315
00:17:27,110 --> 00:17:31,260
measure the predictors precisely and then
the outcomes precisely in a lot of cases.

316
00:17:31,260 --> 00:17:36,420
So the problem here is that because
there's error in the various measurements,

317
00:17:36,420 --> 00:17:38,200
statistical adjustment is imperfect.

318
00:17:38,200 --> 00:17:41,800
And that's what we call residual
confounding, and it comes up a lot.

319
00:17:41,800 --> 00:17:46,590
You really cannot completely wipe out a
confounding simply by adjusting for

320
00:17:46,590 --> 00:17:49,130
variables in multiple regression,

321
00:17:49,130 --> 00:17:52,880
unless you had somehow measured the
variables completely with zero error.

322
00:17:52,880 --> 00:17:56,220
We did it probably in the last example.

323
00:17:56,220 --> 00:18:00,020
Down Syndrome is pretty easy to measure to
get, I mean that's a very easy to measure.

324
00:18:00,020 --> 00:18:01,300
We can probably get a good gauge of that.

325
00:18:01,300 --> 00:18:07,660
Smoking is relatively easy to measure
although there might be some confounding,

326
00:18:07,660 --> 00:18:11,310
some leftover confounding with smoking
because we're not going to measure that

327
00:18:11,310 --> 00:18:13,070
totally precisely.

328
00:18:13,070 --> 00:18:14,850
But age is going to be measured precisely.

329
00:18:14,850 --> 00:18:17,520
So we were able to wipe out most of the
confounding in that example.

330
00:18:17,520 --> 00:18:20,850
But I'll just tell you that residual
confounding comes up

331
00:18:20,850 --> 00:18:22,940
a lot with observational studies.

332
00:18:22,940 --> 00:18:24,110
You say, oh I've adjusted for

333
00:18:24,110 --> 00:18:27,430
smoking, I've adjusted for age, therefore
I've taken care of those.

334
00:18:27,430 --> 00:18:28,140
They're all done with.

335
00:18:28,140 --> 00:18:29,780
There can't be any confounding by those.

336
00:18:29,780 --> 00:18:31,190
But in fact that's not true.

337
00:18:31,190 --> 00:18:33,990
If you've measured your confounders at all
imperfectly or

338
00:18:33,990 --> 00:18:36,734
if you've measured your predictors or your
outcome at all

339
00:18:36,734 --> 00:18:40,910
imperfectly it's impossible to completely
adjust away all the confounding.

340
00:18:40,910 --> 00:18:45,880
In most cases you end up with some
leftover or residual confounding.

341
00:18:45,880 --> 00:18:51,240
And my favorite example to illustrate that
is an example that was

342
00:18:51,240 --> 00:18:56,360
in the American Archives, the Archives of
Internal Medicine a few years back.

343
00:18:56,360 --> 00:18:59,350
Made a lot of headlines in the news media.

344
00:18:59,350 --> 00:19:01,400
I've shown this table before.

345
00:19:01,400 --> 00:19:05,610
I just love this table as an illustration
of why confounding occurs, or

346
00:19:05,610 --> 00:19:07,450
why we have to worry about confounding.

347
00:19:07,450 --> 00:19:10,050
This was a study that was looking at, I'm
just showing you the men here,

348
00:19:10,050 --> 00:19:14,570
that was looking at red meat eating, and
whether or not that related to mortality.

349
00:19:14,570 --> 00:19:17,910
And they classified men into different
quintiles of red meat eating,

350
00:19:17,910 --> 00:19:20,510
Q5 being the highest red meat eater.

351
00:19:20,510 --> 00:19:24,910
And I've shown this before, but if you
look carefully at this table, all the,

352
00:19:24,910 --> 00:19:29,090
the risk factors for for, you know, all
the bad health behaviors and

353
00:19:29,090 --> 00:19:32,430
risk factors go up as you go up across the
different quintiles.

354
00:19:32,430 --> 00:19:36,660
So as you go up, your BMI increases, as
you eat more red meat, you're,

355
00:19:36,660 --> 00:19:38,760
you're more likely to be a current smoker.

356
00:19:38,760 --> 00:19:42,230
You're less likely to exercise, you're
more likely to eat a lot of calories.

357
00:19:42,230 --> 00:19:44,280
So basically, people who eat a lot of,

358
00:19:44,280 --> 00:19:49,700
especially men here, who eat a lot of red
meat, tend to be risky in many ways.

359
00:19:49,700 --> 00:19:50,410
They have,

360
00:19:50,410 --> 00:19:55,340
they just have bad health habits, maybe
they're more risk-takers in general.

361
00:19:55,340 --> 00:19:58,430
So this paper was attempting to say,

362
00:19:58,430 --> 00:20:04,480
what is the precise effect of actually
eating read meat with mortality.

363
00:20:04,480 --> 00:20:06,680
And, so they said, we're going to try to
adjust away for all these confounders.

364
00:20:06,680 --> 00:20:09,440
We're going to throw BMI in the model,
we're going to throw smoking in the model,

365
00:20:09,440 --> 00:20:10,760
we're going to throw exercise in the
model,

366
00:20:10,760 --> 00:20:12,870
we're going to try to adjust away for all
those things.

367
00:20:12,870 --> 00:20:17,730
So I'm going to show you the, the results
of this from this study.

368
00:20:17,730 --> 00:20:23,030
So this is showing you the mortality
risks.

369
00:20:23,030 --> 00:20:25,430
These are relative risks, something called
hazard ratios.

370
00:20:25,430 --> 00:20:28,670
This actually came out a Cox regression
and not logistic regression but

371
00:20:28,670 --> 00:20:31,250
residual confounding applies to all types
of regression.

372
00:20:31,250 --> 00:20:34,480
So what you're seeing here is that for,
let's start with overall mortality, and

373
00:20:34,480 --> 00:20:36,240
actually let me zoom in a little bit here
so

374
00:20:36,240 --> 00:20:38,730
it's easier on small screens to see this.

375
00:20:38,730 --> 00:20:41,880
So, we're going to start with all caused
mortality.

376
00:20:41,880 --> 00:20:44,470
The reference group is the lowest red meat
eaters.

377
00:20:44,470 --> 00:20:47,470
The Q5 is the highest red meat eaters.

378
00:20:47,470 --> 00:20:51,220
And they're showing you the risk, the
relative risk, and this is called hazard

379
00:20:51,220 --> 00:20:55,570
ratio, but it's just like a risk ratio, as
you go up across the different quintiles.

380
00:20:55,570 --> 00:20:58,040
So the basic models then adjusted for too
many things, and

381
00:20:58,040 --> 00:21:02,610
indeed you're seeing a nice increase in
mortality as you're eating more red meat.

382
00:21:02,610 --> 00:21:04,640
Then they do an adjusted model.

383
00:21:04,640 --> 00:21:05,380
They're trying to adjust for

384
00:21:05,380 --> 00:21:07,990
all these things that are different about
heavy red meat eaters.

385
00:21:09,040 --> 00:21:11,720
And what you'll notice is that as we
adjust esp,

386
00:21:11,720 --> 00:21:15,590
with especially in that last Q5 quintile.

387
00:21:15,590 --> 00:21:17,800
That when you adjust, you started at 1.48.

388
00:21:17,800 --> 00:21:22,220
When you adjust for all these things
indeed, indeed the effect gets attenuated.

389
00:21:22,220 --> 00:21:23,690
It gets closer to one.

390
00:21:23,690 --> 00:21:25,530
So it, it's really, after you adjust for

391
00:21:25,530 --> 00:21:29,650
everything, a 31% increase in the
mortality risk or rate.

392
00:21:30,910 --> 00:21:33,860
And then, but what's interesting here is
you might have just stopped there and

393
00:21:33,860 --> 00:21:36,270
said okay, well we adjusted for BMI, we
adjusted for exercise.

394
00:21:36,270 --> 00:21:39,630
The authors did a great job adjusting for
everything they possibly could.

395
00:21:39,630 --> 00:21:41,430
It was a nicely done study.

396
00:21:41,430 --> 00:21:44,110
You might just stop there and say, I've
adjusted for everything, and

397
00:21:44,110 --> 00:21:48,190
it's still an elevated risk, it's still
statistically significant.

398
00:21:48,190 --> 00:21:48,890
Aha!

399
00:21:48,890 --> 00:21:50,210
Red meat eating is bad for you.

400
00:21:50,210 --> 00:21:52,410
And that's indeed what the conclusion of
this study was.

401
00:21:52,410 --> 00:21:54,600
It was widely reported in the media.

402
00:21:54,600 --> 00:21:59,440
What's interesting, though, is if you
delve into the specific causes of death.

403
00:21:59,440 --> 00:22:01,620
If you look at cancer mortality.

404
00:22:01,620 --> 00:22:04,530
Okay, cancer mortality elevated, also
elevated in

405
00:22:04,530 --> 00:22:09,330
that heavy red meat eating group, and
again adjusting for the confounders does

406
00:22:09,330 --> 00:22:12,630
attenuate that effect somewhat, but it
still remains even after adjustment.

407
00:22:13,650 --> 00:22:19,070
1.22 for cancer mortality, 1.27 for
cardiovascular disease mortality.

408
00:22:19,070 --> 00:22:21,450
But here is where it really gets
interesting.

409
00:22:21,450 --> 00:22:25,710
So they also put in there mortality from
injuries and sudden deaths.

410
00:22:25,710 --> 00:22:27,720
They also looked at that.

411
00:22:27,720 --> 00:22:30,780
That's a category of, of things that
people might die from.

412
00:22:30,780 --> 00:22:34,070
So this includes things like gunshot
wounds, and car accidents.

413
00:22:34,070 --> 00:22:35,555
Those would probably be the most common.

414
00:22:35,555 --> 00:22:41,690
So what's interesting here [LAUGH] is that
if you look at the risk ratio,

415
00:22:41,690 --> 00:22:45,370
the relative risk here between high red
meat eating compared to

416
00:22:45,370 --> 00:22:49,605
the reference group of low red meat
eaters, it's also elevated.

417
00:22:49,605 --> 00:22:54,020
It's, the magnitude is almost identical to
the increase in risk for

418
00:22:54,020 --> 00:22:56,690
cardiovascular disease and for cancer.

419
00:22:56,690 --> 00:23:00,280
So that's really interesting because if
you start make a causal story here,

420
00:23:00,280 --> 00:23:04,030
how does red meat, heavy red meat eating,

421
00:23:04,030 --> 00:23:09,030
how does that increase your risk of car
accidents and gun shot wounds?

422
00:23:09,030 --> 00:23:13,340
Well, the, the logical answer to that is
it probably doesn't.

423
00:23:13,340 --> 00:23:15,960
So what we're seeing here,

424
00:23:15,960 --> 00:23:20,200
this 1.26 is almost certainly the result
of residual confounding.

425
00:23:20,200 --> 00:23:25,300
Heavy red meat eaters tend to be people
who have a lot of risky behaviors.

426
00:23:25,300 --> 00:23:27,480
They probably take a lot of chances.

427
00:23:27,480 --> 00:23:31,030
They might own guns and, you know, drive
too fast.

428
00:23:31,030 --> 00:23:32,660
Might drink while driving.

429
00:23:32,660 --> 00:23:35,330
The kinds of things that increase your
risk of this type of death.

430
00:23:36,450 --> 00:23:39,440
And notice that when we adjust for
confounding here there's no change.

431
00:23:39,440 --> 00:23:44,100
So you had 1.24 before adjustment for all
the confounders And 1.26 after you adjust.

432
00:23:44,100 --> 00:23:47,890
That's telling you that there's just
something inherently risky about red meat

433
00:23:47,890 --> 00:23:53,230
eaters that we are unable to capture with
the confounders that we've measured here.

434
00:23:53,230 --> 00:23:59,990
And that leads me to believe that the the
relationships detected here between cancer

435
00:23:59,990 --> 00:24:04,108
mortality and cardiovascular mortality,
which are about the same magnitude,

436
00:24:04,108 --> 00:24:09,730
1.27 and 1.22, that that, those also might
just be due to residual confounding.

437
00:24:09,730 --> 00:24:12,770
Heavy red meat eaters just do a lot of
things bad, and

438
00:24:12,770 --> 00:24:14,390
that increases their risk of mortality.

439
00:24:14,390 --> 00:24:17,370
But it might not be that it's because red
meat eating,

440
00:24:17,370 --> 00:24:21,100
per se, has a direct effect on mortality.

441
00:24:21,100 --> 00:24:26,040
All other deaths, also which is sort of a
catch all category, also is elevated in

442
00:24:26,040 --> 00:24:28,770
a way makes you suspicious that there
might be residual confounding.

443
00:24:28,770 --> 00:24:32,350
And some people actually wrote letters to
the editor on this particular one,

444
00:24:32,350 --> 00:24:34,960
worried about the, the issue of residual
confounding.

445
00:24:37,980 --> 00:24:41,880
And just to help you recognize when there
might be residual confounding.

446
00:24:41,880 --> 00:24:44,600
If you're talking about a binary
predictor, like, you know,

447
00:24:44,600 --> 00:24:46,720
you're a high red meat eater or you're
not.

448
00:24:46,720 --> 00:24:49,380
Incomplete con, adjustment for

449
00:24:49,380 --> 00:24:54,470
confounding can generate spurious relative
risks in the range of 0.6 to 1.6.

450
00:24:54,470 --> 00:24:59,062
So notice the, the relative risk we were
talking about with the red

451
00:24:59,062 --> 00:25:03,681
meat eating example, was about 1.2, 1.3,
in that range.

452
00:25:03,681 --> 00:25:06,180
So these are small effect sizes.

453
00:25:06,180 --> 00:25:10,688
For the Down Syndrome example the effect
size was .8, so

454
00:25:10,688 --> 00:25:14,870
those were both within the range of 0.6 to
1.6.

455
00:25:14,870 --> 00:25:17,970
And statisticians have done simulations
where they actually show

456
00:25:17,970 --> 00:25:22,510
that residual confounding can cause
spurious associations in that

457
00:25:22,510 --> 00:25:26,028
general relative risk range, 0.6 to 1.6.

458
00:25:26,028 --> 00:25:31,740
In other words, if you see an effect size
of two or three in your data set,

459
00:25:31,740 --> 00:25:35,730
that is usually beyond what residual
confounding can cause.

460
00:25:35,730 --> 00:25:38,980
So residual confounding is kind of limited
in its scope.

461
00:25:38,980 --> 00:25:43,410
But if you pick up a lot of observational
studies and papers in the literature,

462
00:25:43,410 --> 00:25:48,742
you will see plenty of risk ratios and
odds ratios that are 1.3, 0.8, 1.4.

463
00:25:48,742 --> 00:25:54,160
These are the, the when the effect sizes
are this,

464
00:25:54,160 --> 00:25:57,500
this small, this is where you start to
worry that there might be residual

465
00:25:57,500 --> 00:26:02,010
confounding that can completely explain
the association that you are seeing.

466
00:26:02,010 --> 00:26:06,060
I've given you examples here where the
residual, failure to,

467
00:26:06,060 --> 00:26:10,980
to completely, perfectly adjust for
confounding creates spurious associations.

468
00:26:10,980 --> 00:26:13,000
however, it can also go in the other
direction.

469
00:26:13,000 --> 00:26:16,010
Residual confounding might also obscure
relationships.

470
00:26:16,010 --> 00:26:17,900
It's causing you to miss associations.

471
00:26:17,900 --> 00:26:22,055
So you might end up with a risk ration of
1.0 when the true risk ratio is

472
00:26:22,055 --> 00:26:24,670
0.8 because of residual confounding.

473
00:26:24,670 --> 00:26:27,530
Those examples are actually much harder to
find.

474
00:26:27,530 --> 00:26:28,500
They're much harder to uncover.

475
00:26:28,500 --> 00:26:29,320
But they can happen.

476
00:26:29,320 --> 00:26:31,340
So just keep in mind that it can go either
way.
