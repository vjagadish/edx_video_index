1
00:00:00,025 --> 00:00:05,320
[BLANK_AUDIO].

2
00:00:05,320 --> 00:00:10,700
In this next module, we're going to dive
right into the Logistic regression model.

3
00:00:10,700 --> 00:00:13,400
We've talked about when you have binary or

4
00:00:13,400 --> 00:00:16,350
categorical outcomes, that, that is when
you're looking at proportions.

5
00:00:16,350 --> 00:00:19,140
We've talked about some statistical tests.

6
00:00:19,140 --> 00:00:21,740
When you don't want to adjust for anything
that you can use.

7
00:00:21,740 --> 00:00:24,840
So this is like the risk difference,
relative risk, the Chi-square test,

8
00:00:24,840 --> 00:00:26,160
the McNemar's chi-square test.

9
00:00:26,160 --> 00:00:29,220
So we talked about those, but what about
when you want to

10
00:00:29,220 --> 00:00:33,910
do a regression analysis when you have a
binary or categorical outcome.

11
00:00:33,910 --> 00:00:37,490
In that case, you're going to be using
Logistic regression.

12
00:00:37,490 --> 00:00:39,680
Which we're going to talk about in this
module.

13
00:00:41,430 --> 00:00:44,010
I want to just give you a little overview
of logistic regression.

14
00:00:44,010 --> 00:00:46,880
So, to remind you when we did linear
regression,

15
00:00:46,880 --> 00:00:48,670
the model looked like the following.

16
00:00:48,670 --> 00:00:54,480
We were predicting the value of some
continuous outcome variable, call it y.

17
00:00:54,480 --> 00:00:56,780
As a function of x.

18
00:00:56,780 --> 00:01:00,720
Here's a simple linear regression model,
we have the intercept alpha and

19
00:01:00,720 --> 00:01:01,870
beta is our slope.

20
00:01:01,870 --> 00:01:04,100
So that was the simple linear regression
model.

21
00:01:04,100 --> 00:01:08,090
In the multivariate regression model, we
can add some additional betas and x's.

22
00:01:08,090 --> 00:01:14,240
Additional predictors that makes it then a
plane or a higher dimension thing but

23
00:01:14,240 --> 00:01:19,180
essentially we're fitting lines when you
are regression.

24
00:01:19,180 --> 00:01:21,960
Logistic regression actually isn't that
different.

25
00:01:21,960 --> 00:01:26,750
The right hand side of the equation is
identical in logistic regression, so

26
00:01:26,750 --> 00:01:28,590
it's going to look something like this.

27
00:01:28,590 --> 00:01:33,420
There's a linear combination of betas and,
and x's, risk factors.

28
00:01:33,420 --> 00:01:36,500
It looks exactly the same, and
everything's going to be

29
00:01:36,500 --> 00:01:40,890
set up exactly the same in terms of
defining the predictors of, the, of the.

30
00:01:40,890 --> 00:01:45,000
The model, you can put binary predictors
then you can put continuous,

31
00:01:45,000 --> 00:01:47,010
you can put categorical, you can do dummy
coding.

32
00:01:47,010 --> 00:01:50,010
All of that is the same as with linear
regression.

33
00:01:50,010 --> 00:01:54,100
What's different in logistic regression is
the left-hand side of the equation.

34
00:01:54,100 --> 00:01:59,360
We are no longer modeling Yi, the outcome
variable, directly.

35
00:01:59,360 --> 00:02:04,240
Rather, we're going to be modeling a
transformation of the outcome variable.

36
00:02:04,240 --> 00:02:07,260
In logistic, the case of logistic
regression, we're going to be modeling.

37
00:02:07,260 --> 00:02:09,780
The log odds of the outcome.

38
00:02:09,780 --> 00:02:11,410
And I know that sounds terrible.

39
00:02:11,410 --> 00:02:13,250
That's called a logit function.

40
00:02:13,250 --> 00:02:14,300
But don't worry.

41
00:02:14,300 --> 00:02:16,150
I, I'll explain why in a minute.

42
00:02:16,150 --> 00:02:18,440
We're going to be modeling this logit
function.

43
00:02:18,440 --> 00:02:19,820
The log odds of the outcome.

44
00:02:19,820 --> 00:02:21,370
That's our outcome variable.

45
00:02:22,490 --> 00:02:25,180
We're going to be modeling that as a line
though.

46
00:02:25,180 --> 00:02:27,451
As an alpha plus beta x.

47
00:02:27,451 --> 00:02:30,230
I intercept and the slope.

48
00:02:30,230 --> 00:02:34,830
Because we made this transformation, the
beta is just slightly most

49
00:02:34,830 --> 00:02:38,120
complicated to interpret than the beta
from linear regression.

50
00:02:38,120 --> 00:02:40,950
So for linear regression, the beta was
just the slope.

51
00:02:40,950 --> 00:02:44,670
Is, as x goes up by one unit, how much
does y go up.

52
00:02:44,670 --> 00:02:46,650
So there was a direct interpretation.

53
00:02:46,650 --> 00:02:50,560
It's just slightly more tricky to
interpret the betas from

54
00:02:50,560 --> 00:02:55,280
logistic regression but actually, it's not
terrible because the betas from

55
00:02:55,280 --> 00:02:59,850
logistic regression, if you exponentiate
them, what it gives you out.

56
00:02:59,850 --> 00:03:01,090
Is the odds ratio.

57
00:03:01,090 --> 00:03:05,690
And I refer to, in the second week of the
course, we talked a lot about odds ratios.

58
00:03:05,690 --> 00:03:09,200
And I told you, hey logistic regression
spits out odds ratios.

59
00:03:09,200 --> 00:03:10,560
And this is where it comes from.

60
00:03:10,560 --> 00:03:16,310
The betas can be directly interpreted as
odds ratios just by exponentiating.

61
00:03:16,310 --> 00:03:19,480
And the reason there's an exponential
involved is because our outcome variable

62
00:03:19,480 --> 00:03:21,080
involves a natural log.

63
00:03:21,080 --> 00:03:22,790
So this is just the overview of where
we're going.

64
00:03:22,790 --> 00:03:25,830
This is just, keep in mind that the right
hand side of

65
00:03:25,830 --> 00:03:28,670
the equation is the same as with linear
regression.

66
00:03:28,670 --> 00:03:31,780
So we're still fitting a line, it's just
we're fitting the outcome variable there's

67
00:03:31,780 --> 00:03:33,290
something, trickier going on.

68
00:03:33,290 --> 00:03:38,530
Let me start here with a simple example

69
00:03:38,530 --> 00:03:43,070
using that example dataset, dataset on my
Stanford students.

70
00:03:44,090 --> 00:03:47,950
One of the questions I asked my Stanford
students is whether or

71
00:03:47,950 --> 00:03:52,350
not they considered themselves to be book
smart or street smart.

72
00:03:52,350 --> 00:03:56,840
This is a binary variable, book smart if
the variable is

73
00:03:56,840 --> 00:03:59,600
equal to one that means that they consider
themselves book smart.

74
00:03:59,600 --> 00:04:03,430
If the variable is equal to zero the way
I've coded it.

75
00:04:03,430 --> 00:04:06,250
That means that they consider themselves
to be street smart.

76
00:04:06,250 --> 00:04:11,620
So we can see that this is the book smart
group over here and

77
00:04:11,620 --> 00:04:15,310
this is the street smart book, group over
here.

78
00:04:15,310 --> 00:04:18,870
And I have, just as an aside, I have been
asking this question to lots of

79
00:04:18,870 --> 00:04:20,850
Stanford students just for fun over the
years.

80
00:04:20,850 --> 00:04:25,240
And it generally comes down about 80% of
my Stanford students consider

81
00:04:25,240 --> 00:04:29,030
themselves to be book smart, and about 20%
consider themselves to be street smart.

82
00:04:29,030 --> 00:04:31,950
And that was indeed, the case in this
little practice data set.

83
00:04:31,950 --> 00:04:32,810
It's right around 80%.

84
00:04:32,810 --> 00:04:38,290
I wondered if book smart students did
more,

85
00:04:38,290 --> 00:04:43,870
spent more time on homework or more
studios than street smart students.

86
00:04:43,870 --> 00:04:47,260
And so I wanted to look at the variable
homework in the number of

87
00:04:47,260 --> 00:04:48,730
hours per week that they spent on
homework.

88
00:04:48,730 --> 00:04:51,500
That's another question that I had asked
these students and

89
00:04:51,500 --> 00:04:54,710
we can look at that here and I've done
this, I've looked at this graphicly here.

90
00:04:54,710 --> 00:04:56,050
So I'm graphing.

91
00:04:56,050 --> 00:04:59,735
My predictor variable is just the binary,
a yes no, book smart or

92
00:04:59,735 --> 00:05:04,390
not, and my outcome variable here is
homework, in the number of hours per week.

93
00:05:04,390 --> 00:05:07,970
Now surprisingly, there's students who
consider themselves book-smart as you

94
00:05:07,970 --> 00:05:12,290
can see from this graphic here, actually
on average do slightly less homework

95
00:05:12,290 --> 00:05:16,420
than the students who consider themselves
street-smart, for whatever reason.

96
00:05:16,420 --> 00:05:21,130
So it went, kind of in the opposite
direction from what I might have expected.

97
00:05:21,130 --> 00:05:25,970
This analysis, this graphic that I'm
showing you here actually coincides with

98
00:05:25,970 --> 00:05:27,750
a linear regression.

99
00:05:27,750 --> 00:05:32,160
So the way I've set up the problem now is,
I have got a binary predictor variable and

100
00:05:32,160 --> 00:05:34,430
a continuous outcome variable homework.

101
00:05:35,510 --> 00:05:38,230
And we could model this with a linear
regression,

102
00:05:38,230 --> 00:05:40,620
using what we talked about last week.

103
00:05:40,620 --> 00:05:44,190
When we have a binary predictor, it's a
very simple linear regression.

104
00:05:44,190 --> 00:05:47,220
Right, that line is just, you connect to
the mean for

105
00:05:47,220 --> 00:05:49,760
the street smart group with the mean for
the book smart group.

106
00:05:49,760 --> 00:05:53,340
And when, if you connect two points with a
line that, that defines a line and

107
00:05:53,340 --> 00:05:55,510
so we end up with a very simple line.

108
00:05:55,510 --> 00:05:57,730
So we could answer this question.

109
00:05:57,730 --> 00:06:01,560
Look at this question in this data set
with a linear regression, or

110
00:06:01,560 --> 00:06:05,404
even a t-test, which of course is a
special case of a linear regression.

111
00:06:05,404 --> 00:06:10,260
This is cross-sectional data, however, so
I've happened to pick, just for

112
00:06:10,260 --> 00:06:15,390
the purposes of illustration in, for this
slide to put book smart as

113
00:06:15,390 --> 00:06:19,260
the x variable as the predictor, and
homework as the outcome variable.

114
00:06:19,260 --> 00:06:23,800
But because this is cross sectional data,
I could ask, I could flip that.

115
00:06:23,800 --> 00:06:28,370
I could ask not, whether or not whether
you classify yourself as book smart

116
00:06:28,370 --> 00:06:32,600
predicts the amount of homework you would
do, but I could ask it the other way.

117
00:06:32,600 --> 00:06:36,500
Does the amount of homework you do, the
amount of time you spend on homework,

118
00:06:36,500 --> 00:06:39,220
predict whether or not you consider
yourself book smart.

119
00:06:39,220 --> 00:06:42,040
So I'm just going to flip the x and y
variables.

120
00:06:42,040 --> 00:06:44,980
So here is the graphic when I flip the x
and y variable.

121
00:06:44,980 --> 00:06:50,050
So now my x, my predictor variable, is
continuous, the number of hours spent on

122
00:06:50,050 --> 00:06:55,610
homework per week and my outcome variable
is this binary, this yes, no.

123
00:06:55,610 --> 00:06:56,980
Zero is street smart, one is book smart.

124
00:06:58,330 --> 00:07:00,280
We've got a new graphic.

125
00:07:00,280 --> 00:07:04,790
And we could try to fit a line to these
data, but

126
00:07:04,790 --> 00:07:06,880
look what happens when I try to fit a line
to these data.

127
00:07:06,880 --> 00:07:10,360
Notice that all of my my outcomes are
either all one or

128
00:07:10,360 --> 00:07:12,470
all zero, so you just get these two
stripes.

129
00:07:12,470 --> 00:07:16,560
And it doesn't, it doesn't seem optimal to
try to fit a line.

130
00:07:16,560 --> 00:07:18,210
I, did fit a line to these data.

131
00:07:18,210 --> 00:07:19,120
You can do it.

132
00:07:19,120 --> 00:07:20,480
This is a linear regression line, so

133
00:07:20,480 --> 00:07:23,690
I fit the linear, the best fit line to
these data.

134
00:07:23,690 --> 00:07:26,730
But you can see that if you look at this
picture,

135
00:07:26,730 --> 00:07:30,480
it doesn't really look like that line, it
fits the data very well, right?

136
00:07:30,480 --> 00:07:33,590
I mean, when we were doing linear
regression last week,

137
00:07:33,590 --> 00:07:35,000
we had these scatter plots and

138
00:07:35,000 --> 00:07:37,850
we were trying to find a line that was
kind of very close to all those dots.

139
00:07:37,850 --> 00:07:40,270
Well, this line does, you can fit it, but

140
00:07:40,270 --> 00:07:42,260
it doesn't look very close to all of those
dots.

141
00:07:42,260 --> 00:07:44,820
It doesn't look like a very good fit.

142
00:07:44,820 --> 00:07:48,050
So, a linear regression model was not
optimal, here.

143
00:07:49,700 --> 00:07:50,500
Well, what do we do?

144
00:07:50,500 --> 00:07:53,440
Again, I told you we're still going to
want to fit lines here.

145
00:07:53,440 --> 00:07:59,430
So how can we still fit a line but do
something other than linear regression.

146
00:08:02,390 --> 00:08:04,793
How could we, what could we do.

147
00:08:04,793 --> 00:08:08,770
So what we could do here, is to transform
the outcome variable.

148
00:08:08,770 --> 00:08:11,520
Again, I still want to fit a line
ultimately but

149
00:08:11,520 --> 00:08:14,110
I don't want a 0-1 variable.

150
00:08:14,110 --> 00:08:18,980
As my outcome, because a 0-1 variable
doesn't fit well to a line.

151
00:08:18,980 --> 00:08:20,690
It has only two possible values.

152
00:08:21,690 --> 00:08:23,460
So what could I do?

153
00:08:23,460 --> 00:08:26,990
How could I transform that binary, that
0-1 variable,

154
00:08:26,990 --> 00:08:29,500
into something that better fits a line.

155
00:08:30,670 --> 00:08:31,630
Well, here is one thing.

156
00:08:31,630 --> 00:08:34,230
This is really just a starting point
because we're going to go further with

157
00:08:34,230 --> 00:08:37,900
this but let's just start with, with some
simple thing that we can do.

158
00:08:37,900 --> 00:08:42,820
So, recognize that instead of modelling it
as a yes no, a 0-1.

159
00:08:42,820 --> 00:08:46,130
What if we could somehow model it as a
probability?

160
00:08:47,250 --> 00:08:52,500
A probability, as you know, is a value
that ranges from 0-1 but

161
00:08:52,500 --> 00:08:55,290
it can take on any value between 0-1.

162
00:08:55,290 --> 00:08:57,030
So we would do a little bit better.

163
00:08:57,030 --> 00:09:01,890
We would put in all these values between
0-1 back into play,

164
00:09:01,890 --> 00:09:05,570
if somehow we could model this as, we
could make the outcome variable

165
00:09:05,570 --> 00:09:10,920
the probability of being book-smart,
rather than yes, no, you are book-smart.

166
00:09:10,920 --> 00:09:14,920
Just looking at the data here you can see
well, that's not immediate obvious,

167
00:09:14,920 --> 00:09:17,490
immediately obvious, how you would do it.

168
00:09:19,560 --> 00:09:23,140
In fact we are going to do something
called the logit which is a function of

169
00:09:23,140 --> 00:09:26,320
the probability and in fact what the
computer is going to do, is do

170
00:09:26,320 --> 00:09:31,709
a little algorithm involving some calculus
to calculate the probability or the logit.

171
00:09:31,709 --> 00:09:35,030
But we can kind of approximate it just
actually looking at the data.

172
00:09:35,030 --> 00:09:38,100
And I think this is the best way to
understand it.

173
00:09:38,100 --> 00:09:41,230
So if I wanted to get a probability here
rather than a 0-1 variable.

174
00:09:41,230 --> 00:09:42,440
Well imagine I did the following.

175
00:09:42,440 --> 00:09:43,970
What if I just,

176
00:09:43,970 --> 00:09:46,352
to get a probability I'm going to have to
look at more than one person.

177
00:09:46,352 --> 00:09:47,930
I'm going to have to look at a group of
people.

178
00:09:47,930 --> 00:09:53,150
So what if, I kind of arbitrarily looked
at, you know, a group of people.

179
00:09:53,150 --> 00:09:54,660
Let's just pick say.

180
00:09:54,660 --> 00:10:00,190
The, the ten people with the lowest amount
of homework time per week.

181
00:10:00,190 --> 00:10:03,750
Notice that these little red dots here,
actually some of these dots represent more

182
00:10:03,750 --> 00:10:07,930
than one person, because there were a lot
of people who had tied values.

183
00:10:07,930 --> 00:10:11,580
They selected the same number of homework
and they also called themselves book-smart

184
00:10:11,580 --> 00:10:15,050
for example so that, each of these dots
may represent more than one person.

185
00:10:15,050 --> 00:10:15,940
So imagine that this,

186
00:10:15,940 --> 00:10:21,940
this is what I've put in this box here
represents ten people.

187
00:10:21,940 --> 00:10:25,920
I could look at it, say the ten people
with the lowest amount of homework time,

188
00:10:25,920 --> 00:10:28,030
putting in the lowest amount of homework
time and

189
00:10:28,030 --> 00:10:31,470
I could count up how many of them called
themselves book smart.

190
00:10:31,470 --> 00:10:35,150
And how many of them called themselves
street smart.

191
00:10:35,150 --> 00:10:38,870
And I could calculate a proportion, a
percentage, a probability.

192
00:10:38,870 --> 00:10:39,430
So let's say.

193
00:10:39,430 --> 00:10:42,570
I'm just making this up, but let's say
when I calculate that against since some

194
00:10:42,570 --> 00:10:46,950
of these dots are overlapping I can't do
it exactly but, from the graphic.

195
00:10:46,950 --> 00:10:48,060
But let's say I count up and

196
00:10:48,060 --> 00:10:52,180
I find that among those doing the least
amount of homework.

197
00:10:52,180 --> 00:10:58,336
Let's say that 90% of them, again, I'm
just making this up, let's say 90% of that

198
00:10:58,336 --> 00:11:04,270
ten, nine out of ten, consider themselves
book smart rather than street smart.

199
00:11:04,270 --> 00:11:07,260
Now I've got a probability rather than a
0-1.

200
00:11:07,260 --> 00:11:10,490
And I could do this for the next group of
ten.

201
00:11:10,490 --> 00:11:12,550
The next highest in homework.

202
00:11:13,590 --> 00:11:20,080
Maybe in the next group, the probability
is again, I'm just making this up, is 85%.

203
00:11:20,080 --> 00:11:24,190
And I can't tell because again these dots
are overlapping.

204
00:11:24,190 --> 00:11:25,290
Maybe that comes out to be 85%.

205
00:11:25,290 --> 00:11:28,890
But I could go along and I could group
people.

206
00:11:28,890 --> 00:11:32,310
I have to group people in order to be able
to get a percentage or probability.

207
00:11:32,310 --> 00:11:34,760
But I could come up with some
probabilities.

208
00:11:34,760 --> 00:11:38,320
And of course the probabilities I come up
with are going to depend exactly on

209
00:11:38,320 --> 00:11:39,140
how I group them.

210
00:11:39,140 --> 00:11:44,020
But, just realize that the grouping is
just for the purposes of illustration.

211
00:11:44,020 --> 00:11:45,890
When you actually do this with calculus,

212
00:11:45,890 --> 00:11:48,240
you don't have to make any arbitrary
groupings.

213
00:11:48,240 --> 00:11:50,590
Of course we won't go through the calculus
here.

214
00:11:50,590 --> 00:11:52,910
But the computer does that for you.

215
00:11:52,910 --> 00:11:57,490
So if I could get that probability now, I
can make a graphic.

216
00:11:57,490 --> 00:12:01,460
Which is my predictor variable is the
number of hours of homework per week,

217
00:12:01,460 --> 00:12:03,230
the continuous variable.

218
00:12:03,230 --> 00:12:05,900
Now my outcome variable is a probability.

219
00:12:05,900 --> 00:12:08,051
A number that ranges between 0-1.

220
00:12:09,100 --> 00:12:11,470
I have fewer dots on this graphic.

221
00:12:11,470 --> 00:12:15,080
I'm going to just, for now talk about the
graphic on the left side, this 10 groups.

222
00:12:16,090 --> 00:12:18,000
I notice I have fewer dots on that
graphic.

223
00:12:18,000 --> 00:12:19,750
None of these dots are overlapping any
more.

224
00:12:19,750 --> 00:12:24,920
So I actually have a I had my previous
graphic represented 48 people.

225
00:12:24,920 --> 00:12:27,590
This graphic only represents ten
observations.

226
00:12:27,590 --> 00:12:32,494
I divided those 48 people up into roughly
even groups of

227
00:12:32,494 --> 00:12:35,533
10 groups of you know somewhere between of
about four or

228
00:12:35,533 --> 00:12:39,650
five per group and I got those percentages
and I plotted them here.

229
00:12:39,650 --> 00:12:40,810
What you notice is,

230
00:12:40,810 --> 00:12:46,100
this is much more amenable, this, this
scatter plot to fitting a line.

231
00:12:46,100 --> 00:12:48,710
Right now we've got dots all around the
range and

232
00:12:48,710 --> 00:12:51,950
you can imagine actually fitting a
meaningful line to those dots.

233
00:12:51,950 --> 00:12:55,190
As I mentioned, this is just for

234
00:12:55,190 --> 00:12:59,340
the purposes of graphing, just to show you
the concept.

235
00:12:59,340 --> 00:13:03,160
And so it's arbitrary how I group people,
so of course the graphics is

236
00:13:03,160 --> 00:13:05,400
going to look a little bit different
depending on how I group people.

237
00:13:05,400 --> 00:13:08,090
So just to show you that I also divided
into five groups.

238
00:13:08,090 --> 00:13:12,140
There's 48 people, so this is roughly
groups of, you know, eight, nine, or ten.

239
00:13:12,140 --> 00:13:15,340
And I calculate the proportion who
considered themselves book smart.

240
00:13:15,340 --> 00:13:19,310
That probability, in each of those five
groups and I plotted it here.

241
00:13:19,310 --> 00:13:24,220
By the way, when I'm each of these groups,
of course, has a range of homework values.

242
00:13:24,220 --> 00:13:28,240
I'm using the mean homework time for

243
00:13:28,240 --> 00:13:30,100
each group that's what I'm putting in the
plot here.

244
00:13:30,100 --> 00:13:33,870
So for example the lowest homework group
on this plot here,

245
00:13:33,870 --> 00:13:39,250
they have a mean homework time of just,
you know it maybe 30 minutes to an hour.

246
00:13:39,250 --> 00:13:43,780
And there, the proportion that consider
themselves book smart was right about 78%.

247
00:13:43,780 --> 00:13:49,070
So I'm using the mean just for graphical
purposes but the picture

248
00:13:49,070 --> 00:13:53,270
looks a little different depending on, how
you group it but you can get the idea.

249
00:13:53,270 --> 00:13:57,120
Now we've put back all the numbers between
0-1 into play it

250
00:13:57,120 --> 00:13:59,330
makes much more sense to feel a life.

251
00:13:59,330 --> 00:14:03,370
But this is still limited okay, so this is
not, we're going to go further than that

252
00:14:03,370 --> 00:14:08,750
with this because a probability is a
number that's bounded between 0-1.

253
00:14:08,750 --> 00:14:12,210
So it's still a bounded quantity, you
can't go lower than zero,

254
00:14:12,210 --> 00:14:14,130
you can't go higher than one.

255
00:14:14,130 --> 00:14:16,590
The problem with that is imagine when we
fit a line.

256
00:14:16,590 --> 00:14:20,560
When you fit a line, theoretically, that
line can go anywhere from negative,

257
00:14:20,560 --> 00:14:24,230
the y value can go anywhere from negative
infinity to positive infinity.

258
00:14:24,230 --> 00:14:25,830
So with this it's still not perfect.

259
00:14:27,710 --> 00:14:33,210
So what we're going to do, is instead of
directly having the probability

260
00:14:33,210 --> 00:14:36,020
be the outcome variable, we're going to
make a little transformation.

261
00:14:36,020 --> 00:14:39,010
So probability is bounded from 0-1.

262
00:14:39,010 --> 00:14:44,990
If I divide the probability, however, by
one minus the probability, that,

263
00:14:44,990 --> 00:14:48,380
of course, is what we had learned earlier
is the odds.

264
00:14:48,380 --> 00:14:52,330
So divide the probability by the
probability of something not happening to

265
00:14:52,330 --> 00:14:55,110
the probability divided by one minus the
probability.

266
00:14:55,110 --> 00:14:59,100
That's what we learned in week two of the
course, is in odds.

267
00:14:59,100 --> 00:15:01,010
Why would I want to do that?

268
00:15:01,010 --> 00:15:04,220
Well, a probability can't be bigger than
one but

269
00:15:04,220 --> 00:15:09,350
an odds can go from anywhere, anywhere
from zero to positive infinity.

270
00:15:09,350 --> 00:15:11,600
The odds has no right hand bound.

271
00:15:11,600 --> 00:15:15,390
So now we've put in all the positive real
numbers back into play.

272
00:15:16,920 --> 00:15:18,000
We're still missing the negatives.

273
00:15:18,000 --> 00:15:19,440
How can we get the negatives back in?

274
00:15:19,440 --> 00:15:21,530
So, here's where the natural log comes in.

275
00:15:21,530 --> 00:15:27,760
So, if I then take the natural log of the
odds, it turns out, that when you take

276
00:15:27,760 --> 00:15:33,730
a natural log, any value that was below
one now becomes a negative number.

277
00:15:33,730 --> 00:15:38,220
So this, the natural log of the odds,
actually ranges from negative,

278
00:15:38,220 --> 00:15:41,770
can theoretically range from negative
infinity to positive infinity.

279
00:15:41,770 --> 00:15:44,280
So now we put all the numbers back into
play.

280
00:15:44,280 --> 00:15:47,020
So now this is the most optimal
fore-fitting a line.

281
00:15:47,020 --> 00:15:51,750
So the outcome variable for logistic
regression, as I'd mentioned earlier,

282
00:15:51,750 --> 00:15:55,060
is actually going to be the natural log of
the odds of the outcome.

283
00:15:55,060 --> 00:15:59,500
A little complicated, but, it's for
mathematical reasons that we,

284
00:15:59,500 --> 00:16:00,260
we choose that.

285
00:16:01,660 --> 00:16:06,322
And just to illustrate, I took those, the
same groupings, ten groups and

286
00:16:06,322 --> 00:16:10,540
five groups where I had calculated the
percent, the probability of

287
00:16:10,540 --> 00:16:15,318
being book smart in each of those groups
and I transformed that into a log on.

288
00:16:15,318 --> 00:16:19,566
So that as I took the percentage, divided
one minus the percentage and

289
00:16:19,566 --> 00:16:22,430
then took the natural log of that number.

290
00:16:22,430 --> 00:16:24,640
This gives me some numbers like 0.5, 1.0,
1.5, 2.0.

291
00:16:24,640 --> 00:16:29,590
The, the log odds has no inherent meaning
to me.

292
00:16:29,590 --> 00:16:30,190
But that's okay,

293
00:16:30,190 --> 00:16:33,820
because when we eventually will translate
it back into something meaningful.

294
00:16:33,820 --> 00:16:36,895
But now this outcome variable is a number
that

295
00:16:36,895 --> 00:16:40,710
can theoretically go anywhere from
negative infinity to positive infinity.

296
00:16:40,710 --> 00:16:43,010
Makes a very fit, nicer fore-fitting a
line.

297
00:16:43,010 --> 00:16:46,260
I'm graphing here the logit for each of
these groups against, again,

298
00:16:46,260 --> 00:16:49,270
the mean homework time in each of those
groups.

299
00:16:50,800 --> 00:16:53,440
And you could fit a line to that.

300
00:16:53,440 --> 00:16:55,250
And that's what we're doing in logistic
regression.

301
00:16:55,250 --> 00:16:56,760
That's the line that we're fitting.

302
00:16:56,760 --> 00:16:59,750
So we're going to estimate an alpha and a
beta, an intercept and

303
00:16:59,750 --> 00:17:01,380
a slope for that particular line.

304
00:17:02,690 --> 00:17:06,030
As I mentioned, the groupings here are
completely arbitrary.

305
00:17:06,030 --> 00:17:09,860
When you actually do this in a computer,
there's calculus going on and so

306
00:17:09,860 --> 00:17:12,830
you're going to get the best alpha and the
best beta.

307
00:17:12,830 --> 00:17:17,050
When I do this graphically, it's a little
bit of an approximation.

308
00:17:17,050 --> 00:17:20,660
You can see that the line is slightly
different when I group it into ten groups

309
00:17:20,660 --> 00:17:24,050
than when I group it into five groups,
these lines are slightly different.

310
00:17:24,050 --> 00:17:28,310
But we can actually kind of look at these
graphics and just kind of

311
00:17:28,310 --> 00:17:31,740
visually approximate the equation of that
light, so I'm going to do that now.

312
00:17:33,370 --> 00:17:36,150
This is not as good as doing the calculus
in the computer, but

313
00:17:36,150 --> 00:17:38,610
it is, it'll give us a little bit of an
approximation, and

314
00:17:38,610 --> 00:17:41,730
I think is also good for illustrating
what's actually going on here.

315
00:17:41,730 --> 00:17:43,610
So what is the equation of this line?

316
00:17:43,610 --> 00:17:45,080
There's two lines here, and

317
00:17:45,080 --> 00:17:49,040
we can kind of say the, the real line is
going to be somewhere around these two.

318
00:17:49,040 --> 00:17:51,610
So first of all what's the intercept
value.

319
00:17:51,610 --> 00:17:55,220
So if I just look at the graphic, my
intercept for this line over on the left

320
00:17:55,220 --> 00:18:01,180
side is then, I guess about 1.6 you know,
I'm just approximating here.

321
00:18:01,180 --> 00:18:03,200
And the intercept over for

322
00:18:03,200 --> 00:18:07,300
the line on the right hand side of the
page is maybe about 1.8.

323
00:18:07,300 --> 00:18:12,570
So let's just kind of call it that the,
the equation for this line.

324
00:18:12,570 --> 00:18:15,390
Again, if we're kind of saying there's one
line here that we're trying to

325
00:18:15,390 --> 00:18:17,220
approximate, maybe it's around 1.7.

326
00:18:17,220 --> 00:18:18,900
I'm just making,

327
00:18:18,900 --> 00:18:23,620
somewhere in that ballpark, maybe around
1.7 is the intercept.

328
00:18:23,620 --> 00:18:27,060
How would I get the slopes of these lines,
just visually here,

329
00:18:27,060 --> 00:18:29,510
how could I approximate the slopes of
these lines.

330
00:18:29,510 --> 00:18:34,670
So what's the slope is rise over run, so I
can say well, how much have I gone up

331
00:18:34,670 --> 00:18:39,249
in the y variable and how much have I gone
up in the x variable.

332
00:18:40,340 --> 00:18:43,410
So, I can just kind of look at this as a
triangle here, and say,

333
00:18:43,410 --> 00:18:48,510
well, what's the, how far did I go on the,
on the x-axis, I went from about 0 to 40.

334
00:18:48,510 --> 00:18:52,550
So, the x change is about 40, here, for
this triangle.

335
00:18:52,550 --> 00:18:55,680
On the y end of things, yeah, if you
kind of approximate it,

336
00:18:55,680 --> 00:19:00,450
it's maybe going from about 0.3 maybe to
1.6.

337
00:19:00,450 --> 00:19:04,140
I'm going to just call it 1.3.

338
00:19:04,140 --> 00:19:10,510
So, sort of the, you, the, we go up 1.3,
in the log odds.

339
00:19:10,510 --> 00:19:14,200
So the slope here is going to be rise over
run, so, 1.3 divided by 40,

340
00:19:14,200 --> 00:19:15,800
or approximately 0.03.

341
00:19:15,800 --> 00:19:17,360
So the slope here is about 0.03.

342
00:19:17,360 --> 00:19:20,439
For this line we can use the same kind of
logic.

343
00:19:22,220 --> 00:19:24,790
So, for the x we went over about 35 here.

344
00:19:24,790 --> 00:19:26,530
That's about 0 to 35.

345
00:19:26,530 --> 00:19:29,590
On the y end we've gone up a little bit
bigger,

346
00:19:29,590 --> 00:19:33,230
maybe about 1.8 from one to about 1.8.

347
00:19:33,230 --> 00:19:37,900
The slope then would be 1.8 divided by 35
which is about 0.05.

348
00:19:37,900 --> 00:19:41,008
So, you know, the somewhere, I'm going to
approximate somewhere between 0.03 and

349
00:19:41,008 --> 00:19:42,410
0.05, let's call it 0.04.

350
00:19:42,410 --> 00:19:44,210
And, of course, this is negative.

351
00:19:44,210 --> 00:19:47,950
I should emphasize here that this is a
downward slope.

352
00:19:47,950 --> 00:19:52,020
We've gone down 1.3 or down 1.8 in y.

353
00:19:52,020 --> 00:19:53,440
So both of these slopes are negative.

354
00:19:53,440 --> 00:19:56,260
So on average, the slope's maybe somewhere
around negative 0.04.

355
00:19:56,260 --> 00:19:58,600
This is just my graphical approximation,
but

356
00:19:58,600 --> 00:20:01,270
that's what we're doing on logistic
regression.

357
00:20:01,270 --> 00:20:02,400
That's the line we're fitting.

358
00:20:03,870 --> 00:20:06,310
Now I'm actually going to put the data in
the computer, and

359
00:20:06,310 --> 00:20:10,730
let the computer use the beautiful
calculus to estimate this perfectly.

360
00:20:10,730 --> 00:20:16,730
To get the actual best intercept and best
beta slope that fits these data.

361
00:20:16,730 --> 00:20:19,220
And I'll show you that our, our
approximation wasn't bad.

362
00:20:19,220 --> 00:20:22,256
Here's what the compute comes out with
when it does what's called the maximum

363
00:20:22,256 --> 00:20:23,400
likelihood estimation.

364
00:20:23,400 --> 00:20:25,130
This involves calculus.

365
00:20:25,130 --> 00:20:27,290
And it gives us some maximum likelihood
estimates.

366
00:20:27,290 --> 00:20:30,090
I'm not going to go into the details of
maximum likelihood estimation.

367
00:20:30,090 --> 00:20:33,880
That's for another course, but this is
just a way that, using some calculus,

368
00:20:33,880 --> 00:20:38,530
that we're getting the best alpha and the
best beta here, the best fit line.

369
00:20:38,530 --> 00:20:42,070
So, when I do this on the computer, the
intercept comes out to be just over two,

370
00:20:42,070 --> 00:20:45,110
so just slightly different than what we
did, got in our approximation.

371
00:20:45,110 --> 00:20:49,650
The slope came out to be right around
negative 0.04, negative 0.039.

372
00:20:49,650 --> 00:20:51,550
So, and where did I get those in the
output?

373
00:20:51,550 --> 00:20:53,530
Here's the estimate for the intercept.

374
00:20:53,530 --> 00:20:54,940
That's the alpha estimate.

375
00:20:54,940 --> 00:20:56,480
Here's the estimate for the beta.

376
00:20:56,480 --> 00:20:59,320
That's the estimate for the slope.

377
00:20:59,320 --> 00:21:02,900
Along with the alpha and the beta, we also
get a p value to tell us whether that

378
00:21:02,900 --> 00:21:06,550
alpha or beta is statistically different
than zero.

379
00:21:06,550 --> 00:21:11,390
A beta of zero would mean no association,
same as it did with linear regression.

380
00:21:11,390 --> 00:21:14,940
So the intercept is clearly significantly
different than zero.

381
00:21:14,940 --> 00:21:17,130
Again, that's not all that meaningful
here,

382
00:21:17,130 --> 00:21:20,230
because we wouldn't expect the intercept
to be zero.

383
00:21:20,230 --> 00:21:24,640
However, the, the p value for homework is
not statistically significant.

384
00:21:24,640 --> 00:21:25,380
So as you might have guessed,

385
00:21:25,380 --> 00:21:28,520
just looking at the picture, there isn't a
strong association here.

386
00:21:28,520 --> 00:21:33,010
So it's not statistically different than
zero, probably meaning it doesn't look

387
00:21:33,010 --> 00:21:36,210
like there's a lot of evidence that
homework and whether or

388
00:21:36,210 --> 00:21:38,550
not you consider your book, yourself book
smart are related.

389
00:21:39,700 --> 00:21:41,020
But here's the final model.

390
00:21:41,020 --> 00:21:45,960
The log odds of your champion, the log
odds of being book smart, or

391
00:21:45,960 --> 00:21:50,750
considering yourself book smart, is equal
to the intercept 2.12, minus the slope,

392
00:21:50,750 --> 00:21:52,030
point 0.039.

393
00:21:52,030 --> 00:21:55,770
And we would, of course, multiply that
beta times the amount of homework.

394
00:21:55,770 --> 00:21:57,890
So this is a continuous predictor.

395
00:21:57,890 --> 00:22:01,060
So we would be multiplying the amount of
homework for

396
00:22:01,060 --> 00:22:03,390
a particular person times that slope.

397
00:22:05,580 --> 00:22:07,850
What's this logistic regression equation
useful for?

398
00:22:07,850 --> 00:22:10,640
So, just like when we talked about linear
regression,

399
00:22:10,640 --> 00:22:14,320
there's many couple different pote,
potential functions here.

400
00:22:14,320 --> 00:22:17,970
So one thing is we could use the logistic
regression equation model to try to

401
00:22:17,970 --> 00:22:19,780
do some kind of prediction.

402
00:22:19,780 --> 00:22:21,850
This is very useful in clinical
applications,

403
00:22:21,850 --> 00:22:25,770
because often we want to classify people
as having a condition, or

404
00:22:25,770 --> 00:22:28,940
not having a condition, that's a binary
outcome.

405
00:22:28,940 --> 00:22:31,120
So, if we're doing that kind of
prediction,

406
00:22:31,120 --> 00:22:34,080
we would likely be using logistic
regression.

407
00:22:34,080 --> 00:22:37,690
So we can try to come up with a set of

408
00:22:37,690 --> 00:22:43,510
predictors that predicts who's going to
have a disease or not have a disease.

409
00:22:43,510 --> 00:22:45,870
So that's one of the functions of logistic
regression prediction.

410
00:22:47,030 --> 00:22:53,320
A second function of logistic regression
which is probably used even more often,

411
00:22:53,320 --> 00:22:56,690
is just to try to say, is a particular
predictor,

412
00:22:56,690 --> 00:23:00,640
like homework, related to the outcome.

413
00:23:00,640 --> 00:23:01,940
In this case, book smart or

414
00:23:01,940 --> 00:23:06,440
street smart, are those variables related,
these are treatment related to an outcome.

415
00:23:06,440 --> 00:23:08,800
And, that's very useful in an,

416
00:23:08,800 --> 00:23:12,390
in and of itself, and of course, we can
say, is that predictor,

417
00:23:12,390 --> 00:23:17,360
that treatment, related to the outcome
after adjusting for potential confounders.

418
00:23:17,360 --> 00:23:22,060
So that's another very important use of
the logistic regression model.

419
00:23:22,060 --> 00:23:26,580
I'm going to start by talking just briefly
about the first of these aims which is

420
00:23:26,580 --> 00:23:27,780
the prediction end of things.

421
00:23:29,550 --> 00:23:31,920
Again, here is the logistic regression
model.

422
00:23:31,920 --> 00:23:35,702
We've modeled the outcome variable, is the
log odds of the outcome the,

423
00:23:35,702 --> 00:23:37,360
we're modeling that as a line.

424
00:23:39,790 --> 00:23:42,500
Let me take that model that we just fit
and

425
00:23:42,500 --> 00:23:45,460
how would we actually apply it to make
some predictions?

426
00:23:47,000 --> 00:23:49,850
So, if you're somebody who does no
homework per week.

427
00:23:49,850 --> 00:23:51,500
Zero hours of homework per week.

428
00:23:51,500 --> 00:23:57,350
You could plug in that [COUGH] value for
homework of zero into the model.

429
00:23:57,350 --> 00:24:00,360
You get that you're predicted logit is
2.12.

430
00:24:00,360 --> 00:24:04,200
As I've mentioned, most of us don't think
in logit, so in a minute,

431
00:24:04,200 --> 00:24:08,060
we're going to do something with that
number to make it more interpretable but

432
00:24:08,060 --> 00:24:10,810
if you did ten hours per week of homework,
you could plug in ten.

433
00:24:10,810 --> 00:24:12,930
The predicted logit there was 1.73.

434
00:24:12,930 --> 00:24:17,430
If you did 50 hours per week, the
predicted logit is 0.17.

435
00:24:17,430 --> 00:24:21,240
So the more homework you do the more the
logit goes down.

436
00:24:21,240 --> 00:24:23,890
Well I don't want to deal with a logit,
because the logit I,

437
00:24:23,890 --> 00:24:25,330
I can't think in logit.

438
00:24:25,330 --> 00:24:29,470
However, we can get this, notice the
[UNKNOWN], there's a probability in there.

439
00:24:29,470 --> 00:24:32,900
What we want to get out is the predicted
probability of the outcome.

440
00:24:32,900 --> 00:24:36,960
The predictive probability of considering
yourself book smart in this case.

441
00:24:36,960 --> 00:24:40,360
I want to isolate this predicted
probability.

442
00:24:40,360 --> 00:24:41,880
Well it's actually not that hard to do,
right.

443
00:24:41,880 --> 00:24:43,610
We've got one equation one unknown.

444
00:24:43,610 --> 00:24:45,900
We can isolate that probability.

445
00:24:45,900 --> 00:24:46,800
So, how do we do it?

446
00:24:47,960 --> 00:24:50,264
First thing that we're going to want to do
is,

447
00:24:50,264 --> 00:24:52,730
we are going to want to exponentiate both
sides.

448
00:24:52,730 --> 00:24:55,060
So for example the zero hours per week.

449
00:24:55,060 --> 00:24:59,051
That person would be predicted to have a
logit of 2.12.

450
00:24:59,051 --> 00:25:02,750
I can then exponentiate both sides of
those, of that equation.

451
00:25:02,750 --> 00:25:08,010
So that would tell me that the odds for
somebody who does zero homework per

452
00:25:08,010 --> 00:25:12,980
week is the exponential raised to 2.12.

453
00:25:12,980 --> 00:25:16,720
That value is 8.33, if you actually
calculate out.

454
00:25:16,720 --> 00:25:18,811
So e raised to 2.2 is 8.33.

455
00:25:18,811 --> 00:25:22,700
That's the odds for a person who does the
odds of being book smart, for

456
00:25:22,700 --> 00:25:25,750
a person who does zero hours per week of
homework.

457
00:25:25,750 --> 00:25:27,110
Then we could exponentiate this 1.73,

458
00:25:27,110 --> 00:25:31,340
that will give me the odds for somebody
who does ten hours per week, that's 5.64.

459
00:25:31,340 --> 00:25:35,560
We could exponentiate the 0.17, that gives
me the odds for

460
00:25:35,560 --> 00:25:38,680
somebody who does 50 hours per week, that
comes out to be 1.19.

461
00:25:38,680 --> 00:25:42,110
So your odds of being book smart again
goes down as

462
00:25:42,110 --> 00:25:43,620
you do more homework per week.

463
00:25:43,620 --> 00:25:45,970
Sort of the opposite of what you might
expect.

464
00:25:45,970 --> 00:25:47,940
But of course this is not statistically
significant.

