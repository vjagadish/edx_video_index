1
00:00:07,338 --> 00:00:08,630
[BLANK_AUDIO].
In this next module,

2
00:00:08,630 --> 00:00:12,354
I'm going to introduce you to the concept
of covariance and

3
00:00:12,354 --> 00:00:16,352
we'll talk about linear correlations.

4
00:00:16,352 --> 00:00:18,910
First of all, I want to review from last
week.

5
00:00:18,910 --> 00:00:22,713
We talked about the fact that ttests,
ANOVA, linear correlation, and

6
00:00:22,713 --> 00:00:27,309
linear regression all belong to the same
family of models.

7
00:00:27,309 --> 00:00:29,394
They're actually mathematically doing.

8
00:00:29,394 --> 00:00:31,386
All those tests are doing exactly the same
thing.

9
00:00:31,386 --> 00:00:34,483
You could draw that mathematically they
are identical.

10
00:00:34,483 --> 00:00:38,028
We talked about the assumptions that all
of those linear models have.

11
00:00:38,028 --> 00:00:43,433
One of the assumptions was that the
outcome variable is normally distributed.

12
00:00:43,433 --> 00:00:46,855
Of course, it turns out that these models
are quite robust against this

13
00:00:46,855 --> 00:00:49,898
assumptions as I showed you last week.

14
00:00:49,898 --> 00:00:53,706
We have something called the central limit
theorem which tells us that the means,

15
00:00:53,706 --> 00:00:56,898
remember ttests, ANOVA, linear regression
or all looking at means,

16
00:00:56,898 --> 00:01:00,426
tells us the means will be normally
distributed if your sample size is large

17
00:01:00,426 --> 00:01:06,975
enough even when the underlying trait that
your looking at is highly skewed.

18
00:01:06,975 --> 00:01:09,140
Eventually the central limit theorem kicks
in.

19
00:01:09,140 --> 00:01:13,250
You can't guarantee exactly where that's
going to kick in.

20
00:01:13,250 --> 00:01:15,850
So if you're dealing with small samples,
under a 100,

21
00:01:15,850 --> 00:01:18,340
you have to worry about the normality
assumption.

22
00:01:18,340 --> 00:01:21,500
Usually as long as our sample size is
above 100.

23
00:01:21,500 --> 00:01:24,880
Most times unless you have an extremely
skewed distribution,

24
00:01:24,880 --> 00:01:26,905
that central limit there will, will apply.

25
00:01:26,905 --> 00:01:30,220
But for small symbols we're going to worry
about this and I'll show

26
00:01:30,220 --> 00:01:35,270
you some ways to evaluate the, this normal
assumption for linear regression.

27
00:01:35,270 --> 00:01:39,086
We also have an assumption called the
homogeneity of variances.

28
00:01:39,086 --> 00:01:40,454
For the ttest and ANOVA,

29
00:01:40,454 --> 00:01:44,846
that assumption said that in different
groups, in the different groups we

30
00:01:44,846 --> 00:01:49,030
were comparing the variances across the
different groups had to be the same.

31
00:01:49,030 --> 00:01:53,460
This week we're going to talk about linear
correlation and linear regression.

32
00:01:53,460 --> 00:01:56,390
The assumption is the same except we might
be

33
00:01:56,390 --> 00:01:58,610
talking about now a continuous predictor.

34
00:01:58,610 --> 00:02:03,070
Instead of comparing groups our predictor
variable might also be continuous.

35
00:02:03,070 --> 00:02:04,730
So how this assumption works for

36
00:02:04,730 --> 00:02:08,130
continuous predictors is it says that the
variances for

37
00:02:08,130 --> 00:02:13,670
the outcome variable have to be equal at
all levels of the predictor variable.

38
00:02:13,670 --> 00:02:16,900
Again, models are quite robust against
this assumption.

39
00:02:16,900 --> 00:02:20,310
That means if you have slight differences
in the variances at different levels of

40
00:02:20,310 --> 00:02:24,210
the predictor variable, it's not going to,
be a big problem.

41
00:02:24,210 --> 00:02:27,580
We need to worry about this when there are
really big violations of

42
00:02:27,580 --> 00:02:29,120
homogeneity of variances.

43
00:02:29,120 --> 00:02:32,100
I'm going to show how to do something
called residual analysis today,

44
00:02:32,100 --> 00:02:34,332
to, to check this assumption for linear
regression.

45
00:02:34,332 --> 00:02:39,826
Just keep these two assumptions in the
back of your mind.

46
00:02:39,826 --> 00:02:43,310
[SOUND] I'm going to start here by talking
about Pearson's correlation coefficient,

47
00:02:43,310 --> 00:02:46,390
which is again directly related to linear
regression.

48
00:02:46,390 --> 00:02:50,280
So we've got a continuous outcome and
we've got observations that

49
00:02:50,280 --> 00:02:55,590
are independent and we want to compare a
continuous to that continuous outcome.

50
00:02:55,590 --> 00:02:58,050
So we have now both a continuous predictor
and

51
00:02:58,050 --> 00:03:02,840
a continuous outcome variable, we're going
to use Pearson's correlation coefficient.

52
00:03:02,840 --> 00:03:06,030
Just to give you an example, in the first
week of the course,

53
00:03:06,030 --> 00:03:10,770
I showed you some data from that I
collected from my Stanford classes I

54
00:03:10,770 --> 00:03:15,120
had a simple little data set of 50
observations, 50 students and

55
00:03:15,120 --> 00:03:19,460
I had asked them things about coffee
drinking and their political bent.

56
00:03:19,460 --> 00:03:22,770
I'm showing you here some data relating to
variables.

57
00:03:22,770 --> 00:03:26,720
In the first week of the course, I showed
you how to look at one variable at a time.

58
00:03:26,720 --> 00:03:29,920
Today we're going to be trying to relate
variables.

59
00:03:29,920 --> 00:03:31,570
You see how variables go together.

60
00:03:31,570 --> 00:03:35,950
Here I'm showing you on the x axis is the
variable political bent.

61
00:03:35,950 --> 00:03:38,910
I asked the students to rate themselves
from zero to 100.

62
00:03:38,910 --> 00:03:42,340
Zero being the most conservative, 100
being the most liberal.

63
00:03:42,340 --> 00:03:46,300
I also asked the students to rate their
feelings about President Obama,

64
00:03:46,300 --> 00:03:49,800
whether it's favorable or whether he, they
favor him or not.

65
00:03:49,800 --> 00:03:54,330
So zero was an unfavorable rating, a 100
was the highest and most favorable rating.

66
00:03:54,330 --> 00:03:57,840
Not surprisingly, as you would expect,

67
00:03:57,840 --> 00:04:01,674
as people's political bent went up towards
100, towards the more liberal end.

68
00:04:01,674 --> 00:04:05,510
Their ratings of President Obama also went
up towards a 100.

69
00:04:05,510 --> 00:04:08,900
You can see that relationship displayed
here.

70
00:04:08,900 --> 00:04:12,750
There's a definitely a strong relationship
between your political leanings and

71
00:04:12,750 --> 00:04:15,340
how you feel about President Obama; again,
not surprisingly.

72
00:04:15,340 --> 00:04:15,840
[INAUDIBLE].

73
00:04:16,930 --> 00:04:20,320
How would we statistically evaluate this
relationship?

74
00:04:20,320 --> 00:04:24,580
Now notice that the graphic is a great way
to evaluate this relationship.

75
00:04:24,580 --> 00:04:26,990
We already know a ton just from plotting
it.

76
00:04:26,990 --> 00:04:29,170
So all this stuff I'm going to be talking
about today,

77
00:04:29,170 --> 00:04:32,285
where it's going to be very important to
plot your variables, as you'll see.

78
00:04:32,285 --> 00:04:35,570
But what if we wanted to add one more
piece of information?

79
00:04:35,570 --> 00:04:38,540
An actual statistic, and a p-value to this
graphic.

80
00:04:38,540 --> 00:04:39,680
What statistic could we use?

81
00:04:39,680 --> 00:04:43,720
Our statistical question here is,

82
00:04:43,720 --> 00:04:49,000
is the political bent of a person related
to how they rate President Obama.

83
00:04:49,000 --> 00:04:50,797
So what's the outcome variable here.

84
00:04:50,797 --> 00:04:55,410
Because I graphed the ratings of Obama as
the y variable,

85
00:04:55,410 --> 00:04:57,450
we're assuming that's the outcome
variable.

86
00:04:57,450 --> 00:05:00,850
So, the outcome here is the ratings of
Obama on that scale from 0 to 100.

87
00:05:00,850 --> 00:05:04,040
It's a continuous variable.

88
00:05:04,040 --> 00:05:07,230
We of course, have to ask the question, is
it normally distributed?

89
00:05:07,230 --> 00:05:09,370
Well, let me show you the distribution.

90
00:05:09,370 --> 00:05:13,565
So here's the distribution for ratings of
Obama here's the histogram.

91
00:05:13,565 --> 00:05:18,070
It turns out that it's not exactly a
perfect normal curve, right?

92
00:05:18,070 --> 00:05:20,630
You can see there's a tiny bit of a left
heel here.

93
00:05:20,630 --> 00:05:25,830
Those are the republicans, right in the,
in the class, but it's pretty close.

94
00:05:25,830 --> 00:05:29,910
We have 50 people, 50 observations in this
data set.

95
00:05:29,910 --> 00:05:33,200
By the time you get 50 people, if you
start with a distribution that's

96
00:05:33,200 --> 00:05:36,180
reasonably normal, even if it's a little
bit off like this.

97
00:05:36,180 --> 00:05:39,100
There's pretty good guarantee that the
central limit there is going to kick in.

98
00:05:39,100 --> 00:05:42,860
By the time I get to 50 people, when I
start with something that's just a little

99
00:05:42,860 --> 00:05:45,980
off from normal here, the means are
probably going to be normally distributed.

100
00:05:45,980 --> 00:05:50,130
So for my purposes, for this linear
correlation evaluation,

101
00:05:50,130 --> 00:05:53,850
I'm going to call this close enough to a
normal distribution.

102
00:05:53,850 --> 00:05:56,660
If I only had, maybe, ten people in the
sample, I might not think that

103
00:05:56,660 --> 00:06:00,180
this was close enough, and I might worry
about the normal assumption.

104
00:06:00,180 --> 00:06:02,635
But this is not that far off of a normal,
and for

105
00:06:02,635 --> 00:06:05,810
50 people, I'm going to say close enough.

106
00:06:05,810 --> 00:06:06,960
Are the observations correlated?

107
00:06:06,960 --> 00:06:10,090
No, I just have 50 independent students,
remember, in this data set.

108
00:06:10,090 --> 00:06:11,350
Are we comparing groups here?

109
00:06:11,350 --> 00:06:14,040
No in this case, the independent variable,

110
00:06:14,040 --> 00:06:18,220
last week, we did all these statistical
tests where we compared groups.

111
00:06:18,220 --> 00:06:21,840
But this week I'm talking about the
situations where the independent variable

112
00:06:21,840 --> 00:06:22,740
may not be groups.

113
00:06:22,740 --> 00:06:26,310
May not be categorical, it may be a
continuous variable as it is here.

114
00:06:26,310 --> 00:06:29,640
So if you've got a continuous predictor,
and a continuous outcome variable,

115
00:06:29,640 --> 00:06:32,770
your outcome is normally distributed or
close enough.

116
00:06:32,770 --> 00:06:37,615
And you have independence, then that leads
us to Pearson's correlation coefficient.

117
00:06:37,615 --> 00:06:41,800
I've actually talked about the correlation
coefficient already,

118
00:06:41,800 --> 00:06:45,005
in week five of the course, I actually
mentioned it last week as well.

119
00:06:45,005 --> 00:06:48,370
So as I mentioned, the [INAUDIBLE] the
correlation coefficient is

120
00:06:48,370 --> 00:06:51,890
a value that ranges from negative one to
positive one.

121
00:06:51,890 --> 00:06:54,630
Positive one is perfect positive
correlation.

122
00:06:54,630 --> 00:06:56,300
Here we can see that r,

123
00:06:56,300 --> 00:06:59,680
we represent the correlation coefficient,
we abbreviate that as r.

124
00:06:59,680 --> 00:07:03,270
So we can see that r, the correlation
coefficient is 0.78.

125
00:07:03,270 --> 00:07:04,260
So that's pretty high.

126
00:07:04,260 --> 00:07:07,532
As you can see from the graphc, there is a
strong correlation here.

127
00:07:07,532 --> 00:07:09,950
The p value is less than 0.0001.

128
00:07:09,950 --> 00:07:13,600
The null hypothesis here is that r is
zero.

129
00:07:13,600 --> 00:07:16,140
Because an r of zero would mean no
correlation.

130
00:07:16,140 --> 00:07:19,830
So all that p-value tells us here is that
we can be pretty confident that these two

131
00:07:19,830 --> 00:07:21,620
variables are really correlated.

132
00:07:21,620 --> 00:07:23,540
That there is some correlation here.

133
00:07:25,320 --> 00:07:29,300
I'm going to now walk you through where
the correlation coefficient comes from.

134
00:07:29,300 --> 00:07:31,100
I haven't told you that yet in the course,
so

135
00:07:31,100 --> 00:07:32,656
now I'm going to tell you where it comes
from.

136
00:07:32,656 --> 00:07:36,999
So to understand the correlation
coefficient we have to actually go

137
00:07:36,999 --> 00:07:40,330
back and introduce a new concept called
covariance.

138
00:07:40,330 --> 00:07:42,180
So let me start by talking about
covariance and

139
00:07:42,180 --> 00:07:45,430
then you'll see how covariance leads into
the correlation coefficient.

140
00:07:45,430 --> 00:07:48,300
So here I'm going to start with the
formula for covariance.

141
00:07:48,300 --> 00:07:50,215
The formula is actually instructive here.

142
00:07:50,215 --> 00:07:54,619
Covariance is evaluating the relationship
between 2 variables.

143
00:07:54,619 --> 00:07:57,950
But if you look carefully at this
covariance formula you

144
00:07:57,950 --> 00:08:01,820
might recognize that it looks awfully
similar to the formula for

145
00:08:01,820 --> 00:08:04,860
variance that I gave you in the first week
of the course.

146
00:08:04,860 --> 00:08:05,530
Then of course the,

147
00:08:05,530 --> 00:08:09,040
the title covariance will give you a clue
that it's related to variance.

148
00:08:09,040 --> 00:08:11,600
I'm just going to walk you through what
this formula means,

149
00:08:11,600 --> 00:08:13,750
what it actually produces.

150
00:08:13,750 --> 00:08:18,030
What we're talking about here is when
you've measured two variables on everyone

151
00:08:18,030 --> 00:08:20,780
in your dataset.

152
00:08:20,780 --> 00:08:24,788
So xi and yi would represent, say, you
measured height and weight on people.

153
00:08:24,788 --> 00:08:27,920
X1 and y1 would be the height and

154
00:08:27,920 --> 00:08:31,200
the weight of the first person in your
dataset.

155
00:08:31,200 --> 00:08:32,700
So you've got pairs of heights and

156
00:08:32,700 --> 00:08:36,190
weights that are related because they are,
they belong to the same person.

157
00:08:36,190 --> 00:08:38,380
What are we doing with covariance?

158
00:08:38,380 --> 00:08:41,910
We are trying to estimate, we are trying
to decide whether or

159
00:08:41,910 --> 00:08:45,190
not two variables are, are related to one
another.

160
00:08:45,190 --> 00:08:47,610
Do they vary together?

161
00:08:47,610 --> 00:08:51,300
So imagine you've got, imagine x is height
and y is weight.

162
00:08:51,300 --> 00:08:55,890
And x bar therefore is the mean height in
your sample and

163
00:08:55,890 --> 00:08:59,180
y bar is the mean weight in your sample.

164
00:08:59,180 --> 00:09:01,023
Imagine you've got a person who's really
tall.

165
00:09:01,023 --> 00:09:08,610
So there, that xi imagine that xi is
somebody who's, say a foot above average.

166
00:09:08,610 --> 00:09:10,630
So if you do XI minus X bar for

167
00:09:10,630 --> 00:09:14,615
somebody who's really tall your going to
get a positive deviation from the mean.

168
00:09:14,615 --> 00:09:19,110
If somebody's really tall because height
and weight tend to go together.

169
00:09:19,110 --> 00:09:20,870
Some, if somebody is extremely tall there,

170
00:09:20,870 --> 00:09:24,150
they are also likely to be heavier on
average.

171
00:09:24,150 --> 00:09:26,550
So it's very likely that if you're very
tall,

172
00:09:26,550 --> 00:09:28,050
you're also going to weigh more than
average.

173
00:09:28,050 --> 00:09:31,275
That means the, the deviation for

174
00:09:31,275 --> 00:09:36,360
weighty yi minus y bar is likely to also
be positive.

175
00:09:36,360 --> 00:09:40,810
Will always be but if height and weight
tend to go together positively.

176
00:09:40,810 --> 00:09:43,050
It's, will tend to be that you're going to
end up when you're going to end up with

177
00:09:43,050 --> 00:09:46,600
positive for the deviation for height, you
will also end up with a positive for

178
00:09:46,600 --> 00:09:47,450
the deviation for weight.

179
00:09:47,450 --> 00:09:51,890
So when you multiply together two
positives, of course, you get a positive.

180
00:09:51,890 --> 00:09:54,930
Now let's say you were person who was
short.

181
00:09:54,930 --> 00:09:58,840
So if you're, then your deviation for
height would tend to be negative.

182
00:09:58,840 --> 00:10:01,840
But because you're shorter, you'd probably
also tend to weigh less, so

183
00:10:01,840 --> 00:10:05,650
it would be likely that your deviation for
weight would also be negative.

184
00:10:05,650 --> 00:10:08,420
When you multiply together two negatives,
you get a positive.

185
00:10:08,420 --> 00:10:11,590
So if two things are positively related to
each other, like height and weight.

186
00:10:11,590 --> 00:10:14,070
Your [INAUDIBLE] the covariance will be
something positive.

187
00:10:15,770 --> 00:10:18,740
Now, let's take the case where we have two
things that are inversely related.

188
00:10:18,740 --> 00:10:20,870
Two variables that are inversely related.

189
00:10:20,870 --> 00:10:27,850
So, say if you party a lot in college,
perhaps the GPA will be lower.

190
00:10:27,850 --> 00:10:29,670
Right?
So if you're partying too much,

191
00:10:29,670 --> 00:10:33,060
you're not studying as much, maybe that
will lower your GPA.

192
00:10:33,060 --> 00:10:35,240
Those things might be inversely related.

193
00:10:35,240 --> 00:10:38,140
So let's just say you're somebody who
likes to party a lot,

194
00:10:38,140 --> 00:10:41,222
you're going to have your xi minus x bar,
your deviation for

195
00:10:41,222 --> 00:10:44,040
partying, you're going to be above average
on partying.

196
00:10:44,040 --> 00:10:46,655
So you're going to get a positive for xi
minus x bar.

197
00:10:46,655 --> 00:10:49,980
But, because you're above average on
partying,

198
00:10:49,980 --> 00:10:52,160
maybe you're below average on GPA.

199
00:10:52,160 --> 00:10:55,260
So for you deviation for GPA, maybe you're
going to get a negative.

200
00:10:55,260 --> 00:10:57,880
So, what's a positive time-, times a
negative?

201
00:10:57,880 --> 00:10:59,310
Well, that's going to be a negative.

202
00:10:59,310 --> 00:11:02,920
And similarly, if you don't party as much,
maybe your, your deviation for

203
00:11:02,920 --> 00:11:04,030
partying is negative.

204
00:11:04,030 --> 00:11:06,090
Maybe that means your GPA is going to be
higher.

205
00:11:06,090 --> 00:11:08,500
A negative times a positive is, again, a
negative.

206
00:11:08,500 --> 00:11:13,780
So, two variables that are inversely
related will have negative covariance.

207
00:11:13,780 --> 00:11:16,890
Finally, let's talk about two variables
that are not related at all.

208
00:11:16,890 --> 00:11:20,028
So, let's say, height and your GPA in
college.

209
00:11:20,028 --> 00:11:25,450
Probably not related, so let's say if
you're height is above average.

210
00:11:25,450 --> 00:11:28,362
If your height's above average, well it's
kind of 50-50, whether or

211
00:11:28,362 --> 00:11:33,220
not your GPA is going to be above average,
so maybe you have, it's random, right?

212
00:11:33,220 --> 00:11:36,288
So you have a maybe 50% chance that you're
going to be multiplying that

213
00:11:36,288 --> 00:11:38,899
that positive times a negative here.

214
00:11:38,899 --> 00:11:41,249
So you're 50% chance that you're going to
below average on GPA,

215
00:11:41,249 --> 00:11:44,995
then there's a 50% chance that you're
going to be above average on GPA.

216
00:11:44,995 --> 00:11:46,598
So it's kind, so it's kind of a watch.

217
00:11:46,598 --> 00:11:47,102
Half the time,

218
00:11:47,102 --> 00:11:50,154
when you multiply these deviations
together, it's going to be negative.

219
00:11:50,154 --> 00:11:51,405
And half the time it's going to be
positive.

220
00:11:51,405 --> 00:11:52,289
You add up all the negatives and

221
00:11:52,289 --> 00:11:54,823
positives, you're going to end up with a
coefficient covariance of 0.

222
00:11:54,823 --> 00:11:58,275
So, two variables that are not related
will get a covariance of 0.

223
00:11:58,275 --> 00:12:00,423
So, that's the idea with covariance.

224
00:12:00,423 --> 00:12:01,823
I just want to point out again,

225
00:12:01,823 --> 00:12:05,377
this formula's very similar to the formula
for variance.

226
00:12:05,377 --> 00:12:08,700
Remember the formula for variance looks
like this.

227
00:12:08,700 --> 00:12:12,450
It was xi minus x bar, the deviation for
x, squared.

228
00:12:12,450 --> 00:12:17,900
So, you can think of a variance as just a
covariance between a variable with itself.

229
00:12:17,900 --> 00:12:20,030
So, there's a direct similarity here.

230
00:12:20,030 --> 00:12:22,340
And this formula is not too had to
understand.

231
00:12:22,340 --> 00:12:24,498
Just to summarize, again, if x and

232
00:12:24,498 --> 00:12:28,565
y are independent variables, then the
covariance will be 0.

233
00:12:28,565 --> 00:12:33,590
If x and y are positively correlated, then
the covariance will be above 0,

234
00:12:33,590 --> 00:12:34,470
will be positive.

235
00:12:34,470 --> 00:12:38,380
If x and y are inversely correlated, then
the covariance will be negative.

236
00:12:38,380 --> 00:12:41,830
Now, notice that sounds awfully similar to
what I told you earlier

237
00:12:41,830 --> 00:12:43,780
about the correlation coefficient.

238
00:12:43,780 --> 00:12:46,190
And indeed, there's a direct relationship
between covariance and

239
00:12:46,190 --> 00:12:47,190
the correlation coefficient.

240
00:12:47,190 --> 00:12:51,575
For covariance, the problem with

241
00:12:51,575 --> 00:12:55,340
covariance as a measure is that it
completely depends on the units.

242
00:12:55,340 --> 00:12:58,080
So think about what the units of
covariance are going to be.

243
00:12:58,080 --> 00:13:01,035
If you're talking about the covariance
between height and weights.

244
00:13:01,035 --> 00:13:05,430
Say, your units will be something like
kilograms times meters.

245
00:13:06,600 --> 00:13:09,330
Well that's a little bit arbitrary, that
means a covariance for

246
00:13:09,330 --> 00:13:13,598
height and weight is going to be different
depending on whether you measure it in

247
00:13:13,598 --> 00:13:17,260
kilograms times meters as opposed to when
you measure it in pounds times inches.

248
00:13:17,260 --> 00:13:19,680
So the covariance that magnitude is
completely,

249
00:13:19,680 --> 00:13:24,080
completely dependent on units which makes
it hard to understand covariance and

250
00:13:24,080 --> 00:13:28,090
interpret it directly, or to compare it
across different situations.

251
00:13:28,090 --> 00:13:31,218
So wouldn't it be nice if we had a measure
that got rid of the units,

252
00:13:31,218 --> 00:13:33,520
that standardized covariance?

253
00:13:33,520 --> 00:13:36,224
And that's what a correlation coefficient
is.

254
00:13:36,224 --> 00:13:42,320
If you divide the covariance by the
standard deviation of x and

255
00:13:42,320 --> 00:13:46,700
the standard deviation of y, you, you are
essentially dividing out the unit.

256
00:13:46,700 --> 00:13:52,030
So again, if your covariance was in
kilograms meters, well the standard

257
00:13:52,030 --> 00:13:56,200
deviation of height is going to be the
meters and

258
00:13:56,200 --> 00:13:59,970
the standard deviation of, of weight is
going to be the kilogram, so

259
00:13:59,970 --> 00:14:01,820
you'll divide out the kilograms and the
meters.

260
00:14:01,820 --> 00:14:04,350
You'll end up with a unitless quantity.

261
00:14:04,350 --> 00:14:07,790
So it turns out if you divide the
covariance by the standard deviation of

262
00:14:07,790 --> 00:14:12,520
x and the standard deviation of y, you
divide out the units and

263
00:14:12,520 --> 00:14:18,640
you always end up with a value that will
range between negative 1 and positive 1.

264
00:14:18,640 --> 00:14:21,290
And this is the Pearson's correlation
coefficient,

265
00:14:21,290 --> 00:14:24,230
that measure that we've talked about
earlier in the course.

266
00:14:24,230 --> 00:14:26,250
It's simply a standardized covariance.

267
00:14:26,250 --> 00:14:27,460
We could have just dealt directly with

268
00:14:27,460 --> 00:14:29,437
the covariance if there weren't this unit
issue.

269
00:14:29,437 --> 00:14:34,370
But by standardizing you are making
something, a correlation coefficient that

270
00:14:34,370 --> 00:14:38,510
has no units and something that always
ranges between negative 1 and positive 1.

271
00:14:38,510 --> 00:14:41,010
It's comparable, you can compare it across
many,

272
00:14:41,010 --> 00:14:46,683
many different situations, which is really
nice.

273
00:14:46,683 --> 00:14:48,900
[SOUND] Again, the Pearson's correlation
coefficient is just

274
00:14:48,900 --> 00:14:52,324
the standardized covariance of and I'm
presenting the formula once again here.

275
00:14:52,324 --> 00:14:55,712
Again, you're probably not going to have
to calculate this by hand, but

276
00:14:55,712 --> 00:14:57,545
if you ever wanted to calculate this by
hand.

277
00:14:57,545 --> 00:14:58,530
This looks messy, but

278
00:14:58,530 --> 00:15:02,140
all this is, is the numerator is
covariance, which I just showed you.

279
00:15:02,140 --> 00:15:03,530
And of course you've,

280
00:15:03,530 --> 00:15:07,090
you''ve already calculated in this course
before, the standard deviation.

281
00:15:07,090 --> 00:15:09,940
A standard deviation of a single variable,
so the standard deviation of x and

282
00:15:09,940 --> 00:15:10,810
the standard deviation of y.

283
00:15:10,810 --> 00:15:14,410
So if you had to calculate this by hand if
you understand this conceptually you

284
00:15:14,410 --> 00:15:15,940
should be able to calculate this by hand.

285
00:15:15,940 --> 00:15:19,040
Of course it's very tedious to do this by
and

286
00:15:19,040 --> 00:15:21,250
you would of course do this on a computer.

287
00:15:21,250 --> 00:15:22,780
But I think the formula here is,

288
00:15:22,780 --> 00:15:25,250
helps you to understand the concept of
covariance and correlation.

289
00:15:25,250 --> 00:15:29,480
So correlation, the one thing to keep in

290
00:15:29,480 --> 00:15:33,060
mind that people forget is that the
correlation is

291
00:15:33,060 --> 00:15:37,525
actually measuring the strength of the
linear relationship between two variables.

292
00:15:37,525 --> 00:15:40,290
I'm going to show you in a minute that if
the relationship is

293
00:15:40,290 --> 00:15:41,390
something other than linear,

294
00:15:41,390 --> 00:15:45,300
the correlation might, might not be picked
up by the correlation coefficient.

295
00:15:45,300 --> 00:15:50,310
The correlation coefficient ranges again
between negative 1 and positive 1.

296
00:15:50,310 --> 00:15:53,500
0 means no correlation, means your
variables are independent.

297
00:15:53,500 --> 00:15:55,900
Negative 1 means perfect inverse
correlation,

298
00:15:55,900 --> 00:15:58,510
positive 1 means perfect positive
correlation.

299
00:15:58,510 --> 00:16:02,070
So here's a picture that's showing you
perfect positive correlation,

300
00:16:02,070 --> 00:16:05,680
that means that all of the dots lie on a
single line.

301
00:16:05,680 --> 00:16:10,820
That would be the pictorial representation
of an r of positive 1.

302
00:16:10,820 --> 00:16:13,940
If you have inverse, perfect inverse
correlation.

303
00:16:13,940 --> 00:16:15,570
All the dots lie on the same line.

304
00:16:15,570 --> 00:16:17,100
But your, your slope is negative.

305
00:16:17,100 --> 00:16:17,880
You're going down.

306
00:16:17,880 --> 00:16:19,071
So that's the picture for that.

307
00:16:19,071 --> 00:16:27,070
I'm just going to draw you a few pictures
here to illustrate correlation.

308
00:16:27,070 --> 00:16:30,890
So greater scatter around the line
generally means weaker correlation.

309
00:16:30,890 --> 00:16:33,970
So let me just show you some pictures of
what different correlations look like.

310
00:16:33,970 --> 00:16:38,890
So here's a situation, imagine a situation
where basically if you know x, you pretty

311
00:16:38,890 --> 00:16:45,110
much can nail y, you're everything all of
the dots are essentially on the same line.

312
00:16:45,110 --> 00:16:51,650
That would be an example, as I just showed
you, of r of positive 1.

313
00:16:51,650 --> 00:16:55,110
If, however, there's a little bit of
scatter around the line, that is,

314
00:16:55,110 --> 00:16:59,610
if you know x, you can, you can get pretty
close to y, but it's not perfect, they're

315
00:16:59,610 --> 00:17:03,450
not all perfectly on the same line, but
you're still pretty tight around the line.

316
00:17:03,450 --> 00:17:05,380
That's going to be a strong correlation,

317
00:17:05,380 --> 00:17:09,230
maybe something of the magnitude of
somewhere around 0.8.

318
00:17:09,230 --> 00:17:12,040
So that's little bit of scatter, not
perfect correlation, but

319
00:17:12,040 --> 00:17:13,248
still very strong.

320
00:17:13,248 --> 00:17:18,131
However, if you have a lot of scatter
around the line [SOUND].

321
00:17:18,131 --> 00:17:20,080
We have a big scatter here.

322
00:17:20,080 --> 00:17:23,060
You know, here is the best fit line, the
line that kind of best fits

323
00:17:23,060 --> 00:17:25,210
those points and there's just a lot of
scatter around that.

324
00:17:25,210 --> 00:17:28,240
Well that's going to be much weaker
correlation because now if you know x

325
00:17:28,240 --> 00:17:33,010
there's still a lot of variation in what
your outcome variable is going to be.

326
00:17:33,010 --> 00:17:37,210
There is some correlation here but knowing
x doesn't tell you so much about y.

327
00:17:37,210 --> 00:17:38,685
There's a lot of variability.

328
00:17:38,685 --> 00:17:43,280
And this might be something in the range
of say, 0.3.

329
00:17:43,280 --> 00:17:44,690
That's kind of a weak correlation.

330
00:17:44,690 --> 00:17:49,646
And of course, you know, if you are
talking about a,

331
00:17:49,646 --> 00:17:55,020
something that has a negative
relationship, an inverse relationship,

332
00:17:55,020 --> 00:17:56,740
the slope is going to be in the other
direction.

333
00:17:56,740 --> 00:18:01,360
This might be, say something like an r of
negative 0.6.

334
00:18:01,360 --> 00:18:03,846
It's negative because the slope is
downward.

335
00:18:03,846 --> 00:18:07,020
There's an inverse relationship there.

336
00:18:07,020 --> 00:18:12,000
So that's just to give you a sense of what
different r's different correlations will

337
00:18:12,000 --> 00:18:13,302
look like.

338
00:18:13,302 --> 00:18:16,984
As I mentioned, the correlation
coefficient measures linear correlation.

339
00:18:16,984 --> 00:18:21,150
Sometimes x and y are related,

340
00:18:21,150 --> 00:18:25,630
that is there they're non-independent but
they don't have a linear relationship.

341
00:18:25,630 --> 00:18:27,810
So here's a nice simple example.

342
00:18:27,810 --> 00:18:32,020
Imagine that x and y have a beautiful
quadratic relationship like this, right?

343
00:18:32,020 --> 00:18:33,430
This is a beautiful relationship so

344
00:18:33,430 --> 00:18:36,530
clearly there's a direct relationship here
between x and y.

345
00:18:36,530 --> 00:18:39,580
It's just that it's a quadratic
relationship.

346
00:18:39,580 --> 00:18:43,260
If I were to try to fit a line to these
points, guess what?

347
00:18:43,260 --> 00:18:45,900
The line is basically going to go right
through the points,

348
00:18:45,900 --> 00:18:48,160
essentially having a slope of 0.

349
00:18:48,160 --> 00:18:51,950
My r, my correlation coefficient here is
going to come out to 0.

350
00:18:51,950 --> 00:18:55,480
I might, if I just calculated the
correlation coefficient in

351
00:18:55,480 --> 00:18:57,920
the computer without plotting my data.

352
00:18:57,920 --> 00:19:01,410
I might look at that r and say, oh x and y
aren't related, but that's wrong.

353
00:19:01,410 --> 00:19:05,490
x and y are very strongly related in this
case,

354
00:19:05,490 --> 00:19:07,580
it's just that they don't have a linear
relationship.

355
00:19:07,580 --> 00:19:09,590
They have a quadratic relationship.

356
00:19:09,590 --> 00:19:12,890
So always keep that in mind that the
correlation coefficient is only going

357
00:19:12,890 --> 00:19:15,340
to capture linear relationships.

358
00:19:15,340 --> 00:19:17,770
That's why it's so important to always
plot your data,

359
00:19:17,770 --> 00:19:20,810
to evaluate [INAUDIBLE] are x and y
linearly related?

360
00:19:20,810 --> 00:19:21,965
Because sometimes they're not.

361
00:19:21,965 --> 00:19:26,270
Finally, if you don't have, if you have
two variables that are not correlated at

362
00:19:26,270 --> 00:19:27,920
all the picture is going to look something
like this.

363
00:19:27,920 --> 00:19:31,261
It's just completely random, x and y have
nothing to do with one another.

364
00:19:31,261 --> 00:19:34,300
If you draw a line that best fits those
points,

365
00:19:34,300 --> 00:19:36,610
of course it's going to be flat, the slope
is going to be zero,

366
00:19:36,610 --> 00:19:41,030
and the correlation is going to be zero as
we said.

367
00:19:41,030 --> 00:19:44,710
Back to my example data from my Stanford
class.

368
00:19:44,710 --> 00:19:48,070
As I mentioned the correlation coefficient
between your political bent and

369
00:19:48,070 --> 00:19:51,510
[UNKNOWN] ratings of [INAUDIBLE] President
Obama not surprisingly was high.

370
00:19:51,510 --> 00:19:54,000
It was strong correlation, 0.78.

371
00:19:54,000 --> 00:19:55,772
You can see that there's some scatter
around the line but

372
00:19:55,772 --> 00:20:01,220
it's a pretty strong relationship highly
statistically significant.

373
00:20:01,220 --> 00:20:05,310
I then went and looked at how does your
political bent relate to,

374
00:20:05,310 --> 00:20:06,850
not your ratings of Obama, but

375
00:20:06,850 --> 00:20:10,630
your ratings of the healthcare reform that
just happened in America.

376
00:20:10,630 --> 00:20:11,720
Some people call it Obamacare.

377
00:20:11,720 --> 00:20:16,710
I asked the students to rate how they felt
about that new healthcare law,

378
00:20:16,710 --> 00:20:18,680
the new healthcare reform on a scale of
from 0-100.

379
00:20:18,680 --> 00:20:24,700
Now you'd expect since the healthcare
reform was largely supported by Obama and

380
00:20:24,700 --> 00:20:26,261
the democrats in the US.

381
00:20:26,261 --> 00:20:29,970
You'd expect that your political bent
would predict your

382
00:20:29,970 --> 00:20:31,910
feelings about the health care law.

383
00:20:31,910 --> 00:20:35,010
But you can see that that's true to some
extent but

384
00:20:35,010 --> 00:20:37,970
is not as strong as a relationship as we
saw in the last slide,

385
00:20:37,970 --> 00:20:41,770
it's a much weaker relationship, there's a
lot of scatter here.

386
00:20:41,770 --> 00:20:46,080
You'll notice that it's not it's certainly
not a perfect relationship because, for

387
00:20:46,080 --> 00:20:49,730
example, I've got somebody over here who's
pretty Republican, and

388
00:20:49,730 --> 00:20:52,520
they're really, they really like that
healthcare law.

389
00:20:52,520 --> 00:20:57,650
And here's another Republican who's pretty
favorable towards that healthcare law.

390
00:20:57,650 --> 00:20:59,010
On the other hand, I've got this really,

391
00:20:59,010 --> 00:21:01,970
really liberal person over here who
doesn't like the healthcare law so much.

392
00:21:01,970 --> 00:21:04,210
So you can see there's a lot of
variability.

393
00:21:04,210 --> 00:21:05,520
It tends to go up.

394
00:21:05,520 --> 00:21:09,050
Your feelings about the health care law
tend to go up as you tend to

395
00:21:09,050 --> 00:21:09,890
get more liberal.

396
00:21:09,890 --> 00:21:12,646
But it's definitely a wide range here and

397
00:21:12,646 --> 00:21:16,860
the correlation coefficient of 0.3
reflects that this is a weak correlation.

398
00:21:16,860 --> 00:21:19,440
It does come out to be statistically
significant,

399
00:21:19,440 --> 00:21:23,440
which indicates that it's probably a real
association,

400
00:21:23,440 --> 00:21:26,660
it's just that it's not [INAUDIBLE] not
totally tight, it's not very strong.

401
00:21:26,660 --> 00:21:33,650
I want to just show you, I showed you this
graphic on the previous slide,

402
00:21:33,650 --> 00:21:37,250
that had, I had put a line there, I had
already put the best fit line in.

403
00:21:37,250 --> 00:21:40,320
The reason I want to show you the graphic
without the line is,

404
00:21:40,320 --> 00:21:44,100
notice that when the graphic has the line
superimposed, when I've already asked

405
00:21:44,100 --> 00:21:48,480
the computer to draw that line on, it can
be very misleading to the eye.

406
00:21:48,480 --> 00:21:51,640
If I had shown you this picture first, you
might look at that and

407
00:21:51,640 --> 00:21:54,600
wonder if there really was any correlation
at all there.

408
00:21:54,600 --> 00:21:58,710
Your eye can pick up that there's maybe a
little bit of positive correlation, but

409
00:21:58,710 --> 00:22:03,560
this graphic much better represents kind
of that it's, it's fairly random,

410
00:22:03,560 --> 00:22:07,280
there's a little bit of a trend in
association here, but it's pretty random.

411
00:22:07,280 --> 00:22:12,430
When you draw that line on, it's, it can
totally draw your eye to the light and

412
00:22:12,430 --> 00:22:15,130
you can be misled to think that the
correlation is stronger than it

413
00:22:15,130 --> 00:22:15,920
actually is.

414
00:22:15,920 --> 00:22:19,370
So, just be careful about that, always
look at the picture without the light on.

415
00:22:19,370 --> 00:22:21,950
On top as well, to make sure you're not
getting misled.

416
00:22:21,950 --> 00:22:27,490
One final association that I wanted to
look at was whether or

417
00:22:27,490 --> 00:22:30,920
not your political bent had anything to do
with your optimism.

418
00:22:30,920 --> 00:22:34,750
You might think that since a Democrat's
running the country, maybe the Democrats

419
00:22:34,750 --> 00:22:39,190
would be more optimistic or you know,
something like that.

420
00:22:39,190 --> 00:22:43,470
It turns out that there's no relationship
between your political bent and

421
00:22:43,470 --> 00:22:44,290
your optimism.

422
00:22:44,290 --> 00:22:46,740
At least, not in my class.

423
00:22:46,740 --> 00:22:49,790
So here I'm showing the, the correlation
and again,

424
00:22:49,790 --> 00:22:53,120
the best fit line you can see that it's
pretty much random.

425
00:22:54,410 --> 00:22:58,610
How optimistic you are doesn't really have
anything to do with your political bent.

426
00:22:58,610 --> 00:23:02,884
The correlation coefficient here comes out
to be 0.09, which is pretty close to 0.

427
00:23:02,884 --> 00:23:04,950
The p-value is 0.57.

428
00:23:04,950 --> 00:23:07,790
Again, the null hypothesis here is that
the correlation is zero.

429
00:23:07,790 --> 00:23:11,100
We clearly don't have any evidence to
reject that null hypothesis.

430
00:23:11,100 --> 00:23:13,830
It looks like very well could be that the
correlation coefficient here is

431
00:23:13,830 --> 00:23:15,280
right around zero.

432
00:23:15,280 --> 00:23:17,940
Again that line is probably fooling you
right now.

433
00:23:17,940 --> 00:23:19,640
It's drawing your eye and making you think
oh,

434
00:23:19,640 --> 00:23:22,440
maybe there is a little bit of positive
correlation.

435
00:23:22,440 --> 00:23:25,650
In fact let me take the line off, and you
can see that it's completely random.

436
00:23:25,650 --> 00:23:26,970
Really there's no correlation here.

437
00:23:26,970 --> 00:23:29,199
That line is misleading your eye a bit.
