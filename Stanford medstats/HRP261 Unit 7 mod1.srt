1
00:00:00,000 --> 00:00:06,170
[BLANK_AUDIO]

2
00:00:06,170 --> 00:00:09,887
>> In this next module I'm going to tell
you about bootstrap re-sampling, and

3
00:00:09,887 --> 00:00:12,601
I decided to put this module first because
I'm going to be

4
00:00:12,601 --> 00:00:16,463
using the term bootstrap sample in a lot
of the other modules for this week.

5
00:00:16,463 --> 00:00:20,245
So I thought it would be important for you
to understand this term before I

6
00:00:20,245 --> 00:00:22,984
even got to the discussion of prediction
modeling.

7
00:00:22,984 --> 00:00:27,583
So first of all, the bootstrap was
invented by our very own Bradley Efron in

8
00:00:27,583 --> 00:00:31,014
1979, and just to kind of motivate the
bootstrap for

9
00:00:31,014 --> 00:00:35,110
you, let me take a step back here and do a
little review.

10
00:00:35,110 --> 00:00:38,880
So, the bootstrap is something that's used
for repeated sampling.

11
00:00:38,880 --> 00:00:42,450
And the idea of repeated sampling has come
up before in,

12
00:00:42,450 --> 00:00:45,510
when we've talked about simulation.

13
00:00:45,510 --> 00:00:50,480
So, let me just review here the concept of
the distribution of a statistic.

14
00:00:50,480 --> 00:00:52,890
So, when we talk about the distribution of
a statistic,

15
00:00:52,890 --> 00:00:54,060
there are three things we want to know.

16
00:00:54,060 --> 00:00:58,340
We want to know what's the shape of that
distribution, what's the mean, and

17
00:00:58,340 --> 00:01:00,290
what's the standard error.

18
00:01:00,290 --> 00:01:02,540
And if you think about the way we do
studies.

19
00:01:02,540 --> 00:01:04,290
If I say, well I went out and

20
00:01:04,290 --> 00:01:09,420
I collected my sample of 100 people, and I
calculated a mean, then you would say,

21
00:01:09,420 --> 00:01:11,980
well what do you mean by the distribution
of that statistic?

22
00:01:11,980 --> 00:01:14,350
That statistic only has one number.

23
00:01:14,350 --> 00:01:18,710
But remember that in statistics, what we
do is we think about theoretically if I

24
00:01:18,710 --> 00:01:22,300
could repeat that same experiment over and
over and over and

25
00:01:22,300 --> 00:01:25,480
over again, what would the distribution of
those means be?

26
00:01:25,480 --> 00:01:27,930
In other words, if I could go out to my
original population and

27
00:01:27,930 --> 00:01:31,400
take a new sample of 100 and calculate the
mean in that sample, and

28
00:01:31,400 --> 00:01:33,520
then I could do that again and again and
again and

29
00:01:33,520 --> 00:01:38,950
again, I could actually literally see what
the distribution of the means was.

30
00:01:38,950 --> 00:01:42,810
And, of course, that information is
important because I need it in order to

31
00:01:42,810 --> 00:01:45,920
be able to calculate P values and
confidence intervals.

32
00:01:45,920 --> 00:01:50,880
So imagine that I could do somehow a
repeated sampling like this and I could,

33
00:01:50,880 --> 00:01:54,830
since we've been talking about beta
coefficients from logistic regression,

34
00:01:54,830 --> 00:01:58,230
I could calculate a beta coefficient from
logistic regression again, again and

35
00:01:58,230 --> 00:02:00,680
again in multiple samples of the same
size.

36
00:02:00,680 --> 00:02:02,280
Then I could actually look and

37
00:02:02,280 --> 00:02:05,290
see what's the distribution, of that beta
coefficient.

38
00:02:05,290 --> 00:02:08,260
You can see I'm putting it here in a
histogram, and we would say,

39
00:02:08,260 --> 00:02:11,338
well it looks normally distributed, the
mean is the true beta, and

40
00:02:11,338 --> 00:02:14,030
the standard error is some value.

41
00:02:14,030 --> 00:02:17,940
Now I showed you in, in the fall that
there's two ways to

42
00:02:17,940 --> 00:02:20,000
figure out the distribution of a
statistic.

43
00:02:20,000 --> 00:02:22,315
There was simulation which I like to show
for

44
00:02:22,315 --> 00:02:25,710
teaching, because it's very hands on and
it's very intuitive.

45
00:02:25,710 --> 00:02:30,170
I can, in the computer, virtually do
repeated sampling.

46
00:02:30,170 --> 00:02:32,380
And in a way the boot strap is going to
look a lot like that.

47
00:02:33,960 --> 00:02:38,588
Of course more practically statisticians
spend their time figuring out formulas so

48
00:02:38,588 --> 00:02:40,430
that we don't have to do a simulation
every time.

49
00:02:40,430 --> 00:02:44,060
So for example, I told you that for an
odds ratio or actually for

50
00:02:44,060 --> 00:02:48,840
the natural log of the odds ratio, it
follows a z distribution, and the standard

51
00:02:48,840 --> 00:02:54,300
error there is the square root of 1 over a
plus 1 over b plus 1 over c plus 1 over d.

52
00:02:54,300 --> 00:02:56,420
So that formula is there for you.

53
00:02:56,420 --> 00:03:00,140
You can just apply it and you don't have
to run a simulation every time to

54
00:03:00,140 --> 00:03:03,396
figure out the distribution of an odds
ratio anew every time.

55
00:03:03,396 --> 00:03:06,464
And in most cases we're just going to use
formulas, and we don't have to

56
00:03:06,464 --> 00:03:10,079
go through this whole process, but that's
the process behind the scenes.

57
00:03:10,079 --> 00:03:14,047
and, there may be situations when where
you want to deal with a statistic or

58
00:03:14,047 --> 00:03:16,927
make up a statistic that's new and
complicated and,

59
00:03:16,927 --> 00:03:21,430
you know, nobody's ever used before and
there is no formula available for you.

60
00:03:21,430 --> 00:03:25,330
In that case, that's where the bootstrap
sampling can come in.

61
00:03:25,330 --> 00:03:28,010
You can calculate a bootstrap standard
error, and

62
00:03:28,010 --> 00:03:31,070
you can look at the shape of the
distribution in bootstrap samples.

63
00:03:31,070 --> 00:03:33,710
So that's one of the uses of the
bootstrap.

64
00:03:33,710 --> 00:03:37,760
Also if, perhaps your using a well known
statistics that already has a formula, but

65
00:03:37,760 --> 00:03:41,920
you're not meeting some of the assumptions
that went into calculating that formula,

66
00:03:41,920 --> 00:03:44,250
the bootstrap could come up there as well.

67
00:03:44,250 --> 00:03:48,820
And again, the bootstrap is a means of
doing repeated sampling.

68
00:03:48,820 --> 00:03:53,510
But the key is that you're sampling, not
from the original population, but

69
00:03:53,510 --> 00:03:57,840
you're sampling from your, own dataset,
from your one sample.

70
00:03:57,840 --> 00:04:00,780
And this, if you start to think about it,
sounds impossible.

71
00:04:00,780 --> 00:04:04,240
So I want to somehow take repeated samples
of the same size,

72
00:04:04,240 --> 00:04:06,930
of size n, from my dataset of size n and
that,

73
00:04:06,930 --> 00:04:10,590
of course, if you, says, that sounds
impossible, you can only take one sample.

74
00:04:10,590 --> 00:04:11,710
because they're the same size.

75
00:04:11,710 --> 00:04:12,970
So how can you do this?

76
00:04:12,970 --> 00:04:19,210
Well the, the trick is the key to all of
this is sampling with replacement.

77
00:04:19,210 --> 00:04:21,480
So what is sampling with replacement?

78
00:04:21,480 --> 00:04:24,340
That means that when you're creating your
new datasets,

79
00:04:24,340 --> 00:04:29,680
your repeated samples, some observations
can appear more than once, and

80
00:04:29,680 --> 00:04:32,550
some observations will not appear at all.

81
00:04:32,550 --> 00:04:35,360
Sampling with the replacement is something
like rolling a die,

82
00:04:35,360 --> 00:04:37,870
the one you can roll a one twice in a row.

83
00:04:37,870 --> 00:04:38,670
That's okay.

84
00:04:38,670 --> 00:04:43,020
So you can get the same observation to
appear in your news sample twice and

85
00:04:43,020 --> 00:04:45,550
some of the observations won't appear at
all.

86
00:04:45,550 --> 00:04:46,660
And that's the key to this.

87
00:04:46,660 --> 00:04:51,430
This is what will allow you to take
multiple samples from an an initial

88
00:04:51,430 --> 00:04:52,840
sample, all of size n.

89
00:04:52,840 --> 00:04:55,750
And this is just bell, best illustrated
with a picture.

90
00:04:55,750 --> 00:04:57,490
So let me just illustrate that now.

91
00:04:57,490 --> 00:05:01,020
So what we're doing, is we are taking some
original samples.

92
00:05:01,020 --> 00:05:02,320
Say I had a very simple dataset.

93
00:05:02,320 --> 00:05:03,330
It only has six observations.

94
00:05:03,330 --> 00:05:06,320
because A B C D E and F in my original
dataset.

95
00:05:06,320 --> 00:05:11,150
I want to take repeated samples of size N
from this original sample and

96
00:05:11,150 --> 00:05:13,060
we're going to call those the bootstrap
samples.

97
00:05:18,190 --> 00:05:22,020
So, here's how it works here's a new
dataset.

98
00:05:22,020 --> 00:05:25,470
I want to fill a new dataset with six
observations from my original observation.

99
00:05:25,470 --> 00:05:25,970
Observations.

100
00:05:27,950 --> 00:05:32,120
And what I'm going to do in order to that,
is for every kind of slot in this

101
00:05:32,120 --> 00:05:35,890
new dataset, I'm going to reach my hand
into this bag of letters,

102
00:05:35,890 --> 00:05:39,910
this bag of observations, A, B, C, D, E,
and F, and with equal probability.

103
00:05:39,910 --> 00:05:41,530
Every letter has an equal probability.

104
00:05:41,530 --> 00:05:44,120
Or every observation has an equal
probability of being chosen here.

105
00:05:44,120 --> 00:05:46,080
So I reach my hand into, to that bag and

106
00:05:46,080 --> 00:05:49,220
maybe for the first slot here I pick out
B.

107
00:05:49,220 --> 00:05:50,170
Okay.

108
00:05:50,170 --> 00:05:54,550
Now, then I put B back into the bag, so
that it's again, with replacement.

109
00:05:54,550 --> 00:05:56,560
So B can be selected again.

110
00:05:56,560 --> 00:05:59,870
So, now I reach my hand into that bag of
six letters, and

111
00:05:59,870 --> 00:06:01,430
I pull out another letter.

112
00:06:01,430 --> 00:06:04,280
Every letter has a one in six chance of
being pulled out, and

113
00:06:04,280 --> 00:06:07,256
let's say I happened to pull out B again,
that could happen.

114
00:06:07,256 --> 00:06:10,142
Now I put B back in, I pull out another
letter,

115
00:06:10,142 --> 00:06:13,768
in this case let's say I pull out C, and
then let's say for

116
00:06:13,768 --> 00:06:18,620
the next one I pull out an E, and then
let's say I pull out an F, and another F.

117
00:06:18,620 --> 00:06:19,820
So notice what happened.

118
00:06:19,820 --> 00:06:24,370
B got pulled out twice, F got pulled out
twice, and A and

119
00:06:24,370 --> 00:06:26,630
D were not included at all in this sample.

120
00:06:26,630 --> 00:06:27,410
So it's a new sample.

121
00:06:27,410 --> 00:06:30,640
It's different than my original, but it
still has an n of 6 and

122
00:06:30,640 --> 00:06:35,450
it has all the same observations except
it's missing a few of them.

123
00:06:35,450 --> 00:06:36,170
We can do this again.

124
00:06:36,170 --> 00:06:37,500
So, that's one bootstrap sample.

125
00:06:37,500 --> 00:06:38,580
Then we can repeat it.

126
00:06:38,580 --> 00:06:45,312
So now, I've got to, again I want to make
a dataset with size of 6.

127
00:06:45,312 --> 00:06:49,120
I'm going to, for the first observation in
this new dataset, I reach into my bag,

128
00:06:49,120 --> 00:06:50,264
I pull out one letter.

129
00:06:50,264 --> 00:06:52,340
Every letter has a one sixth chance of
being pulled out.

130
00:06:52,340 --> 00:06:53,110
Let's say I get an E.

131
00:06:53,110 --> 00:06:54,960
I put the E back in.

132
00:06:54,960 --> 00:06:57,060
Now I'm going to pull out another letter.

133
00:06:57,060 --> 00:07:01,660
Let's say then I pull out the C, put the C
back in, maybe C comes out again, and

134
00:07:01,660 --> 00:07:06,970
then I, maybe I get an A, and then I get a
D, and then maybe I get another A.

135
00:07:06,970 --> 00:07:09,810
So, now again, a slightly different
sample,

136
00:07:09,810 --> 00:07:13,120
we have slightly different constitution of
observation.

137
00:07:13,120 --> 00:07:14,300
Then I can do this again.

138
00:07:14,300 --> 00:07:18,392
You get the idea by now so maybe this next

139
00:07:18,392 --> 00:07:23,923
bootstrap sample will have D, E, F, C, F,
and F.

140
00:07:23,923 --> 00:07:25,950
Maybe F will appear three times in that
sample.

141
00:07:25,950 --> 00:07:27,790
That could happen as well.

142
00:07:27,790 --> 00:07:31,480
So now I'm going to come out with a large
number of different,

143
00:07:31,480 --> 00:07:35,100
I can come with a whole bunch of bootstrap
samples and with each of these samples.

144
00:07:35,100 --> 00:07:38,170
Now of course this is a very small sample
so I can't calculate too many statistics

145
00:07:38,170 --> 00:07:41,650
here, but imagine doing this for a bigger
sample, and in each one of these samples,

146
00:07:41,650 --> 00:07:46,580
I could run a logistic regression on the
new sample, and come out with a beta,

147
00:07:46,580 --> 00:07:49,780
from say, bootstrap sample one for my
logistic regression.

148
00:07:49,780 --> 00:07:53,230
And then I run it again on the, the next
sample and

149
00:07:53,230 --> 00:07:56,160
I get a beta from bootstrap sample two.

150
00:07:56,160 --> 00:07:57,175
And I do it again and

151
00:07:57,175 --> 00:07:59,830
I come out with a different beta, beta
from bootstrap sample three.

152
00:07:59,830 --> 00:08:01,820
They're all going to be slightly
different.

153
00:08:01,820 --> 00:08:05,870
Now, I can actually see what is the
distribution of beta,

154
00:08:05,870 --> 00:08:10,880
because I've got beta calculated from all
these different samples of size N.

155
00:08:10,880 --> 00:08:13,850
I can actually look at the distribution.

156
00:08:13,850 --> 00:08:15,808
So that turns out to be very, very useful.

157
00:08:15,808 --> 00:08:18,890
Now it would be useful to just talk about
for

158
00:08:18,890 --> 00:08:24,150
a minute, how often does any particular
observation appear in, in a given sample?

159
00:08:24,150 --> 00:08:25,400
How likely is it, say, for

160
00:08:25,400 --> 00:08:29,770
example, that A happens to come up, in say
bootstrap sample one?

161
00:08:29,770 --> 00:08:32,550
We can actually calculate this out, it's a
good review of probability.

162
00:08:33,690 --> 00:08:35,920
So, say this is bootstrap sample one.

163
00:08:35,920 --> 00:08:39,840
What's the probability that A is chosen
for bootstrap sample one?

164
00:08:39,840 --> 00:08:43,855
That it appears at all, in bootstrap
sample one?

165
00:08:43,855 --> 00:08:45,560
Well remember,

166
00:08:45,560 --> 00:08:49,040
there is a 1 6th chance that each one of
these slots in the dataset.

167
00:08:49,040 --> 00:08:51,940
There is a 1 6th chance that we're
going to pick A.

168
00:08:51,940 --> 00:08:52,690
Okay?

169
00:08:52,690 --> 00:08:56,530
So, how do we calculate the probability
that A appears somewhere in

170
00:08:56,530 --> 00:08:58,180
a given bootstrap sample?

171
00:08:58,180 --> 00:09:03,160
We need to calculate the probability of
that A appears at least once.

172
00:09:03,160 --> 00:09:05,300
So this is at, on at least one problem.

173
00:09:05,300 --> 00:09:08,330
So you know if we're doing it at least on
problem we're going to be

174
00:09:08,330 --> 00:09:13,980
using a one minus the probability that A
never appears in this sample.

175
00:09:13,980 --> 00:09:16,970
Well, what's the probability that A never
appears?

176
00:09:16,970 --> 00:09:19,720
Every slot has a 1 6th chance of picking A
so

177
00:09:19,720 --> 00:09:22,120
the chance of not picking A is going to be
5 6th.

178
00:09:22,120 --> 00:09:26,140
So for A not to appear in this dataset we
have to not pick it with 5 6th chance for

179
00:09:26,140 --> 00:09:26,805
the first slot.

180
00:09:26,805 --> 00:09:29,957
5 6th chance for the second, 5 6th again,
again, again, again.

181
00:09:29,957 --> 00:09:32,430
5 6th raised to the sixth,

182
00:09:32,430 --> 00:09:35,048
that's the chance that we're not going to
pick A in this dataset.

183
00:09:35,048 --> 00:09:42,230
Well, 5 6th raised to to the sixth is a
probability of 33.6%.

184
00:09:42,230 --> 00:09:51,316
So one minus that probability is going to
be a probability of 66.4%.

185
00:09:51,316 --> 00:09:58,190
So we had in this case a 66.4% of A,
appearing in that dataset.

186
00:09:58,190 --> 00:10:02,980
Now it turns out that we can actually
solve this more generally, and so

187
00:10:02,980 --> 00:10:07,300
the probability more generally of, a,
particular value, a particular observation

188
00:10:07,300 --> 00:10:14,620
appearing in a dataset is 1 minus n minus
1 over n divided by n.

189
00:10:14,620 --> 00:10:18,370
I'm just filling in, where n here is 6.

190
00:10:18,370 --> 00:10:20,950
So then I'm just generalizing this
formula.

191
00:10:20,950 --> 00:10:27,642
Well it turns out that if n gets bigger,
this will approach the value of 63.2%.

192
00:10:27,642 --> 00:10:34,370
So, about 63.2% of the observations will
appear in any given dataset.

193
00:10:34,370 --> 00:10:39,050
That means about 37% of the observations
are left out of any given dataset.

194
00:10:39,050 --> 00:10:43,712
So that's kind of useful to know about 2
3rds of the observations end up in

195
00:10:43,712 --> 00:10:47,094
a boot strap sample and about 1 3rd will
be left out.

196
00:10:47,094 --> 00:10:48,944
Now, the bootstrap is a very general
method.

197
00:10:48,944 --> 00:10:51,040
It has all sorts of uses.

198
00:10:51,040 --> 00:10:53,910
So far I have been eluding to the fact
that it can be used to

199
00:10:53,910 --> 00:10:56,320
calculate the distribution of a statistic.

200
00:10:56,320 --> 00:10:59,520
So you might want to do that you get
standard errors, P values, and

201
00:10:59,520 --> 00:11:02,840
confidence intervals when you have a new
statistic or

202
00:11:02,840 --> 00:11:06,010
a complicated where we don't have a
formula.

203
00:11:06,010 --> 00:11:09,890
Or you might want to do that where its a
well-known statistic but

204
00:11:09,890 --> 00:11:11,180
we've violated some assumptions, so

205
00:11:11,180 --> 00:11:14,960
maybe you have a really small sample or
you violated normality.

206
00:11:14,960 --> 00:11:18,960
You don't have a normal distribution and
so you're not sure if

207
00:11:18,960 --> 00:11:21,810
those formulas are really good so you
could actually bootstrap it and

208
00:11:21,810 --> 00:11:25,870
there would be nothing wrong with just
basing it on the bootstrap distribution.

209
00:11:25,870 --> 00:11:27,580
We're also going to use it in certain
cases for

210
00:11:27,580 --> 00:11:30,360
prediction modeling, which we'll talk
about later this week.

211
00:11:31,660 --> 00:11:34,880
But just to show you how the bootstrap
would be used specifically to calculate

212
00:11:34,880 --> 00:11:37,670
standard errors for say a new statistic,
or

213
00:11:37,670 --> 00:11:41,930
just a statistic where you may have
violated some of the assumptions.

214
00:11:41,930 --> 00:11:43,250
So first thing you're going to do is
you're number your

215
00:11:43,250 --> 00:11:47,050
observations 1 through n, and then you're
going to sample with replacement.

216
00:11:47,050 --> 00:11:49,820
So you're going to draw a random sample of
size n.

217
00:11:49,820 --> 00:11:54,600
So observation 1 might appear twice, and
observation 2 might not appear at all.

218
00:11:54,600 --> 00:11:56,320
You're going to make a bootstrap sample.

219
00:11:56,320 --> 00:11:58,670
Then you're going to calculate your
statistic, say the mean or

220
00:11:58,670 --> 00:12:02,899
the beta coefficient or whatever statistic
you're interested in on that sample.

221
00:12:04,800 --> 00:12:07,160
They you're going to do this a large
number of times.

222
00:12:07,160 --> 00:12:11,620
So you're going to do this say a 1,000
times a 100 times 200 times

223
00:12:11,620 --> 00:12:14,940
a large number of times, so that you can
figure out,

224
00:12:14,940 --> 00:12:18,550
what is actually the distribution of this
statistic.

225
00:12:18,550 --> 00:12:22,340
Then you take say those 1,000 beta
coefficients and you can say plot them in

226
00:12:22,340 --> 00:12:25,570
a histogram and just look that what's the
shape of the distribution.

227
00:12:25,570 --> 00:12:29,370
You can calculate the standard deviation
of those betas,

228
00:12:29,370 --> 00:12:32,680
as you would calculate the standard
deviations of any number.

229
00:12:32,680 --> 00:12:36,090
You've got a thousand, beta coefficients,
you calculate their standard deviation.

230
00:12:36,090 --> 00:12:37,420
That's the standard error.

231
00:12:37,420 --> 00:12:39,541
And then you can use that for calculating
p values.

232
00:12:41,110 --> 00:12:42,720
You can also calculate confidence
intervals, so

233
00:12:42,720 --> 00:12:43,830
you can just literally look and

234
00:12:43,830 --> 00:12:48,120
say okay, where do I have, where do 95% of
my beta coefficients lie,

235
00:12:48,120 --> 00:12:51,870
that's my 95% confidence interval,
whatever's in the tail is left out.

236
00:12:51,870 --> 00:12:55,800
So, you can get empirical standard errors
and empirical confidence intervals, and

237
00:12:55,800 --> 00:13:00,230
this one of the main uses of the
bootstrap, but again it has very a lot of

238
00:13:00,230 --> 00:13:03,300
uses, and it will also see some uses in
prediction modeling.

239
00:13:03,300 --> 00:13:08,330
[BLANK_AUDIO]
