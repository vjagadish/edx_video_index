1
00:00:06,070 --> 00:00:10,890
In this next module, I'm going to give you
some mathematical details about two of

2
00:00:10,890 --> 00:00:15,210
the hypothesis tests that you can get out
of logistic regression.

3
00:00:15,210 --> 00:00:19,760
So in logistic regression, the hypotheses
that we might be testing, we might want to

4
00:00:19,760 --> 00:00:24,470
test whether or not an individual beta for
one predictor is significant or not.

5
00:00:24,470 --> 00:00:25,220
That will tell us if

6
00:00:25,220 --> 00:00:28,980
that particular predictor has an
independent relationship with the outcome.

7
00:00:28,980 --> 00:00:30,900
That's one kind of test that we can do.

8
00:00:30,900 --> 00:00:33,850
A second kind of test that we can do is
what's called a global test.

9
00:00:33,850 --> 00:00:38,490
We can look at a set of betas at once and
say whether or not that group of betas,

10
00:00:38,490 --> 00:00:42,370
if any of them, have a relationship with
the outcome variable.

11
00:00:42,370 --> 00:00:45,900
So the null hypothesis for the global test
will be that all the betas are equal to

12
00:00:45,900 --> 00:00:49,530
zero, and if we reject that one, that will
just tell us that at least one of

13
00:00:49,530 --> 00:00:52,760
the betas is predictive, has some
relationship with the outcome.

14
00:00:52,760 --> 00:00:54,770
So I'm going to turn to the first of those
tests first,

15
00:00:54,770 --> 00:00:58,440
which is just something very simple called
the Wald test.

16
00:00:58,440 --> 00:01:02,720
So, the Wald test is going to test the
null hypothesis that a single beta,

17
00:01:04,160 --> 00:01:06,270
call it beta 1, is equal to 0.

18
00:01:06,270 --> 00:01:10,180
And it's very, very straightforward,
because we're just going to take

19
00:01:10,180 --> 00:01:13,810
the beta that we estimated, which we can
get right out of our computer output, and

20
00:01:13,810 --> 00:01:15,150
divide it by the standard error.

21
00:01:15,150 --> 00:01:17,830
I'm not going to make you calculate the
standard error by hand again.

22
00:01:17,830 --> 00:01:21,040
The computer will calculate the standard
error for that beta.

23
00:01:21,040 --> 00:01:24,060
Beta divided by its standard error is a
z-score.

24
00:01:24,060 --> 00:01:28,030
If that z is significant, than that, and
you can reject the null hypothesis, that

25
00:01:28,030 --> 00:01:32,510
tells you that, that particular predictor
has a relationship with the outcome.

26
00:01:32,510 --> 00:01:35,010
The null value of course here, is that
beta is 0.

27
00:01:35,010 --> 00:01:37,080
If there's no association between the
predictor and

28
00:01:37,080 --> 00:01:39,540
the outcome, then the slope will be 0.

29
00:01:39,540 --> 00:01:41,480
In other words, that beta will be 0.

30
00:01:41,480 --> 00:01:43,210
So that's all the Wald test is,

31
00:01:43,210 --> 00:01:45,950
beta divided by the standard error follows
the z-score.

32
00:01:45,950 --> 00:01:47,390
That's, that's it.

33
00:01:47,390 --> 00:01:48,260
Sometimes you'll see,

34
00:01:48,260 --> 00:01:53,490
in some computer outputs, that you will
actually see it as a Wald chi-squared.

35
00:01:53,490 --> 00:01:58,560
All this is is, to avoid negatives, you
can square the z-score, and

36
00:01:58,560 --> 00:02:00,700
you end up with a chi-square with 1 degree
of freedom.

37
00:02:00,700 --> 00:02:04,610
But its no different then just squaring
what we just calcuated and

38
00:02:04,610 --> 00:02:07,750
that of course also comes with the same
p-value.

39
00:02:07,750 --> 00:02:09,660
Again, most of the time you're not
going to be doing this by hand,

40
00:02:09,660 --> 00:02:13,350
the computer will actually calculate the
z-score or the chi-squared for you,

41
00:02:13,350 --> 00:02:14,330
and will give you the p-value.

42
00:02:14,330 --> 00:02:18,590
But that's where that individual p-value
comes from.

43
00:02:18,590 --> 00:02:19,920
Just to give an example here.

44
00:02:19,920 --> 00:02:24,480
Going back to the example I used in the
earlier module that cardiovascular disease

45
00:02:24,480 --> 00:02:25,030
and age.

46
00:02:25,030 --> 00:02:26,900
This was a simple 2 by 2 table.

47
00:02:28,200 --> 00:02:32,540
I can take those values and plug them into
a logistic regression in my computer, and

48
00:02:32,540 --> 00:02:33,640
here's the output that I got.

49
00:02:34,890 --> 00:02:39,740
So, we saw in the last module how to
actually get the betas and the alphas.

50
00:02:39,740 --> 00:02:45,910
So, in this case SAS has estimated for me,
using maximum likelihood estimation,

51
00:02:45,910 --> 00:02:48,272
has estimated the value of the intercept
and age.

52
00:02:48,272 --> 00:02:51,130
The computer has also given me the
standard error.

53
00:02:51,130 --> 00:02:53,990
And I mentioned before that the standard
error falls right off

54
00:02:53,990 --> 00:02:56,220
the second derivative of the likelihood
function.

55
00:02:56,220 --> 00:02:58,089
So that comes out sort of automatically.

56
00:02:58,089 --> 00:02:59,270
We get the standard errors.

57
00:02:59,270 --> 00:03:04,810
If I divide, say the beta, 2.09, by its
standard error 0.5,

58
00:03:04,810 --> 00:03:06,400
I get a value of about 4.

59
00:03:06,400 --> 00:03:09,550
Well, that's the z-score, which we know is
going to be highly significant.

60
00:03:09,550 --> 00:03:12,160
If I square that value of 4, I get a value
of about 16.

61
00:03:12,160 --> 00:03:14,400
That's also going to be highly
significant.

62
00:03:14,400 --> 00:03:20,030
So clearly we, age is very significantly
related to cardiovascular disease.

63
00:03:20,030 --> 00:03:21,340
And very simple.

64
00:03:21,340 --> 00:03:24,720
Beta divided by standard error gives you
this statistic.

65
00:03:24,720 --> 00:03:28,600
Notice that if I go back to the original
two by two table, and

66
00:03:28,600 --> 00:03:32,800
calculate the p-value for an odds ratio in
the way that I told you earlier in

67
00:03:32,800 --> 00:03:36,840
the course, which is just to calculate the
natural log of the odds ratio and

68
00:03:36,840 --> 00:03:40,400
compare that to its standard error, which
is just this 1 over a, plus 1 over b,

69
00:03:40,400 --> 00:03:43,490
plus 1 over c, plus 1 over d, square
rooted.

70
00:03:43,490 --> 00:03:45,600
That follows a z-distribution.

71
00:03:45,600 --> 00:03:48,380
When I calculate that out from the,
directly from the 2 by 2 table here,

72
00:03:48,380 --> 00:03:49,920
I get a z-value of 3.96.

73
00:03:49,920 --> 00:03:53,632
Well, guess what, if you square the 3.96,
you end up with the,

74
00:03:53,632 --> 00:03:55,480
Wald chi-square value.

75
00:03:55,480 --> 00:03:58,220
So the logistic regression here isn't
doing anything more than we

76
00:03:58,220 --> 00:04:00,220
could already do from a 2 by 2 table.

77
00:04:00,220 --> 00:04:01,270
Of course, we're going to want to

78
00:04:01,270 --> 00:04:03,250
eventually add more predictors to the
model.

79
00:04:04,660 --> 00:04:07,730
The next test that I'm going to talk about
is what's called the likelihood

80
00:04:07,730 --> 00:04:08,390
ratio test.

81
00:04:08,390 --> 00:04:14,660
And this is testing the global hypothesis,
the global null hypothesis that all,

82
00:04:14,660 --> 00:04:19,240
a bunch of betas, a set of betas, are all
equal to 0.

83
00:04:19,240 --> 00:04:22,650
So if you reject the likelihood ratio
test, that will

84
00:04:22,650 --> 00:04:26,280
tell you that at least one of the betas is
significantly related to the outcome,

85
00:04:26,280 --> 00:04:28,690
that at least one of them is predictive.

86
00:04:28,690 --> 00:04:30,380
It doesn't necessarily tell you which one,
but

87
00:04:30,380 --> 00:04:33,680
it tells you that a group of betas is
significant or not.

88
00:04:33,680 --> 00:04:35,990
So that's what the likely aim of the
likelihood ratio test is.

89
00:04:35,990 --> 00:04:38,800
And the mechanics of it hearken back to
something I

90
00:04:38,800 --> 00:04:42,720
talked about in the last module,

91
00:04:42,720 --> 00:04:47,060
which was that we can actually calculate
the value of the likelihood function.

92
00:04:47,060 --> 00:04:52,820
The actual probability of our data at its
most at the most likely value for

93
00:04:52,820 --> 00:04:53,450
the parameters.

94
00:04:53,450 --> 00:04:55,490
We can plug those parameters back in and

95
00:04:55,490 --> 00:05:00,450
actually get a, a numerical value, a
probability, for the likelihood function.

96
00:05:00,450 --> 00:05:03,710
And that, we are going to be doing that in
order to perform the likelihood

97
00:05:03,710 --> 00:05:04,538
ratio test.

98
00:05:04,538 --> 00:05:08,354
The way a likelihood ratio test is, again,
works, is,

99
00:05:08,354 --> 00:05:10,530
again we're comparing a group of betas.

100
00:05:10,530 --> 00:05:12,820
So we're going to have what's called a
reduced model and

101
00:05:13,910 --> 00:05:16,120
we're going to have what's called a full
model.

102
00:05:16,120 --> 00:05:18,990
The reduced model will have k parameters
in it, so

103
00:05:18,990 --> 00:05:25,130
maybe it looks like the logit is equal to
alpha plus beta 1 plus beta 2.

104
00:05:25,130 --> 00:05:27,860
The full model will contain all the
parameters that

105
00:05:27,860 --> 00:05:31,060
are in the reduced model plus some
additional ones.

106
00:05:31,060 --> 00:05:34,270
So maybe that looks like logit is equal to
alpha plus beta1 plus

107
00:05:35,490 --> 00:05:38,850
beta2 plus beta3 plus beta4.

108
00:05:38,850 --> 00:05:43,550
And if we do the likelihood ratio test
comparing this reduced and

109
00:05:43,550 --> 00:05:46,200
full model, that will tell us whether
adding 3 and

110
00:05:46,200 --> 00:05:49,830
4 to the model adds anything in terms of
predicting our outcome.

111
00:05:50,920 --> 00:05:54,855
If we compare a reduced model with three
parameters in it, as we see here,

112
00:05:54,855 --> 00:05:58,750
as we see here to a, to a full model with
five parameters in it,

113
00:05:58,750 --> 00:06:01,350
then the difference between those two
models is two parameters.

114
00:06:01,350 --> 00:06:04,730
Our likelihood ratio test is going to be a
chi-square with two degrees of freedom.

115
00:06:04,730 --> 00:06:05,350
So we all,

116
00:06:05,350 --> 00:06:08,815
we have a reduced model with k parameters,
a full model with k plus p parameters.

117
00:06:08,815 --> 00:06:11,670
We're going to end up with a chi-squared,
on a chi-squared distribution with

118
00:06:11,670 --> 00:06:15,245
p parameters, p being the difference in
the number of parameters between the two,

119
00:06:15,245 --> 00:06:18,100
the two in this example.

120
00:06:18,100 --> 00:06:21,630
It turns out that if the null hypothesis
is true, if all those betas are equal to

121
00:06:21,630 --> 00:06:27,840
zero, then the negative 2 log of the
likelihood for

122
00:06:27,840 --> 00:06:31,730
the reduced model, that value of the
likelihood, that probability, divided by

123
00:06:31,730 --> 00:06:36,180
the likelihood for that full model, if you
take the ratio of those two and

124
00:06:36,180 --> 00:06:39,970
then take negative 2 log of that, that
that will follow a chi-squared

125
00:06:39,970 --> 00:06:44,000
distribution with p degrees of freedom if
the null hypothesis is true.

126
00:06:45,040 --> 00:06:50,110
Now, the ratio can be broken down because
the log of a quotient,

127
00:06:50,110 --> 00:06:52,660
of two things divided, is the difference
of logs.

128
00:06:52,660 --> 00:06:56,300
So we can actually take this formula and
make it a little simpler by just realizing

129
00:06:56,300 --> 00:06:59,730
that what we're going to end up with is
negative 2 log of the likelihood of

130
00:06:59,730 --> 00:07:02,750
the reduce model minus negative 2 log of
the likelihood of the full model.

131
00:07:02,750 --> 00:07:05,680
That's the same as the ratio, but it's
easier to calculate, so

132
00:07:05,680 --> 00:07:08,240
we're generally going to just do this as
differences.

133
00:07:08,240 --> 00:07:11,470
That's going to have a chi-square with p
degrees of freedom distribution under

134
00:07:11,470 --> 00:07:12,119
the null hypothesis.

135
00:07:13,160 --> 00:07:16,400
Now I'm going to go through a numerical
example that will illustrate how

136
00:07:16,400 --> 00:07:17,990
the likelihood ratio test works.

137
00:07:17,990 --> 00:07:20,750
So first of all, I just want to start by
showing you that for

138
00:07:20,750 --> 00:07:25,000
that simple 2 by 2 data set, I plug that
into the computer, and

139
00:07:25,000 --> 00:07:28,960
it gives me out automatically something
called the likelihood ratio test.

140
00:07:28,960 --> 00:07:33,460
Notice it says above, testing the global
null hypothesis that beta equals zero.

141
00:07:33,460 --> 00:07:38,604
Well for the very simple example of the 2
by 2 table, the reduced model, so,

142
00:07:38,604 --> 00:07:42,280
SAS automatically gives you a reduced
model where you

143
00:07:42,280 --> 00:07:46,850
have a a model where it only contains the
intercept.

144
00:07:46,850 --> 00:07:52,700
So when you get output from your computer,
that's called the likelihood ratio test.

145
00:07:52,700 --> 00:07:55,540
For the global test, this is testing
whether or

146
00:07:55,540 --> 00:07:59,390
not a model with only the intercept is
different than a model with

147
00:07:59,390 --> 00:08:03,270
all the parameters that you have put into
your, into your model.

148
00:08:03,270 --> 00:08:06,821
So, in this very simple 2 by 2 case, we
only had one beta, and

149
00:08:06,821 --> 00:08:08,350
that was the beta for age.

150
00:08:08,350 --> 00:08:12,640
So, here we are comparing the model with
age to the model without age.

151
00:08:12,640 --> 00:08:16,364
In general, you'll be, the statistic will,
that's automatically calculated, will

152
00:08:16,364 --> 00:08:19,745
compare the model with only the intercept
to the model with all the parameters that

153
00:08:19,745 --> 00:08:22,555
you had put in, all the predictors that
you've put in the model.

154
00:08:22,555 --> 00:08:25,841
So this, in this case, we're only going to
be testing whether or

155
00:08:25,841 --> 00:08:29,110
not beta age is significant is a
significant predictor.

156
00:08:29,110 --> 00:08:33,260
If you had multiple betas you'd be testing
whether that set of betas was significant.

157
00:08:33,260 --> 00:08:35,582
Indeed, we get a chi-square that's very
high here.

158
00:08:35,582 --> 00:08:40,780
The p-value is highly significant, so we
do have evidence that age is predictive.

159
00:08:40,780 --> 00:08:43,960
And we saw that from the individual Wald
test as well.

160
00:08:43,960 --> 00:08:46,910
Let me walk you through the math of how
this works now.

161
00:08:46,910 --> 00:08:51,560
So the Reduced Model in this case is just
the intercept only model.

162
00:08:51,560 --> 00:08:54,560
So what does it mean to fit a model with
only the intercept?

163
00:08:54,560 --> 00:08:58,660
All that that means is that what you're
going to get out at the end of the day is

164
00:08:58,660 --> 00:09:05,550
that alpha will be the log odds of having
the disease in the sample,

165
00:09:05,550 --> 00:09:09,300
not accounting for anything, so just the
marginal odds of getting the disease.

166
00:09:09,300 --> 00:09:14,220
So in the case of that, the cardiovascular
disease example, with age,

167
00:09:14,220 --> 00:09:18,060
we had, these were the 2 by 2 numbers,
just to remind you.

168
00:09:18,060 --> 00:09:22,740
We had 43 people with the disease, and we
had 57 people without the disease.

169
00:09:22,740 --> 00:09:25,870
So the odds of getting the disease
overall, the marginal odds,

170
00:09:25,870 --> 00:09:28,340
not accounting for age, is 43 to 57.

171
00:09:28,340 --> 00:09:33,110
So alpha in this case is just going to be
the natural log of the 43 to 57,

172
00:09:33,110 --> 00:09:34,830
of the odds.

173
00:09:34,830 --> 00:09:39,330
Whatever value that comes out to be, I
think that comes out to be negative 0.28,

174
00:09:39,330 --> 00:09:42,320
so that's what alpha will be in this case.

175
00:09:42,320 --> 00:09:44,950
It relates just directly to the overall
probability,

176
00:09:44,950 --> 00:09:47,270
the overall odds of having the disease.

177
00:09:47,270 --> 00:09:49,010
So that's the reduced model.

178
00:09:49,010 --> 00:09:53,980
We can actually go through the logic of
finding out the,

179
00:09:53,980 --> 00:09:58,270
estimating the, the value for alpha, using
maximum likelihood estimation.

180
00:09:58,270 --> 00:09:59,450
I'm going to just run through this
quickly.

181
00:09:59,450 --> 00:10:01,810
This is what we did in the last module as
well.

182
00:10:01,810 --> 00:10:05,960
But what would the likelihood function for
the reduced model actually be?

183
00:10:05,960 --> 00:10:08,010
Well, for the reduced model, we're not
accounting for age.

184
00:10:08,010 --> 00:10:09,050
There are no betas.

185
00:10:09,050 --> 00:10:10,850
The only thing we have in the model is
alpha.

186
00:10:10,850 --> 00:10:13,230
Everybody just has e raised to alpha.

187
00:10:13,230 --> 00:10:17,950
Everybody who gets the disease, there were
43 of them, has e over 1 plus e.

188
00:10:17,950 --> 00:10:19,320
That's the probability of getting that
disease.

189
00:10:19,320 --> 00:10:21,720
Everybody who doesn't get the disease,
there were 57 of them,

190
00:10:21,720 --> 00:10:23,380
has 1 over 1 plus e.

191
00:10:23,380 --> 00:10:25,060
That's it.
So there's nothing taken to

192
00:10:25,060 --> 00:10:27,850
account except whether or not you have the
disease.

193
00:10:27,850 --> 00:10:31,790
We then take that likelihood function, we
take the log as I've done here,

194
00:10:31,790 --> 00:10:35,520
I'm going to just you're going to just
trust me that that's the correct log, or

195
00:10:35,520 --> 00:10:37,950
you could stop the video and try it on
your own.

196
00:10:37,950 --> 00:10:39,420
We take the log of that function,

197
00:10:39,420 --> 00:10:43,310
we take the derivative of the log
likelihood function, we set it equal to 0.

198
00:10:43,310 --> 00:10:48,310
We solve algebraically to find out that e
raised to alpha is 43 out of 57,

199
00:10:48,310 --> 00:10:49,808
the odds of having the disease, or

200
00:10:49,808 --> 00:10:53,690
0.75, and alpha is the natural log of
that, or negative 0.28.

201
00:10:53,690 --> 00:10:57,998
So we solve and find that alpha is equal
to negative 0.28.

202
00:10:57,998 --> 00:11:01,130
The 0.75, the 43 to 57,

203
00:11:01,130 --> 00:11:03,990
is, again, it's just the marginal odds of
cardiovascular disease.

204
00:11:03,990 --> 00:11:07,830
e raised to alpha gives you the marginal
odds of the disease, and,

205
00:11:07,830 --> 00:11:11,520
if there is nothing else in the model,
that's of the disease in the whole sample.

206
00:11:11,520 --> 00:11:12,980
Now I'm going to take that negative 0.28,

207
00:11:12,980 --> 00:11:16,170
and I'm going to plug it back into that
likelihood function, so that I

208
00:11:16,170 --> 00:11:20,160
can get the actual value of the likelihood
function, the actual probability.

209
00:11:20,160 --> 00:11:21,630
So I'm plugging in the negative 0.28,

210
00:11:21,630 --> 00:11:23,965
actually, what I've plugged in just to
make it easy here was,

211
00:11:23,965 --> 00:11:28,010
I plugged in the 0.75, because that's the
e raised to alpha.

212
00:11:28,010 --> 00:11:30,010
So for those who have the disease,

213
00:11:30,010 --> 00:11:33,840
their probability of getting the disease
was 0.75 divided by 1, 0.75.

214
00:11:33,840 --> 00:11:35,590
We raise that to 43.

215
00:11:35,590 --> 00:11:39,120
Those the probability of not getting the
disease was 1 over 1.75,

216
00:11:39,120 --> 00:11:40,670
there's 57 of them.

217
00:11:40,670 --> 00:11:43,088
We plug that in we get an actual
probability value.

218
00:11:43,088 --> 00:11:47,410
Now notice [LAUGH] that the probability of
having these 43 people having

219
00:11:47,410 --> 00:11:51,900
the disease and these other 47 people not
have the disease, that exact probability,

220
00:11:51,900 --> 00:11:55,030
because there's many, many possible
outcomes here, it's really, really small.

221
00:11:55,030 --> 00:11:57,900
It comes out to be 2.1 times 10 to the
negative 30.

222
00:11:57,900 --> 00:12:00,540
That's a tiny, tiny probabilty.

223
00:12:00,540 --> 00:12:04,830
But that's the probability at the alpha
value that's most likely.

224
00:12:04,830 --> 00:12:06,480
There is so many out, possible outcomes
here,

225
00:12:06,480 --> 00:12:09,590
that the highest possible probability is
still a really small probability.

226
00:12:10,610 --> 00:12:11,540
Just keep that in the back of your mind.

227
00:12:11,540 --> 00:12:13,470
But that's the actual value of the
likelihood.

228
00:12:13,470 --> 00:12:17,040
That's the probability of our data with
the most likely alpha in in it.

229
00:12:18,120 --> 00:12:20,170
Now what about the likelihood value of the
full model here?

230
00:12:20,170 --> 00:12:22,920
So, remember, all that the full model has
in this case is

231
00:12:22,920 --> 00:12:27,290
just an additional beta accounting for,
you're either older or you're younger.

232
00:12:27,290 --> 00:12:30,770
And we already solved for that beta in the
earlier module.

233
00:12:30,770 --> 00:12:34,960
We solved for alpha and beta in the
earlier module for that particular model.

234
00:12:34,960 --> 00:12:37,480
And it turns out that we solved and

235
00:12:37,480 --> 00:12:43,170
found that e raised to alpha plus beta,
that was equal to 21 over 6.

236
00:12:43,170 --> 00:12:45,960
So that I'm just plugging in to

237
00:12:45,960 --> 00:12:48,830
the likelihood that we saw in the earlier
module.

238
00:12:48,830 --> 00:12:52,790
I'm plugging in the value that we saw for
e alpha and e beta.

239
00:12:52,790 --> 00:12:55,980
When I plug in e alpha plus beta or e
alpha for

240
00:12:55,980 --> 00:13:00,350
those who are, who don't have, or who
aren't older, they just get e alpha.

241
00:13:00,350 --> 00:13:03,390
I plug in the rest, you know, these were
the, the values of the 2 by 2 tables.

242
00:13:03,390 --> 00:13:05,830
So I'm just plugging in the most likely
values for

243
00:13:05,830 --> 00:13:07,730
alpha beta into the likelihood function.

244
00:13:07,730 --> 00:13:12,710
And I can actually solve out the actual
value of that likelihood function.

245
00:13:12,710 --> 00:13:13,730
So when I solve that out,

246
00:13:13,730 --> 00:13:17,780
it comes out to be again, a really, really
small probability.

247
00:13:17,780 --> 00:13:21,350
The probability of me seeing exactly the
constellation of events that I saw in

248
00:13:21,350 --> 00:13:23,520
my sample is small, but this,

249
00:13:23,520 --> 00:13:27,450
notice, that this probability is times 10
to the negative 26.

250
00:13:27,450 --> 00:13:33,140
It is higher than the probability that we
got from the reduced model.

251
00:13:33,140 --> 00:13:34,160
And that makes sense,

252
00:13:34,160 --> 00:13:39,530
because age does have something to do with
cardiovascular disease.

253
00:13:39,530 --> 00:13:42,100
So when we account for age in our model,

254
00:13:42,100 --> 00:13:46,630
we are doing better at predicting who gets
cardiovascular disease and who doesn't.

255
00:13:46,630 --> 00:13:50,050
So the probability of our data goes up
when we account for age.

256
00:13:51,080 --> 00:13:54,890
Now, the question for the likelihood ratio
test is, does our

257
00:13:54,890 --> 00:13:59,830
probability go up enough when we add age
or whatever the betas are to the model?

258
00:13:59,830 --> 00:14:02,650
Does it go up enough that it's worth
keeping age in the model?

259
00:14:02,650 --> 00:14:04,480
Have we budged our probability enough?

260
00:14:04,480 --> 00:14:06,460
When you go from the reduced model to the
full model,

261
00:14:06,460 --> 00:14:08,210
the likelihood value will always get
bigger.

262
00:14:08,210 --> 00:14:11,650
But does it get bigger by enough that it's
worth keeping, say, age in the model?

263
00:14:12,710 --> 00:14:16,350
So the likelihood ratio test, then, we get
by just plugging in.

264
00:14:16,350 --> 00:14:19,560
We do negative 2 log the negative log
likelihood for

265
00:14:19,560 --> 00:14:24,550
the reduced model, minus the negative 2
log likelihood for the full model.

266
00:14:24,550 --> 00:14:29,620
The reduced model always has a smaller
probability, but when you take negative 2

267
00:14:29,620 --> 00:14:33,090
log of a smaller probability, you're going
to end up with a bigger value, so

268
00:14:33,090 --> 00:14:35,790
the reduced, the negative 2 log,
likelihood of

269
00:14:35,790 --> 00:14:38,440
the reduced model will always be bigger
than that for the full model.

270
00:14:38,440 --> 00:14:42,240
So we do reduced minus full to end up with
a positive number.

271
00:14:42,240 --> 00:14:43,160
So I've just plugged in

272
00:14:43,160 --> 00:14:46,310
those probabilities that I calculated on
the earlier slides.

273
00:14:46,310 --> 00:14:47,550
Negative 2 log likelihood for

274
00:14:47,550 --> 00:14:52,210
the intercept only model, minus for the
model that included age.

275
00:14:52,210 --> 00:14:54,530
I get values for the negative 2 log
likelihoods.

276
00:14:54,530 --> 00:14:58,020
Notice I get nice whole values instead of
those really tiny probabilities.

277
00:14:58,020 --> 00:15:03,010
I get 136 minus 117, the difference
between those turns out to be 18.7.

278
00:15:03,010 --> 00:15:07,430
The null hypothesis in this case, because
the only difference between the two

279
00:15:07,430 --> 00:15:11,930
models is the beta for age is, is that,
that the beta age is equal to 0.

280
00:15:11,930 --> 00:15:14,820
Clearly this is a highly significant
chi-square, and

281
00:15:14,820 --> 00:15:18,370
so we can reject the null hypothesis that
beta is equal to 0.

282
00:15:18,370 --> 00:15:20,930
We gain something by adding beta to the
model.

283
00:15:21,930 --> 00:15:25,570
Now notice that the Wald test here give us
a very similar answer to

284
00:15:25,570 --> 00:15:27,490
the likelihood ratio test.

285
00:15:27,490 --> 00:15:31,640
The difference is that the Likelihood
ratio test can be used when we

286
00:15:31,640 --> 00:15:36,360
want to look at groups of betas, rather
than a single beta at a time.

287
00:15:36,360 --> 00:15:38,940
And that's generally where we end up using
the likelihood ratio test.

288
00:15:38,940 --> 00:15:43,630
So just to give you an example of where
the likelihood ratio test would be useful,

289
00:15:43,630 --> 00:15:49,760
take the example that we had in a previous
module which was a 2 by 4 table,

290
00:15:49,760 --> 00:15:53,420
cardiovascular disease and four categories
of age.

291
00:15:53,420 --> 00:15:56,870
If we do a fetal logistic regression model
here, what does the model look like?

292
00:15:56,870 --> 00:16:01,900
So its the logit for cardiovascular
disease, that is equal to, and

293
00:16:01,900 --> 00:16:03,360
we're going to have to dummy code race,

294
00:16:03,360 --> 00:16:07,430
because race is a a categorical variable
with multiple levels.

295
00:16:07,430 --> 00:16:12,340
So we're going to have three betas in the
model, because there are four categories.

296
00:16:12,340 --> 00:16:16,800
The refererence group gets absorbed into
the intercept.

297
00:16:16,800 --> 00:16:18,300
I'm going to make the reference group the
white group

298
00:16:18,300 --> 00:16:21,560
here because they have the lowest risk of
cardiovascular disease.

299
00:16:21,560 --> 00:16:25,290
Then I have to have a beta for black,
Hispanic and other.

300
00:16:25,290 --> 00:16:26,690
So I've dummy coded here.

301
00:16:27,820 --> 00:16:30,430
And I've added race in the model as three
parameters.

302
00:16:30,430 --> 00:16:35,350
Notice that sometimes I might want to know
the effects of individual races,

303
00:16:35,350 --> 00:16:40,260
but sometimes I might just want to know
the effect of race overall as a single,

304
00:16:40,260 --> 00:16:41,800
categorical variable.

305
00:16:41,800 --> 00:16:44,970
The Wald test does not give me p-values
for

306
00:16:44,970 --> 00:16:48,510
ind, the Wald test gives me p-values for
individual races.

307
00:16:48,510 --> 00:16:50,970
But it doesn't give me a p-value overall
for

308
00:16:50,970 --> 00:16:54,070
race as a kind of a cumulative variable.

309
00:16:54,070 --> 00:16:56,440
That's what the likelihood ratio test is
going to give me.

310
00:16:56,440 --> 00:17:01,690
So when I run a model, in this case, I ran
a model that contained the intercept and

311
00:17:01,690 --> 00:17:04,400
the three dummy coded variables for race.

312
00:17:04,400 --> 00:17:09,440
The likelihood ratio test statistic that I
get up at the top of my output is testing

313
00:17:09,440 --> 00:17:12,640
the global null hypothesis that all the
betas in the model are equal to zero.

314
00:17:12,640 --> 00:17:19,160
So it's testing whether or not the beta
for these three groups,

315
00:17:20,360 --> 00:17:24,240
whether or not those are equal to zero,
and those betas represent whether or

316
00:17:24,240 --> 00:17:27,160
not these groups are different than the
reference group.

317
00:17:27,160 --> 00:17:28,375
So we're testing, overall,

318
00:17:28,375 --> 00:17:32,770
are any of those betas, significantly
different than the white group.

319
00:17:32,770 --> 00:17:35,790
That's what the likelihood ratio test will
test when we look at

320
00:17:35,790 --> 00:17:40,010
the reduced model here, the intercept only
model compared with the model with race,

321
00:17:40,010 --> 00:17:41,830
the likelihood ratio statistic comes out
to be 14.

322
00:17:41,830 --> 00:17:45,310
I'm not going to through the math on that
but we're going to believe the computer.

323
00:17:45,310 --> 00:17:47,710
And that's a significant p-value.

324
00:17:47,710 --> 00:17:50,630
We've three degrees of freedom here
because the reduced model has

325
00:17:50,630 --> 00:17:51,980
only the intercept.

326
00:17:51,980 --> 00:17:54,420
The full model has three additional
parameters, so

327
00:17:54,420 --> 00:17:56,650
we're going to have three degrees of
freedom.

328
00:17:56,650 --> 00:17:57,450
So what does this tell us?

329
00:17:57,450 --> 00:18:01,610
This tells us that race, overall, as a
categorical variable has some

330
00:18:01,610 --> 00:18:04,830
predictive power in predicting
cardiovascular disease.

331
00:18:04,830 --> 00:18:07,840
That's giving us a slightly different
piece of information that if

332
00:18:07,840 --> 00:18:12,560
we look at the individual Wald p-values
for the individual race groups.

333
00:18:12,560 --> 00:18:15,568
Those individual p-values tell us
specifically which

334
00:18:15,568 --> 00:18:18,830
racial groups are different than the
reference group.

335
00:18:18,830 --> 00:18:21,670
That those are giving us p-values for each
individual betas.

336
00:18:21,670 --> 00:18:25,480
Sometimes they want to get a p-value over
all for a group of betas.

337
00:18:25,480 --> 00:18:27,890
Typically, with a categorical variable
like that,

338
00:18:27,890 --> 00:18:29,780
the likelihood ratio test will give you
that.
