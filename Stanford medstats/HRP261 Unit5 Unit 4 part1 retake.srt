1
00:00:00,000 --> 00:00:05,840
[BLANK_AUDIO]

2
00:00:05,840 --> 00:00:09,480
In this next module, I'm going to show you
how to apply principal components

3
00:00:09,480 --> 00:00:11,650
analysis to real data.

4
00:00:11,650 --> 00:00:15,860
So I'm going to use this example of the 91
women runners,

5
00:00:15,860 --> 00:00:18,230
the data set that I've been referring to
this week.

6
00:00:18,230 --> 00:00:21,570
This was a data set I was looking at on
potential predictors of bone density and

7
00:00:21,570 --> 00:00:24,010
stress fractures in adolescent women
runners.

8
00:00:24,010 --> 00:00:27,730
This is a group that's at risk of low bone
density and fractures.

9
00:00:27,730 --> 00:00:29,900
So the research has measured a number of
variables.

10
00:00:29,900 --> 00:00:32,480
They measured some variables on body size
and composition.

11
00:00:32,480 --> 00:00:34,470
We saw those in an earlier module.

12
00:00:34,470 --> 00:00:38,800
They also looked at running performance
and training, since these are all runners.

13
00:00:38,800 --> 00:00:42,520
They also looked at menstrual function,
which is linked to bone density.

14
00:00:42,520 --> 00:00:44,980
They also looked at things about diet and
disordered eating.

15
00:00:46,040 --> 00:00:48,610
I'm just going to focus on a subset of
variables,

16
00:00:48,610 --> 00:00:50,950
just to keep this exercise manageable.

17
00:00:50,950 --> 00:00:53,160
So I'll look at these first three
categories.

18
00:00:53,160 --> 00:00:56,360
And just a few of the variables from
within each of these categories.

19
00:00:56,360 --> 00:00:58,700
I'll look at a total of 11 variables, and

20
00:00:58,700 --> 00:01:01,920
I'm going to apply principal components
analysis to these 11 variables.

21
00:01:03,160 --> 00:01:05,620
So here are the 11 variables that we're
going to be using for

22
00:01:05,620 --> 00:01:07,360
this principal components analysis.

23
00:01:07,360 --> 00:01:12,040
So first of all, the researches did a DEXA
measurement on all of these women.

24
00:01:12,040 --> 00:01:14,590
The DEXA measurement measures their bone
density, but

25
00:01:14,590 --> 00:01:18,220
it also gives you very precise
measurements of fat and lean mass.

26
00:01:18,220 --> 00:01:21,270
So they had measured what's called the
Android fat ratio,

27
00:01:21,270 --> 00:01:24,920
which is a marker of visceral fat like how
much belly fat you're carrying.

28
00:01:24,920 --> 00:01:28,660
They also had fat mass, BMI, lean mass and
height.

29
00:01:28,660 --> 00:01:32,130
The fat mass comes also out of the DEXA as
well as the lean mass.

30
00:01:32,130 --> 00:01:34,970
Not surprisingly, all of these measures of
body size and

31
00:01:34,970 --> 00:01:37,960
composition are all highly correlated.

32
00:01:37,960 --> 00:01:40,660
They also have measures of running
performance and training.

33
00:01:40,660 --> 00:01:43,910
Those would include the best mile time,
how many miles per

34
00:01:43,910 --> 00:01:48,160
week women were running, and the pace at
which they did their interval work.

35
00:01:48,160 --> 00:01:50,780
And of course, those again are all highly
correlated.

36
00:01:50,780 --> 00:01:53,580
They also looked at menarche age,
menstrual periods in the past periods in

37
00:01:53,580 --> 00:01:56,920
the past year, average menstrual periods
since menarche.

38
00:01:56,920 --> 00:02:00,520
These are all obviously menstrual function
variables that also relates to

39
00:02:00,520 --> 00:02:02,090
bone density.

40
00:02:02,090 --> 00:02:07,730
So not only are the variables within each
of these categories highly correlated,

41
00:02:07,730 --> 00:02:10,580
but, they're also correlated across
different categories.

42
00:02:10,580 --> 00:02:13,870
So, it turns out that the women who had
higher fat mass and

43
00:02:13,870 --> 00:02:19,470
weighed more, tended to actually be slower
and to run less.

44
00:02:19,470 --> 00:02:24,830
And, so that was related, correlated to
running performance.

45
00:02:24,830 --> 00:02:26,750
And, this is probably bidirectional.

46
00:02:26,750 --> 00:02:30,090
So, if you're carrying more fat, that may
slow you down.

47
00:02:30,090 --> 00:02:33,660
And plus, the women who are competitive
and who are running more miles per

48
00:02:33,660 --> 00:02:36,960
week are probably also going to be leaner
because they're running a lot, so

49
00:02:36,960 --> 00:02:38,560
that could go in both directions.

50
00:02:38,560 --> 00:02:40,080
But those were correlated.

51
00:02:40,080 --> 00:02:42,960
Also, your measures of body composition
and

52
00:02:42,960 --> 00:02:47,410
size, especially, fat mass, again, was
related to menstrual function.

53
00:02:47,410 --> 00:02:51,180
So in this population it turns out that
carrying more fat is

54
00:02:51,180 --> 00:02:54,250
actually good in terms of having better
menstrual function.

55
00:02:55,590 --> 00:02:59,130
So we're going to take these highly
correlated 11 variables and

56
00:02:59,130 --> 00:03:02,260
we're going to put them in our principal
components analysis.

57
00:03:02,260 --> 00:03:05,430
And the goal here is to try to come up
with a smaller subset of

58
00:03:05,430 --> 00:03:08,910
variables that represent these larger
constructs such as body size and

59
00:03:08,910 --> 00:03:11,330
composition or menstrual function.

60
00:03:11,330 --> 00:03:14,990
And I'm now going to walk you through the
steps of

61
00:03:14,990 --> 00:03:17,550
actually doing a principal components
analysis.

62
00:03:19,090 --> 00:03:22,170
And I'll just walk you through each one of
these steps in turn, so

63
00:03:22,170 --> 00:03:24,800
let's just turn to each one of these steps
now, okay.

64
00:03:24,800 --> 00:03:29,770
So, the first step, before you even put
the data into the principal components

65
00:03:29,770 --> 00:03:34,000
analysis, there's a few processing steps
that you have to do first.

66
00:03:34,000 --> 00:03:37,340
Very importantly, you need to impute any
missing data.

67
00:03:38,460 --> 00:03:43,890
So, if an observation is missing even one
data point, from just one variable,

68
00:03:43,890 --> 00:03:48,060
that observation will not be included in
the principal components analysis.

69
00:03:48,060 --> 00:03:51,970
What that means is, if just a few of your
observations are missing a few data

70
00:03:51,970 --> 00:03:56,470
points on a couple of variables, that can
really add up.

71
00:03:56,470 --> 00:03:58,670
One missing data point gets you kicked out
of the analysis.

72
00:03:58,670 --> 00:04:02,010
There, we're for putting 11 variables into
the principal components analysis.

73
00:04:02,010 --> 00:04:05,480
There's a lot of opportunities for
observations to get kicked out.

74
00:04:05,480 --> 00:04:07,050
If you were putting more than 11
variables,

75
00:04:07,050 --> 00:04:10,870
which is very typical, you can imagine
that a little bit of missing data here and

76
00:04:10,870 --> 00:04:14,560
there can result in, say you start with a
sample size of 500,

77
00:04:14,560 --> 00:04:18,580
that suddenly might turn out to go down to
a sample size of 200.

78
00:04:18,580 --> 00:04:23,530
In other words, your analysis might only
run on 200 of your observations even if

79
00:04:23,530 --> 00:04:24,370
you started with 500.

80
00:04:24,370 --> 00:04:26,810
So you've got to be very careful and

81
00:04:26,810 --> 00:04:30,370
make sure that you always fill in your
missing data in some way.

82
00:04:30,370 --> 00:04:33,210
The other thing that you're going to need
to do is you need to

83
00:04:33,210 --> 00:04:34,590
standardize your variables.

84
00:04:34,590 --> 00:04:36,130
We already talked about that.

85
00:04:36,130 --> 00:04:39,270
They all have to have a mean of zero and a
standard deviation of one.

86
00:04:39,270 --> 00:04:42,860
One other thing I just want to point out
now,

87
00:04:42,860 --> 00:04:47,480
is that principal components analysis
typically run numeric variables.

88
00:04:47,480 --> 00:04:51,740
So, we saw in the last module that
principal components analysis is based on

89
00:04:51,740 --> 00:04:52,990
the linear relationship.

90
00:04:52,990 --> 00:04:57,080
That is, the linear correlation between
pairs of variables.

91
00:04:57,080 --> 00:05:00,720
So technically, all the assumptions of
linear regression would apply here.

92
00:05:00,720 --> 00:05:04,390
That would mean that you should have
continuous normally distributed variables.

93
00:05:04,390 --> 00:05:08,200
Now in practice we actually don't always
worry about these assumptions.

94
00:05:08,200 --> 00:05:11,810
It's important to meet these assumptions
if you're trying to calculate p values.

95
00:05:11,810 --> 00:05:15,020
But that's not what we're usually doing
with principal components analysis.

96
00:05:15,020 --> 00:05:17,590
We're usually doing something more
exploratory.

97
00:05:17,590 --> 00:05:19,010
And for exploratory purposes it

98
00:05:19,010 --> 00:05:21,770
really doesn't matter if you violate these
assumptions.

99
00:05:21,770 --> 00:05:24,760
But generally we're going to be dealing
with numeric variables.

100
00:05:24,760 --> 00:05:26,820
Even if they're not perfectly normally
distributed.

101
00:05:26,820 --> 00:05:29,180
And these nu, numeric variables need to be
standardized.

102
00:05:29,180 --> 00:05:33,920
So we're going to do that first, I've
already done that to this data set.

103
00:05:33,920 --> 00:05:36,884
The next thing we're going to do is we're
going to actually apply the PCA

104
00:05:36,884 --> 00:05:40,004
analysis and you can do this in most
standard statistical packages with

105
00:05:40,004 --> 00:05:42,000
the right code or the right click of a
button.

106
00:05:42,000 --> 00:05:47,313
Its usually fairly straightforward, I put
my 11 variables into a SASS and apply

107
00:05:47,313 --> 00:05:52,733
my principal components analysis and I'll
show you the results in just a minute.

108
00:05:52,733 --> 00:05:57,632
But the first decision that you actually
have to make after you apply this

109
00:05:57,632 --> 00:06:02,890
analysis to your data is to decide how
many components do you want to retain?

110
00:06:02,890 --> 00:06:04,750
So we started with 11 variables, and

111
00:06:04,750 --> 00:06:08,090
the idea is we want to compress those
variables into a smaller number.

112
00:06:08,090 --> 00:06:11,480
So at some point I have to decide what's
that smaller number?

113
00:06:11,480 --> 00:06:15,880
When I run the principal components
analysis on 11 variables, I'm going to get

114
00:06:15,880 --> 00:06:18,510
11 components back out, but I'm going to
want to weed some of these out.

115
00:06:18,510 --> 00:06:21,890
So I have to decide, do I want to get down
to three variables, do I want to

116
00:06:21,890 --> 00:06:25,860
get down to four, five, six, how many
components am I actually going to keep?

117
00:06:25,860 --> 00:06:30,470
And there's a number of different criteria
that can be used to make this decision.

118
00:06:31,500 --> 00:06:32,390
There's no hard and

119
00:06:32,390 --> 00:06:35,180
fast rule, so here's all the different
kind of criteria that we're going to use.

120
00:06:35,180 --> 00:06:39,270
I'll walk you through these in turn, but I
just want to point out that there's,

121
00:06:39,270 --> 00:06:43,370
there's no hard and fast rule, and the
answer at the end of the day really is

122
00:06:43,370 --> 00:06:47,660
going to depend on your particular data
set, and the goals of your analysis.

123
00:06:47,660 --> 00:06:48,740
Okay.

124
00:06:48,740 --> 00:06:51,840
Now let me go through each one of these
steps in turn.

125
00:06:51,840 --> 00:06:57,640
So, first thing is, I applied my principal
components analysis to my data set.

126
00:06:57,640 --> 00:07:01,240
So when I have, start with 11 variables,
I'm going to end up with 11 components.

127
00:07:01,240 --> 00:07:04,370
So you see those 11 components are
pictured here.

128
00:07:04,370 --> 00:07:07,560
So this is one of the outputs that you get
from your statistical package.

129
00:07:07,560 --> 00:07:08,610
You get the Eigenvalues.

130
00:07:08,610 --> 00:07:11,478
We talked about the Eigenvalues in the
last module.

131
00:07:11,478 --> 00:07:13,440
Remember that Eigenvalues just re,

132
00:07:13,440 --> 00:07:19,000
represent how much variance is explained
by that particular component.

133
00:07:19,000 --> 00:07:21,790
And they, the components are going to be
from the highest to the lowest, so

134
00:07:21,790 --> 00:07:24,970
the first component is going explain it in
the most of the variance and

135
00:07:24,970 --> 00:07:28,030
they get diminishingly smaller after that.

136
00:07:28,030 --> 00:07:32,020
Now, some people will use a very simple
criteria for deciding how

137
00:07:32,020 --> 00:07:35,900
many components to retain in your analysis
by just looking at the eigenvalue.

138
00:07:35,900 --> 00:07:38,750
So, they'll say, if the eigenvalue is
greater than one,

139
00:07:38,750 --> 00:07:40,859
then we're going to keep that component
and

140
00:07:40,859 --> 00:07:44,500
if the eigenvalue is less than one, we're
going to drop that component.

141
00:07:44,500 --> 00:07:50,733
According to that criteria, here, we would
retain four components in our analysis.

142
00:07:50,733 --> 00:07:52,040
This is the idea.

143
00:07:52,040 --> 00:07:56,370
Each variable entered into the model
started with a variance of one.

144
00:07:56,370 --> 00:08:00,160
So every original variable, by itself,
explained a variance of one.

145
00:08:00,160 --> 00:08:01,920
Remember because we have standardized
variables, so

146
00:08:01,920 --> 00:08:03,790
all of the variances are going to be equal
to one.

147
00:08:03,790 --> 00:08:05,980
So we started with a variance sort of of
11.

148
00:08:05,980 --> 00:08:10,640
Each one had one for a total of our
variance of 11 in our data set.

149
00:08:10,640 --> 00:08:13,360
So a very simple criteria would just be to
say, okay,

150
00:08:13,360 --> 00:08:17,200
well, any eigenvalue greater than one
would mean that that

151
00:08:17,200 --> 00:08:22,500
component explains more of the variance
than any single original variable.

152
00:08:22,500 --> 00:08:25,790
Since we're getting more information than
a single variable could give us,

153
00:08:25,790 --> 00:08:27,710
it seems like it would be worth keeping
them.

154
00:08:27,710 --> 00:08:31,110
On the other hand, if the eigenvalue is
less than one,

155
00:08:31,110 --> 00:08:36,200
then that component explains less than the
variable contained in any single variable.

156
00:08:36,200 --> 00:08:39,890
So, that would argue that you might want
to drop that variable because it

157
00:08:39,890 --> 00:08:43,320
contains less information than sort of,
you started with.

158
00:08:43,320 --> 00:08:48,290
Now, this eigenvalue greater than one
criteria has advantages and disadvantages.

159
00:08:48,290 --> 00:08:51,060
So an advantage is that it is really easy
to, to apply.

160
00:08:51,060 --> 00:08:54,160
A disadvantage is, it may not always be
the right choice for

161
00:08:54,160 --> 00:08:55,160
your particular data set.

162
00:08:55,160 --> 00:08:58,390
You're going to need to look at other
things to make this decision.

163
00:08:58,390 --> 00:09:01,170
In the end of the day in this particular
example I did in

164
00:09:01,170 --> 00:09:04,142
fact end up retaining four components.

165
00:09:04,142 --> 00:09:08,650
There are several reasons beside just the
eigenvalue greater than one criteria.

166
00:09:08,650 --> 00:09:10,170
So here's some other things that we can
look at.

167
00:09:11,440 --> 00:09:16,160
We can also just look at the percent of
the total variance explained.

168
00:09:16,160 --> 00:09:17,730
So you'll notice that again,

169
00:09:17,730 --> 00:09:22,270
as I mentioned, each of these explains
some amount of variance.

170
00:09:22,270 --> 00:09:23,070
Well obviously the,

171
00:09:23,070 --> 00:09:26,660
the first component always explains the
greatest amount of variance.

172
00:09:26,660 --> 00:09:29,690
So you can see that this output tells me
how much,

173
00:09:29,690 --> 00:09:34,550
what proportion of the variance, did each
component explain.

174
00:09:34,550 --> 00:09:39,610
The first principal component, in this
case, explained a variance of 3.5.

175
00:09:39,610 --> 00:09:41,630
The total variance was 11, so

176
00:09:41,630 --> 00:09:47,850
this principle component explained 31.95%
of the total variance.

177
00:09:47,850 --> 00:09:54,630
The second principal component explained a
variance of 2.15 out of a total of 11.

178
00:09:54,630 --> 00:09:58,710
So that one explained 19.59% of the total
variance.

179
00:09:58,710 --> 00:10:00,090
You can look at the cumulative total.

180
00:10:00,090 --> 00:10:03,260
So when, the first explains 31 or 32%.

181
00:10:03,260 --> 00:10:04,550
Once we've got two components,

182
00:10:04,550 --> 00:10:09,330
we've actually explained more than half of
the variance in our original 11 variables.

183
00:10:09,330 --> 00:10:13,730
If we go to a third component, we've now
got up to explaining 64% of the variance.

184
00:10:13,730 --> 00:10:18,190
If we go up to a fourth component, we've
now explained 76% of the variance.

185
00:10:18,190 --> 00:10:20,270
So in just four variables now,

186
00:10:20,270 --> 00:10:26,020
we have captured 76% of the variability of
the information in the original 11.

187
00:10:26,020 --> 00:10:28,920
And that might be enough, we might say,
well hay that's pretty good.

188
00:10:28,920 --> 00:10:32,300
With just four I can get 76% of the total
variance.

189
00:10:32,300 --> 00:10:35,580
Some might argue that well, if I go up one
more I can get and

190
00:10:35,580 --> 00:10:38,012
additional 8% up to 84%.

191
00:10:38,012 --> 00:10:41,030
And so, some people might argue to go up
to five,

192
00:10:41,030 --> 00:10:45,020
just to get that much higher with only one
additional component.

193
00:10:45,020 --> 00:10:47,930
Again, in this case I ended up deciding to
go with four.

194
00:10:47,930 --> 00:10:50,640
I'll tell you that I also ran on the
analysis with five,

195
00:10:50,640 --> 00:10:54,480
so I can show you what the difference was
in the little a little bit later on.

196
00:10:54,480 --> 00:10:57,998
So that's the eigenvalue greater than one
and just looking at the total variance.

197
00:10:57,998 --> 00:11:03,380
Another common thing that people look at
is something called the Scree plot, okay.

198
00:11:03,380 --> 00:11:07,720
So this scree pot, plot, what it's showing
you, is it basically take the information

199
00:11:07,720 --> 00:11:11,560
that I was showing you, on, in table form,
and it just puts it into a plot.

200
00:11:11,560 --> 00:11:13,890
So on the x axis, we have the components.

201
00:11:13,890 --> 00:11:15,770
They're just numbered here from one to 11.

202
00:11:15,770 --> 00:11:17,780
Those are just, so, the number component.

203
00:11:17,780 --> 00:11:20,200
On the y axis we plot the eigenvalue,
which,

204
00:11:20,200 --> 00:11:22,150
again, I was just showing you in the last
table.

205
00:11:22,150 --> 00:11:25,720
So, for example, the first principal
component had an eigenvalue of 3.5.

206
00:11:25,720 --> 00:11:29,110
And then you just connect the dots.

207
00:11:29,110 --> 00:11:31,880
And the idea with this Scree Plot is
you're looking for

208
00:11:31,880 --> 00:11:34,490
a place where, you know, that, you'll
always see where

209
00:11:34,490 --> 00:11:37,480
this Scree Plot there'll be kind of a
steep drop at the beginning.

210
00:11:37,480 --> 00:11:40,350
And then at some point it might level off.

211
00:11:40,350 --> 00:11:42,500
And if you can find that point where the,
you,

212
00:11:42,500 --> 00:11:47,070
goes from being a steep drop to leveling
off, that's what we call the elbow.

213
00:11:47,070 --> 00:11:50,800
And a lot of people will say you'll just
retain components up until the elbow.

214
00:11:50,800 --> 00:11:53,110
Now, this a is real data and so

215
00:11:53,110 --> 00:11:55,990
real data doesn't always come out the way
we want it to.

216
00:11:55,990 --> 00:11:59,260
And in this particular screen plot doesn't
have a really clear elbow, I mean you

217
00:11:59,260 --> 00:12:02,390
could kind of say, well you know you could
drop down here and then maybe at three.

218
00:12:02,390 --> 00:12:05,850
Maybe that's an elbow, because you start
to level off.

219
00:12:05,850 --> 00:12:08,890
But then it starts dropping steeply again
between four and five.

220
00:12:08,890 --> 00:12:12,138
So, you could say, well maybe, you know,
the elbow is here at six,

221
00:12:12,138 --> 00:12:14,770
because this clearly starts leveling off
at about six.

222
00:12:14,770 --> 00:12:20,060
So, if there's no clear break point in
this particular data set,

223
00:12:20,060 --> 00:12:21,740
I would say, you know, we can say for

224
00:12:21,740 --> 00:12:24,540
sure that by the time you get to six,
you're really leveling off.

225
00:12:24,540 --> 00:12:28,040
So you're probably going to want to retain
fewer than six components.

226
00:12:28,040 --> 00:12:31,860
Again, whether you choose four or five or
three, it's a little you're going to

227
00:12:31,860 --> 00:12:34,930
have to use some other information to help
you make that decision.

228
00:12:36,540 --> 00:12:41,220
Now, because, this is what I, I just
want to show you that our data set didn't

229
00:12:41,220 --> 00:12:45,770
generate the most interesting scree plot,
so I made up a hypothetical scree plot,

230
00:12:45,770 --> 00:12:49,650
just to show you what you're kind of
hoping your scree plot will look like.

231
00:12:49,650 --> 00:12:52,930
So this is sort of a typical scree plot
that you're hoping to get because it

232
00:12:52,930 --> 00:12:55,130
will make your life easier.

233
00:12:55,130 --> 00:12:58,850
This one again has a total of 11
components on the x axis.

234
00:12:58,850 --> 00:13:00,630
The eigenvalues are on the y axis.

235
00:13:00,630 --> 00:13:02,120
And again these are made up data, so

236
00:13:02,120 --> 00:13:05,510
that's why they look so pretty and, and
nice and smooth here.

237
00:13:05,510 --> 00:13:08,830
So, what we're looking for in the screen
plot again, is this elbow.

238
00:13:08,830 --> 00:13:12,180
So the elbow in this case occurs at
component four.

239
00:13:12,180 --> 00:13:13,740
The elbow is the inflection point.

240
00:13:13,740 --> 00:13:18,140
It's where you go from having a steep drop
off to have, to plateauing.

241
00:13:18,140 --> 00:13:20,370
And you can see there's a clear elbow here
at four.

242
00:13:20,370 --> 00:13:25,690
And what people will do is they'll say,
okay, you, you identify the elbow, and

243
00:13:25,690 --> 00:13:29,710
you either decide to retain the number of
components that marks the elbow, so

244
00:13:29,710 --> 00:13:32,710
you, you could retain four, or even more
typically,

245
00:13:32,710 --> 00:13:36,510
more commonly, people will retain, retain
one less than the elbow.

246
00:13:36,510 --> 00:13:38,080
So they might say, okay, the elbow's at
four, so

247
00:13:38,080 --> 00:13:41,090
therefore I'm going to retain three
components.

248
00:13:41,090 --> 00:13:43,940
So looking at a screen plot like this, you
were going to, you're going to decide to

249
00:13:43,940 --> 00:13:48,340
retain either three or four, after three
or four there is clearly a diminishing

250
00:13:48,340 --> 00:13:52,030
returns, you're not getting much from the
additional components.

251
00:13:52,030 --> 00:13:55,031
Again, with real data, doesn't always look
so perfect.

252
00:13:55,031 --> 00:13:58,333
But this is sort of an idealize scree
plot.

253
00:13:58,333 --> 00:14:00,385
So we're going to look at all of those
factors,

254
00:14:00,385 --> 00:14:03,949
the eigenvalues, the percent of the
variants explained and the scree plot and

255
00:14:03,949 --> 00:14:07,630
we're going to make some kind of decision
about how many variable to retain.

256
00:14:07,630 --> 00:14:09,840
In this case, I decided to retain four.

257
00:14:09,840 --> 00:14:13,180
I'll also show you what happened when I
retained five,

258
00:14:13,180 --> 00:14:16,080
just to show you the difference there.

259
00:14:16,080 --> 00:14:19,660
Just to emphasize again, you're always
considering your own

260
00:14:19,660 --> 00:14:25,240
dataset-specific goals, so there may be
cases

261
00:14:25,240 --> 00:14:29,760
where you decide to retain a component
that doesn't explain much of the variance.

262
00:14:29,760 --> 00:14:32,800
So, let's say that there's a particular
construct that doesn't

263
00:14:32,800 --> 00:14:35,570
appear until component six.

264
00:14:35,570 --> 00:14:38,420
But you really care about isolating that
construct and

265
00:14:38,420 --> 00:14:40,640
looking at the effect of that construct.

266
00:14:40,640 --> 00:14:43,870
In that case, even if that particular
component doesn't explain much of

267
00:14:43,870 --> 00:14:48,020
the variance, you might decide to go out
to six components just because there's

268
00:14:48,020 --> 00:14:50,650
something out at six that you really care
about.

269
00:14:50,650 --> 00:14:54,390
So again, you really have to consider the
goals of your particular analysis when

270
00:14:54,390 --> 00:14:55,520
making this decision.

271
00:14:57,080 --> 00:14:59,780
Okay.
So, the next step that we're going to do

272
00:14:59,780 --> 00:15:04,640
is we're going to apply something called a
Varimax rotation, okay?

273
00:15:04,640 --> 00:15:07,760
Actually, the computer is going to do this
for us.

274
00:15:07,760 --> 00:15:10,910
So once we've decided how many components
to retain, we're going to retain four in

275
00:15:10,910 --> 00:15:14,810
this case, then we're going to ask the
computer to apply this Varimax rotation.

276
00:15:14,810 --> 00:15:17,020
What is the point of this?

277
00:15:17,020 --> 00:15:21,280
The point of the Varimax rotation is that
it forces each variable,

278
00:15:21,280 --> 00:15:25,470
each of my original 11 variables, to load
strongly.

279
00:15:25,470 --> 00:15:29,790
To align with only one particular
component and not with the other.

280
00:15:29,790 --> 00:15:32,060
And, I'm going to show you exactly how
this works in a minute.

281
00:15:33,130 --> 00:15:36,330
This is going to make our results more
interpretable because it

282
00:15:36,330 --> 00:15:41,120
will be really clear, for example, that
fat mass loads on principle component one.

283
00:15:41,120 --> 00:15:44,790
And thus that principle component one has
something to do with body fat.

284
00:15:44,790 --> 00:15:49,210
So this rotation helps us purely in terms
of interpretation of the components, and

285
00:15:49,210 --> 00:15:51,940
if you didn't need to interpret the
components for some reason.

286
00:15:51,940 --> 00:15:54,400
If your analysis wasn't focused on that,

287
00:15:54,400 --> 00:15:56,480
then you wouldn't necessarily need to
apply this.

288
00:15:56,480 --> 00:15:58,450
But for most examples in epidemiology and

289
00:15:58,450 --> 00:16:02,230
medicine, we actually want to understand
what those components actually mean.

290
00:16:02,230 --> 00:16:04,640
So we're going to apply this Varimax
rotation.

291
00:16:04,640 --> 00:16:06,210
The rotation is also orthogonal.

292
00:16:06,210 --> 00:16:09,300
All that means is that it guarantees us
that at the end of the day,

293
00:16:09,300 --> 00:16:11,550
our components will be uncorrelated with
one another.

294
00:16:11,550 --> 00:16:12,960
They'll be independent.

295
00:16:12,960 --> 00:16:16,940
And so that's also important, we want to
end up with independent components.

296
00:16:18,450 --> 00:16:20,560
So let me show you just how this works.

297
00:16:20,560 --> 00:16:25,150
So first of all,what I'm showing you here
is what we get before I apply the Varimax

298
00:16:25,150 --> 00:16:27,560
rotation, just to show you here.

299
00:16:27,560 --> 00:16:30,480
I also want to point out something about
the output here.

300
00:16:30,480 --> 00:16:33,330
So if you're looking carefully, you'll
notice that I was talking about

301
00:16:33,330 --> 00:16:37,810
components but all of a sudden, in the
output, we see factor pattern and

302
00:16:37,810 --> 00:16:41,550
factor one, factor two, factor three, et
cetera.

303
00:16:41,550 --> 00:16:46,010
So where did the word factor suddenly pop
up here, okay?

304
00:16:46,010 --> 00:16:50,690
What's happening here is that there is
something called a factor analysis,

305
00:16:50,690 --> 00:16:54,067
which is related to a principal components
analysis.

306
00:16:54,067 --> 00:16:56,470
And a lot of computer programs,

307
00:16:56,470 --> 00:17:02,967
you actually do your principal components
analysis from within the factor analysis.

308
00:17:02,967 --> 00:17:07,517
And that's what I have done here in SASS,
I ran a principal components analysis but

309
00:17:07,517 --> 00:17:09,890
it was run from within a factor analysis.

310
00:17:09,890 --> 00:17:13,910
You actually have to do that if you want
to apply this Varimax rotation.

311
00:17:13,910 --> 00:17:19,310
So the output says factors because this
was done as part of

312
00:17:19,310 --> 00:17:22,440
using the algorithm for doing a factor
analysis.

313
00:17:22,440 --> 00:17:25,790
This is still a principal components
analysis however, so I'm basically going

314
00:17:25,790 --> 00:17:30,070
to end up using the words factor and
component interchangeably here.

315
00:17:31,250 --> 00:17:34,250
You should just be aware that a factor
analysis is something different than

316
00:17:34,250 --> 00:17:35,940
a principal components analysis.

317
00:17:35,940 --> 00:17:37,040
But, again,

318
00:17:37,040 --> 00:17:40,370
you can do a principal components analysis
from within a factor analysis.

319
00:17:40,370 --> 00:17:40,890
If you do that,

320
00:17:40,890 --> 00:17:43,890
obviously, the output from your computer
program is going to say factor.

321
00:17:43,890 --> 00:17:46,550
But these are just components, these are
the components.

322
00:17:46,550 --> 00:17:49,910
So wherever you see, say, see factor, you
can just say, component.

323
00:17:51,130 --> 00:17:53,090
And what I'm showing you in this table,
what the,

324
00:17:53,090 --> 00:17:56,800
the, this output shows you is what are
called the factor loadings.

325
00:18:01,910 --> 00:18:07,600
So recall that all that the factors or the
components are these new variables.

326
00:18:07,600 --> 00:18:12,220
All they are is a linear combination of
your original variables.

327
00:18:12,220 --> 00:18:14,830
So they're, we're going to apply some
weights.

328
00:18:14,830 --> 00:18:19,290
And multiply each one of my original
variables by the, by a weight, and

329
00:18:19,290 --> 00:18:21,970
add those up to get a sort of a composite
variable.

330
00:18:21,970 --> 00:18:23,940
That's called the component.

331
00:18:23,940 --> 00:18:27,570
And, once I calculate those component
variables.

332
00:18:27,570 --> 00:18:31,230
I can then take the, those new variables,
the factor one or

333
00:18:31,230 --> 00:18:35,220
component one, and I can find a
correlation between each of

334
00:18:35,220 --> 00:18:40,450
my original variables with my my new
variable, with my component.

335
00:18:40,450 --> 00:18:41,610
And that's called the factor loading.

336
00:18:41,610 --> 00:18:44,870
So all that you're seeing here in this
table is a correlation coefficient.

337
00:18:44,870 --> 00:18:46,658
So I came out with a new variable.

338
00:18:46,658 --> 00:18:51,340
Principle component one, that new
variable, when I run a correlation

339
00:18:51,340 --> 00:18:54,080
coefficients, when I calculate correlation
coefficients with all my

340
00:18:54,080 --> 00:18:59,290
original variables, I get the correlations
you see pictured in this table.

341
00:18:59,290 --> 00:19:00,930
And remember, these principle components,

342
00:19:00,930 --> 00:19:05,120
again, are just a linear combination of
the original variables.

343
00:19:05,120 --> 00:19:08,500
But the factor loadings are very important
because you can look at

344
00:19:08,500 --> 00:19:13,380
them to get a sense of what is it that the
components represent, so what is

345
00:19:13,380 --> 00:19:17,660
principal component that new variable,
what one, what does that represent.

346
00:19:17,660 --> 00:19:21,170
Well if you look at the the factor
loadings before applying this

347
00:19:21,170 --> 00:19:22,190
Varimax rotation.

348
00:19:22,190 --> 00:19:26,650
It's very hard to say what principle
component one means.

349
00:19:26,650 --> 00:19:29,810
You can see that Android fat mass is
really highly correlated to

350
00:19:29,810 --> 00:19:31,090
this new variable.

351
00:19:31,090 --> 00:19:32,390
Lean mass, somewhat.

352
00:19:32,390 --> 00:19:34,650
Fat mass is highly correlated to this new
variable.

353
00:19:34,650 --> 00:19:36,540
So is BMI.

354
00:19:36,540 --> 00:19:40,440
The others are a little bit lower but you
get down here also mile time and

355
00:19:40,440 --> 00:19:45,140
miles run per week are highly correlated
to this principal component one.

356
00:19:45,140 --> 00:19:50,240
So it's some compilation of body size,
body fat mass, performance.

357
00:19:50,240 --> 00:19:53,760
It doesn't have a clear meaning to it.

358
00:19:53,760 --> 00:19:55,770
We'd like, instead, for

359
00:19:55,770 --> 00:19:59,550
each one of these original variables to
only appear in one of the components.

360
00:19:59,550 --> 00:20:02,330
To make it a very clear, to make it very
clear.

361
00:20:02,330 --> 00:20:03,600
What each of the components mean.

362
00:20:03,600 --> 00:20:05,980
So, we apply the Varimax rotation, and
notice what happens.

363
00:20:05,980 --> 00:20:10,870
This is wonderful, when we apply the
Varimax rotation, all of a sudden,

364
00:20:10,870 --> 00:20:15,410
the meaning of these different principal
components falls out.

365
00:20:15,410 --> 00:20:17,770
So, for example, the Android fat ratio,

366
00:20:17,770 --> 00:20:21,560
where if you look at before the rotation,
it loaded heavily on factor one but

367
00:20:21,560 --> 00:20:25,720
it also loaded somewhat on factor three
and somewhat on factor four.

368
00:20:25,720 --> 00:20:28,730
If you look now after the rotation,
Android fat

369
00:20:28,730 --> 00:20:31,740
ratio has a really high correlation with
principal component one and

370
00:20:31,740 --> 00:20:35,660
it loads hardly at all on the rest of the
principal components.

371
00:20:35,660 --> 00:20:39,780
So this is going to make interpretation
much, much easier.

372
00:20:39,780 --> 00:20:43,720
If you look down the columns, each one of
these columns is one of these,

373
00:20:43,720 --> 00:20:45,460
is the principal component.

374
00:20:45,460 --> 00:20:48,346
So principal component one, for example,

375
00:20:48,346 --> 00:20:53,416
has a high correlation, the factor
loadings are high for Android fat ratio,

376
00:20:53,416 --> 00:20:58,367
which is a measure of visceral fat, or
belly fat, for fat mass and for BMI.

377
00:20:58,367 --> 00:21:01,700
So we have correlations of 0.87, 0.79, and
0.91.

378
00:21:01,700 --> 00:21:07,010
So those three variables load very
strongly on principle component one.

379
00:21:07,010 --> 00:21:09,850
That's telling you that principle
component one has something to

380
00:21:09,850 --> 00:21:12,380
do with body fatness.

381
00:21:12,380 --> 00:21:16,440
If you keep going down the column you'll
notice that most of the other

382
00:21:16,440 --> 00:21:21,000
variables have values very close to zero,
correlations very close to zero.

383
00:21:21,000 --> 00:21:24,630
Mile time is a little bit correlated
still, but it's it's of,

384
00:21:24,630 --> 00:21:27,320
still a fairly small correlation.

385
00:21:27,320 --> 00:21:31,360
So we can now interpret what principal
component one actually means.

386
00:21:31,360 --> 00:21:36,360
It's a measure of body fatness, how much
fat are you carrying on your body.

387
00:21:36,360 --> 00:21:38,830
We go to principal component two, and we
look down the column, and

388
00:21:38,830 --> 00:21:40,880
we look at these correlation coefficients.

389
00:21:40,880 --> 00:21:44,780
So Android fat, fat mass, BMI, are no
longer strongly correlated,

390
00:21:44,780 --> 00:21:48,000
they're not that strongly correlated with
principal component two.

391
00:21:48,000 --> 00:21:52,260
However, the measures of running, training
and performance are.

392
00:21:52,260 --> 00:21:55,210
So mile time is highly correlated with
principal component two.

393
00:21:55,210 --> 00:22:00,350
A higher, faster mile time gives you a
higher correlation here.

394
00:22:00,350 --> 00:22:03,410
A so this is saying that principal
component two is a measure of

395
00:22:03,410 --> 00:22:06,470
how competitive you are, higher numbers
mean more competitive.

396
00:22:07,480 --> 00:22:11,040
Mileage meaning your running more miles
per week that was highly correlated with

397
00:22:11,040 --> 00:22:12,440
principal component two.

398
00:22:12,440 --> 00:22:17,470
Interval pace also strongly loads on
principal component two and so

399
00:22:17,470 --> 00:22:20,940
we can see that principal component two is
a measure of

400
00:22:20,940 --> 00:22:23,270
running performance and competitiveness.

401
00:22:24,370 --> 00:22:26,950
Now, here's something that's really
interesting.

402
00:22:28,010 --> 00:22:33,560
Menarche age, which doesn't have anything
naturally to do with running performance.

403
00:22:33,560 --> 00:22:36,610
This is the age at which a woman begins to
menstruate.

404
00:22:36,610 --> 00:22:41,710
You would think that that variable would
load on the same principal component as

405
00:22:41,710 --> 00:22:44,340
the other variables of menstrual function.

406
00:22:44,340 --> 00:22:46,740
But, actually, in this data set,

407
00:22:46,740 --> 00:22:50,810
menarche age loads most strongly on
principal component two.

408
00:22:50,810 --> 00:22:54,790
Notice it does not load strongly on any of
the other components.

409
00:22:54,790 --> 00:22:59,740
What this is telling you is that menarche
age, in this population,

410
00:22:59,740 --> 00:23:04,160
is strongly related to a woman's running
performance and competitiveness.

411
00:23:04,160 --> 00:23:05,940
And if you think about it carefully,

412
00:23:05,940 --> 00:23:09,700
this kinds of make sense because these are
adolescent woman runners and

413
00:23:09,700 --> 00:23:14,410
some of them have, may have started
running before they hit puberty.

414
00:23:14,410 --> 00:23:20,390
And, that running has a big influence on
the age at which they begin menstruating.

415
00:23:20,390 --> 00:23:23,400
So that's kind of an interesting insight
that comes out of the principal

416
00:23:23,400 --> 00:23:24,820
components analysis.

417
00:23:24,820 --> 00:23:28,930
Menarche age is much more strongly related
to your running competiveness,

418
00:23:28,930 --> 00:23:34,910
competitiveness, than it is to other men,
measures of menstrual functions.

419
00:23:34,910 --> 00:23:36,110
So that's an interesting insight.

420
00:23:37,180 --> 00:23:38,797
Now looking at principal component three,

421
00:23:38,797 --> 00:23:43,140
principal component three turns out to be
some kind of measures of body size.

422
00:23:43,140 --> 00:23:46,310
So, if you look down Android fat ratio,
fat mass and

423
00:23:46,310 --> 00:23:49,200
BMI do not load strongly on principal
component three.

424
00:23:49,200 --> 00:23:55,460
But, lean mass and height load very
strongly on principal component three.

425
00:23:55,460 --> 00:24:00,000
That's telling you that this is some
measure of kind of how tall, and

426
00:24:00,000 --> 00:24:01,860
big boned, and how much muscle you have.

427
00:24:01,860 --> 00:24:08,390
It's some measure of body size, and what's
really cool here is that normally measures

428
00:24:08,390 --> 00:24:14,000
of body size and composition they're kind
of they're kind of hard to tease apart.

429
00:24:14,000 --> 00:24:18,370
Normally all these measures, fat mass,
BMI, even lean mass, height,

430
00:24:18,370 --> 00:24:23,750
all these things conflate both your body
size and your body fatness, right.

431
00:24:23,750 --> 00:24:27,950
Because, if you're somebody who is, has a
higher fat mass,

432
00:24:27,950 --> 00:24:31,700
that could be both because you're just
somebody who is bigger, and it could be

433
00:24:31,700 --> 00:24:34,100
because if you're bigger you're obviously
going to ca, carry more fat mass.

434
00:24:34,100 --> 00:24:37,920
But it also could be because you're
somebody who is carrying extra fat.

435
00:24:37,920 --> 00:24:40,760
Higher fat mass could mean both of those
things.

436
00:24:40,760 --> 00:24:42,240
But what we've done here is,

437
00:24:42,240 --> 00:24:45,130
remember the principal components are
independent of one another.

438
00:24:45,130 --> 00:24:46,480
They're uncorrelated.

439
00:24:46,480 --> 00:24:49,320
So we have untangled body fatness,

440
00:24:49,320 --> 00:24:53,390
that is how much body fat you're carrying,
from body size.

441
00:24:53,390 --> 00:24:58,540
And Principal component three represents
now just a measure of just how big you

442
00:24:58,540 --> 00:25:00,870
are, independent of how much fat you're
carrying.

443
00:25:00,870 --> 00:25:04,700
Whereas principle component one represents
a measure of how much fat you're com,

444
00:25:04,700 --> 00:25:07,050
carrying, independent sort of of how big
you are.

445
00:25:07,050 --> 00:25:10,310
So we've untangled these two constructs,
which is really useful,

446
00:25:10,310 --> 00:25:14,240
because now we can say, we can isolate the
effect of each one of those constructs.

447
00:25:15,380 --> 00:25:19,270
And then finally, principal component four
is just a measure of menstrual regularity.

448
00:25:19,270 --> 00:25:22,000
So this was the periods in the past year
and

449
00:25:22,000 --> 00:25:24,490
the average periods per year since
menarche.

450
00:25:24,490 --> 00:25:27,660
Those two variables look very strongly on
principal component four,

451
00:25:27,660 --> 00:25:30,380
and actually none of the other variables
really have

452
00:25:30,380 --> 00:25:32,620
any correlation with principal component
four.

453
00:25:32,620 --> 00:25:37,040
So, this is really a measure of how
regular your menstrual periods are.

454
00:25:37,040 --> 00:25:40,150
So, those are the meanings of the, the
four components.

455
00:25:40,150 --> 00:25:45,080
And I'll just mention that I also reran
the analysis.

456
00:25:45,080 --> 00:25:48,520
Asking to retain five components, just to
see what would happen.

457
00:25:48,520 --> 00:25:52,110
And so what happened when I did that was
actually that menarche age, which had

458
00:25:52,110 --> 00:25:56,590
loaded with running performance before,
became its own principal component.

459
00:25:56,590 --> 00:26:00,990
So the fifth component became a measure
purely of menarche age.

460
00:26:00,990 --> 00:26:03,500
And the rest of the components stayed
basically the same.

461
00:26:03,500 --> 00:26:07,070
So if I wanted to isolate the effect of
menarche age and

462
00:26:07,070 --> 00:26:11,330
separate it from running performance, I
would have to ask for a fifth component,

463
00:26:11,330 --> 00:26:15,710
so if I cared about specifically about
that, the effect of that variable.

464
00:26:15,710 --> 00:26:19,230
But it's interesting, and it gives me
insight when I ask for four components,

465
00:26:19,230 --> 00:26:22,785
because it tells me that there's this
relationship between running and

466
00:26:22,785 --> 00:26:27,390
menarche age in this population.

467
00:26:27,390 --> 00:26:29,640
Now, these are the factor loadings,

468
00:26:29,640 --> 00:26:32,100
which I was showing you on the previous
slide here.

469
00:26:32,100 --> 00:26:35,700
I just want to tell you exactly how the
principal

470
00:26:35,700 --> 00:26:37,580
components are actually calculated.

471
00:26:37,580 --> 00:26:40,830
The factor loadings are the correlations
between each

472
00:26:40,830 --> 00:26:43,760
principal component variable and the
original variables.

473
00:26:43,760 --> 00:26:47,670
But, what are the actual weights that are
used to calculate those

474
00:26:47,670 --> 00:26:48,360
principal components.

475
00:26:48,360 --> 00:26:50,930
How did, how does the computer actually
calculate those?

476
00:26:50,930 --> 00:26:54,190
So, what I'm showing you on this next
screen is the actual weights

477
00:26:54,190 --> 00:26:56,810
that are used to calculate the principal
component.

478
00:26:56,810 --> 00:27:00,790
So just how, how do we actually calculate
these, so here's a set of weights.

479
00:27:00,790 --> 00:27:03,690
Notice that the pattern is the same here,

480
00:27:03,690 --> 00:27:07,610
so these are the weights that would be
multiplied by the original variables and

481
00:27:07,610 --> 00:27:10,050
summed up, and that would give us
principal component one.

482
00:27:10,050 --> 00:27:13,890
So you would take a woman's Android fat
ratio, whatever number she has for

483
00:27:13,890 --> 00:27:16,402
that, and would multiply it by 0.4.

484
00:27:16,402 --> 00:27:20,050
You would then add that to her fat mass
times 0.28.

485
00:27:20,050 --> 00:27:22,840
You would then add that to her BMI times
0.41.

486
00:27:22,840 --> 00:27:26,350
You would then add that to her lean mass
times 0.09.

487
00:27:26,350 --> 00:27:31,182
You would add that to her height times
negative 0.19, etc.

488
00:27:31,182 --> 00:27:35,410
You would do this for all 11 variables,
you'd come out with some number,

489
00:27:35,410 --> 00:27:40,610
that score is a, is a women's score on
principal component one.

490
00:27:40,610 --> 00:27:45,140
Those scores will turn out to have a mean
of zero and a standard deviation of one so

491
00:27:45,140 --> 00:27:47,550
they will be in standard deviation unit.

492
00:27:47,550 --> 00:27:52,060
So a woman who comes out with a, say a,
you know, a standard deviation of

493
00:27:52,060 --> 00:27:56,150
a score on principal component one of 1.6,
that would mean we had somebody who

494
00:27:56,150 --> 00:28:00,409
was carrying a relatively high amount of
body fat for this population.

495
00:28:01,680 --> 00:28:05,120
And again you can see that whatever
variables are heavily weighted in

496
00:28:05,120 --> 00:28:07,830
the linear combination, those are going to
be the same ones that have

497
00:28:07,830 --> 00:28:10,350
a high correlation with principal
component one.

498
00:28:10,350 --> 00:28:13,920
So that's just how you actually calculate
the principal components,

499
00:28:13,920 --> 00:28:17,080
of course you don't have to do this by
hand the computer will

500
00:28:17,080 --> 00:28:21,640
automatically calculate new variables,
principle of components one, two,

501
00:28:21,640 --> 00:28:25,310
three and four, for all of the
observations in your data set.

502
00:28:25,310 --> 00:28:26,620
But this is just where it comes from.

503
00:28:28,660 --> 00:28:31,490
All right, so now we've gone through all
the steps of the PCA.

504
00:28:31,490 --> 00:28:32,820
We applied the Varimax rotation.

505
00:28:32,820 --> 00:28:35,480
We interpreted the meanings of our
components.

506
00:28:36,760 --> 00:28:38,830
Did that.
And then the final thing we can do is,

507
00:28:38,830 --> 00:28:42,960
now we can take these new variables, now
we have four new variables that

508
00:28:42,960 --> 00:28:47,640
contain scores in standard deviation units
for all the women in our data set.

509
00:28:47,640 --> 00:28:49,940
We can take those new variables.

510
00:28:49,940 --> 00:28:52,110
And we can do further statistics on them.

511
00:28:54,380 --> 00:28:56,240
So what I actually did here was,

512
00:28:56,240 --> 00:28:59,700
I ran a logistic regression to predict
stress fractures.

513
00:28:59,700 --> 00:29:01,010
So we have a binary outcome.

514
00:29:01,010 --> 00:29:03,330
They either had a stress fracture or they
didn't.

515
00:29:03,330 --> 00:29:06,930
I put my new variables, my four principle
components, into a model.

516
00:29:06,930 --> 00:29:12,680
Where the outcome is the logitive stress
fracture, and I've got my body fatness,

517
00:29:12,680 --> 00:29:16,260
running performance, body size, and
menstrual function variables now.

518
00:29:16,260 --> 00:29:19,300
Now, I'll just mention that because the
principal components are actually

519
00:29:19,300 --> 00:29:23,450
independent, they're uncorrelated with one
another, it actually doesn't matter if I

520
00:29:23,450 --> 00:29:26,500
throw them all together in a single
multi-variate analysis.

521
00:29:26,500 --> 00:29:29,770
Or if I run them in separate univariate
regressions.

522
00:29:29,770 --> 00:29:33,210
You really don't even need to do a
multivariate analysis anymore

523
00:29:33,210 --> 00:29:35,020
because there's nothing to adjust for.

524
00:29:35,020 --> 00:29:37,610
The predictors are already independent.

525
00:29:37,610 --> 00:29:39,110
So that makes your model building quite
easy.

526
00:29:39,110 --> 00:29:43,040
So, look, for example, if I were to run
the same model with stress fractures as

527
00:29:43,040 --> 00:29:45,430
my outcome, but only body fatness in the
model and

528
00:29:45,430 --> 00:29:48,020
nothing else, I would get the exact same
data coefficient.

529
00:29:48,020 --> 00:29:49,240
And the exact same people are very,

530
00:29:49,240 --> 00:29:54,640
very close, so that's makes life easy in
terms of model building.

531
00:29:54,640 --> 00:29:57,610
But now let's just kind of look at the
results, now remember that I actually

532
00:29:57,610 --> 00:30:01,590
showed you some results about stress
fractures in an earlier module.

533
00:30:01,590 --> 00:30:02,600
And all of the measures,

534
00:30:02,600 --> 00:30:05,380
we were looking at the measures of body
size and composition.

535
00:30:05,380 --> 00:30:09,580
And it turned out that having a bigger
body size or carrying more fat, all of

536
00:30:09,580 --> 00:30:15,120
those things were linked to protection
against a lower risk of stress fracture.

537
00:30:15,120 --> 00:30:19,270
But now, we can separate out the body
fatness, that is,

538
00:30:19,270 --> 00:30:22,012
how much fat you're carrying from your
body size,

539
00:30:22,012 --> 00:30:25,350
we weren't able really to do that before,
those things were confounded.

540
00:30:25,350 --> 00:30:28,530
When we do that, you'll notice that
principal component one, which is

541
00:30:28,530 --> 00:30:33,420
body fatness, remember in the original
analysis, the earlier analysis I showed

542
00:30:33,420 --> 00:30:38,470
you, the best protection against fractures
was due to having a higher fat mass.

543
00:30:38,470 --> 00:30:43,060
But fat mass was related to menstrual
function, and so it

544
00:30:43,060 --> 00:30:47,120
was probably the case that carrying more
fat improved your menstrual function and

545
00:30:47,120 --> 00:30:49,540
that's why you're protected against
fractures.

546
00:30:49,540 --> 00:30:52,510
Now we've teased out the effective body
fatness from the effective

547
00:30:52,510 --> 00:30:53,190
menstrual function.

548
00:30:53,190 --> 00:30:58,560
When we do that, the body fatness variable
is no longer statistically significant and

549
00:30:58,560 --> 00:31:01,290
you can see that it's, you know that it's
still a little bit protective to

550
00:31:01,290 --> 00:31:04,150
have more body fat but the estimate here
is not very big.

551
00:31:04,150 --> 00:31:05,700
The beta coefficient is not very big.

552
00:31:06,840 --> 00:31:08,020
If you look at principal component two,

553
00:31:08,020 --> 00:31:10,940
which is this measure of running, how much
you run and how fast you run.

554
00:31:12,690 --> 00:31:15,240
That actually is significantly related to
fracture.

555
00:31:15,240 --> 00:31:19,390
The beta coefficient is pretty big, 0.9,
and so if you run more,

556
00:31:19,390 --> 00:31:21,900
if you're more competitive, you have a
higher risk of fracture,

557
00:31:21,900 --> 00:31:24,920
this beta coefficient is positive, higher
risk.

558
00:31:24,920 --> 00:31:27,940
That makes sense, because the harder you
run, the more you run, and

559
00:31:27,940 --> 00:31:31,530
the faster you run, the more like you are,
likely you are to fracture.

560
00:31:31,530 --> 00:31:33,772
The third principal component, body size,

561
00:31:33,772 --> 00:31:36,290
doesn't quite make statistical
significance.

562
00:31:36,290 --> 00:31:37,140
The p value is 0.07.

563
00:31:37,140 --> 00:31:42,770
But the, the beta coefficient is about the
same as it was for running performance.

564
00:31:42,770 --> 00:31:44,964
It's negative, meaning that this is
protective.

565
00:31:44,964 --> 00:31:46,840
And what this is telling you is that,

566
00:31:46,840 --> 00:31:51,190
if you're just a bigger person, if you're
taller, if you're big boned and

567
00:31:51,190 --> 00:31:55,220
maybe carry more muscle, all of that
protects you against fracture.

568
00:31:55,220 --> 00:31:58,160
And that probably has to do with the fact
that if you're a bigger person, you

569
00:31:58,160 --> 00:32:01,870
tend to have bigger bone density because,
you know, we call people big boned.

570
00:32:01,870 --> 00:32:03,480
If you have more bone density,

571
00:32:03,480 --> 00:32:06,210
more bone strength, that obviously would
protect you against fracture.

572
00:32:07,220 --> 00:32:09,970
And then, finally, the principle competent
four.

573
00:32:09,970 --> 00:32:12,270
This one was to have the strongest
relationship with fracture.

574
00:32:12,270 --> 00:32:15,290
It is significant and it's negative 1.04.

575
00:32:15,290 --> 00:32:19,030
So having better menstrual function
protects you

576
00:32:19,030 --> 00:32:20,250
pretty strongly against fracture.

577
00:32:20,250 --> 00:32:24,210
And that's actually a well known thing
that in this population of women runners,

578
00:32:24,210 --> 00:32:27,040
having better menstrual function improves
your bone density and

579
00:32:27,040 --> 00:32:28,310
protects against fracture.
