1
00:00:04,850 --> 00:00:07,500
The three images that you're looking at
the slides,

2
00:00:07,500 --> 00:00:12,750
the upper left is the emergency response
team to the airline crash

3
00:00:12,750 --> 00:00:17,320
that occurred at San Francisco
International Airport in July of 2013.

4
00:00:17,320 --> 00:00:21,910
The photo on the right showing the
airplane jet crashing on

5
00:00:21,910 --> 00:00:25,370
the flight deck on a United States
aircraft carrier.

6
00:00:25,370 --> 00:00:30,890
And then, the bottom slides is showing the
nuclear power

7
00:00:30,890 --> 00:00:35,240
plant in Chernobyl that actually blew up.

8
00:00:35,240 --> 00:00:41,136
Each one of these are reflections of an
industry that has undergone significant

9
00:00:41,136 --> 00:00:46,970
changes in terms of attitudes and culture
when it comes to air events.

10
00:00:46,970 --> 00:00:51,580
The concept that we're going to speak
about over the next few

11
00:00:51,580 --> 00:00:58,030
minutes has to do with high reliability
organizations, or HROs.

12
00:00:59,110 --> 00:01:02,040
What exactly is a HRO?

13
00:01:02,040 --> 00:01:07,650
An organization that has succeeded in
avoiding catastrophes in an environment

14
00:01:07,650 --> 00:01:13,880
where normal accidents can be expected due
to risk factors and complexity.

15
00:01:13,880 --> 00:01:17,270
We're going to speak a little bit about
what are some of the commonalities and

16
00:01:17,270 --> 00:01:20,650
characteristics of high reliability
organization.

17
00:01:21,890 --> 00:01:27,910
Now, big question is, does your healthcare
organization fit this definition?

18
00:01:27,910 --> 00:01:31,729
You know, studies have shown that there
are a fair amount of events that occur at

19
00:01:31,729 --> 00:01:33,382
a patient's, majority of whom,

20
00:01:33,382 --> 00:01:36,320
fortunately, do not create harm to the
patient.

21
00:01:36,320 --> 00:01:38,470
But they do occur.

22
00:01:38,470 --> 00:01:41,828
So, the big classic example, the classic
examples of

23
00:01:41,828 --> 00:01:46,646
high reliability organizations, do include
commercial airline industry,

24
00:01:46,646 --> 00:01:51,510
prior to 1980, they were subject to
multiple catastrophic events.

25
00:01:51,510 --> 00:01:53,810
Planes that land where there are no
runways, or

26
00:01:53,810 --> 00:01:58,060
into the side of a mountain big, makes a
bad day for that company's stock.

27
00:01:58,060 --> 00:02:02,920
The middle photo, the nuclear power plants
with all the hazards that can occur.

28
00:02:02,920 --> 00:02:05,860
If someone is not paying attention to the
monitors and

29
00:02:05,860 --> 00:02:07,610
the controls that are in place.

30
00:02:07,610 --> 00:02:11,834
And then finally, the aircraft carrier
flight decks are huge potential for

31
00:02:11,834 --> 00:02:16,388
errors, especially with multimillion
dollar jets being maneuvered into a space

32
00:02:16,388 --> 00:02:20,744
that's smaller than a football field,
carrying gallons of aviation fuel and

33
00:02:20,744 --> 00:02:23,186
explosive devices being loaded and moved
or

34
00:02:23,186 --> 00:02:27,890
directed by young people that might just
have a high school diploma.

35
00:02:27,890 --> 00:02:31,390
So, what are some of those commonalities
of high reliability organizations?

36
00:02:32,780 --> 00:02:36,320
High reliability organizations operate in
an unforgiving social and

37
00:02:36,320 --> 00:02:38,370
political environment.

38
00:02:38,370 --> 00:02:43,050
Each one of these three industries are
heavily scrutinized,

39
00:02:43,050 --> 00:02:49,260
either by a governmental or regulatory
agency and basically, when the slightest

40
00:02:49,260 --> 00:02:54,480
thing goes wrong, it certainly makes the
paper, makes the media and

41
00:02:54,480 --> 00:03:00,460
generally brings bad press to any of these
industries that has such an event.

42
00:03:01,740 --> 00:03:05,344
Another commonality of high reliability
organization is that

43
00:03:05,344 --> 00:03:10,130
the technologies involved are extremely
risky and present potential for error.

44
00:03:11,634 --> 00:03:15,927
Knowing that something potentially can
happen that can lead to

45
00:03:15,927 --> 00:03:20,706
a power plant blowing up can also be
reliant on the technologies that,

46
00:03:20,706 --> 00:03:25,960
in itself ,may be subject to some
variation, or potential some glitches.

47
00:03:27,260 --> 00:03:30,560
Number three, the scale of possible
consequences from errors or

48
00:03:30,560 --> 00:03:34,050
mistake precludes learning through
experimentation.

49
00:03:34,050 --> 00:03:38,270
That is, we can't see how many times we
can blow up a power plant to figure out

50
00:03:38,270 --> 00:03:41,490
how do we need to fix the problem to
prevent it from happening again.

51
00:03:43,090 --> 00:03:46,966
Some of the characteristics that's been
identified in high

52
00:03:46,966 --> 00:03:52,200
reliability organizations has a lot to do
with the culture of the organization.

53
00:03:53,360 --> 00:03:56,096
Number one, there's almost an obsession or

54
00:03:56,096 --> 00:03:59,740
preoccupation with the possibility of
failure.

55
00:03:59,740 --> 00:04:02,840
Organizations that are high reliability
often they're at

56
00:04:02,840 --> 00:04:07,359
times are concerned about well, what's the
potential next thing that can go wrong.

57
00:04:09,500 --> 00:04:14,502
They also have a reluctance to simplify
interpretations or, you know,

58
00:04:14,502 --> 00:04:20,500
simplify information that they're getting
and taking that as face value.

59
00:04:20,500 --> 00:04:24,700
So, there's a tendency for these
organization that dig in, go deeper.

60
00:04:24,700 --> 00:04:29,323
Get down to the nitty gritty to figure out
how do we understand what's going on; to

61
00:04:29,323 --> 00:04:33,611
figure out what the root causes are and
the contributing factors to be able to

62
00:04:33,611 --> 00:04:37,440
correct or solve the problem, and to keep
it from happening again.

63
00:04:39,060 --> 00:04:42,710
The third characteristic is sensitivity to
operation.

64
00:04:42,710 --> 00:04:47,500
Many, many times folks that are involved
with investigating these events

65
00:04:47,500 --> 00:04:52,510
really do drill down to figure out, well
how was things actually carried out?

66
00:04:52,510 --> 00:04:54,590
How are things supposed to be working?

67
00:04:54,590 --> 00:04:58,760
What is the actual steps or process that
need to be done.

68
00:04:58,760 --> 00:05:01,390
And to borrow something out of the
methodology.

69
00:05:01,390 --> 00:05:02,853
The concept of gemba,

70
00:05:02,853 --> 00:05:07,860
G-E-M-B-A, is going to the align where the
work is actually being done.

71
00:05:09,750 --> 00:05:14,440
Number four, these organizations all have
a commitment to resilience.

72
00:05:14,440 --> 00:05:16,234
And that they're flexible and

73
00:05:16,234 --> 00:05:20,374
they will work within the confines that
they have available to be able to

74
00:05:20,374 --> 00:05:25,280
figure out what is the solution to prevent
these bad things from happening again.

75
00:05:25,280 --> 00:05:30,100
And then number five, the deference to
expertise.

76
00:05:30,100 --> 00:05:34,132
So, in this case, it's not the senior
level employee or

77
00:05:34,132 --> 00:05:37,828
management that are actually, they would
rely on,

78
00:05:37,828 --> 00:05:43,200
to give them information on, what's going
on and what had happened.

79
00:05:43,200 --> 00:05:46,150
They would have to rely on the person that
actually,

80
00:05:46,150 --> 00:05:49,230
really does have the best core knowledge,
of whats going on.

81
00:05:49,230 --> 00:05:54,460
Which potentially may not be management,
and may not be the senior most employee.

82
00:05:54,460 --> 00:05:57,790
But somebody who's actually carrying out
the actual task or detail.

83
00:05:59,100 --> 00:06:05,880
So, those High Reliability Organizations
all have those five characteristics.

84
00:06:05,880 --> 00:06:07,608
Now, one would have to say,

85
00:06:07,608 --> 00:06:12,480
well, what's it take to become a High
Reliability Organization?

86
00:06:12,480 --> 00:06:16,860
And there's three core components that we
tend to think about.

87
00:06:18,000 --> 00:06:21,880
The top one is the need to have
leadership.

88
00:06:21,880 --> 00:06:27,350
Leadership's involvement, leadership's
focus, leadership direction.

89
00:06:27,350 --> 00:06:32,690
Leadership, drives a lot of the strategy
of the organization in heading for

90
00:06:32,690 --> 00:06:33,480
the right direction.

91
00:06:33,480 --> 00:06:37,610
The second piece is a safety culture.

92
00:06:37,610 --> 00:06:40,678
And safety culture, we're going to speak
about that a little bit more,

93
00:06:40,678 --> 00:06:43,538
has some technical components that needs
to be in place in order to

94
00:06:43,538 --> 00:06:45,140
establish an environment.

95
00:06:45,140 --> 00:06:50,749
Where an organization is willing to learn
from either it's own mistakes or mistakes

96
00:06:50,749 --> 00:06:56,170
that have been made by others, so that it
doesn't replay or repeat a bad event.

97
00:06:57,320 --> 00:07:01,276
The third component is a process
improvement system,

98
00:07:01,276 --> 00:07:06,866
some methodology to basically enhance or
improve the outcome or performance

99
00:07:06,866 --> 00:07:12,309
of system process issues that may have
been going on in the organization.

100
00:07:14,390 --> 00:07:17,540
Now, let's take a moment and speak about
safety culture.

101
00:07:17,540 --> 00:07:20,560
We talked a lot about this in healthcare.

102
00:07:20,560 --> 00:07:22,440
I'm not totally convinced though.

103
00:07:22,440 --> 00:07:26,030
Many of us know exactly what constitute a
safety culture.

104
00:07:26,030 --> 00:07:31,370
I think that these five bullets are things
to be thinking about in terms of

105
00:07:31,370 --> 00:07:36,289
assuring that folks have a comfort level
and a trust level within the organization.

106
00:07:37,420 --> 00:07:41,080
Starting off with the aim of the safety
culture is not to

107
00:07:41,080 --> 00:07:43,160
establish a blame free culture.

108
00:07:43,160 --> 00:07:46,995
It is really more geared towards
understanding how do we make things,

109
00:07:46,995 --> 00:07:49,790
staff understand that there's
accountability,

110
00:07:49,790 --> 00:07:53,310
in terms of making sure you're doing the
right thing.

111
00:07:53,310 --> 00:07:57,275
Or making sure that you call things out
when need be and that there's some

112
00:07:57,275 --> 00:08:01,280
degree of an accountability for their
particular action or function.

113
00:08:02,740 --> 00:08:07,283
There really needs to separate the
blameless errors, the ones that we

114
00:08:07,283 --> 00:08:11,518
need to learn from in order to correct the
system process issues,

115
00:08:11,518 --> 00:08:15,907
from the blame worthy ones, where the
expectation is discipline,

116
00:08:15,907 --> 00:08:21,470
consistently and equitably applied on a
regular basis when need be.

117
00:08:21,470 --> 00:08:22,629
An, example of that,

118
00:08:22,629 --> 00:08:26,472
if a staff person intentionally violates
the policy, even though they had

119
00:08:26,472 --> 00:08:30,940
been informed that they need to follow a
particular policy and procedure.

120
00:08:30,940 --> 00:08:35,150
That may be considered, potentially, a
blame worthy situation.

121
00:08:35,150 --> 00:08:40,400
Versus in the blameless situation where
the system and process is so bad

122
00:08:40,400 --> 00:08:46,650
that staff, due to production pressure,
are forced to shortcut things.

123
00:08:46,650 --> 00:08:51,040
Due to the organization's request to be
able to either crank out more

124
00:08:51,040 --> 00:08:55,520
procedures or provide more medications or
see more patients,

125
00:08:55,520 --> 00:08:59,990
the shortcuts potentially are system
process issues that can create errors.

126
00:09:02,020 --> 00:09:07,179
The fourth bullet, there needs to be a
uniform process for assessing errors and

127
00:09:07,179 --> 00:09:09,181
the patterns that is learned or

128
00:09:09,181 --> 00:09:13,140
discovered as part of the performance
improvement system.

129
00:09:13,140 --> 00:09:17,910
That is, collecting the data and reviewing
it to assess whether or

130
00:09:17,910 --> 00:09:19,210
not there's a need to improve.

131
00:09:20,560 --> 00:09:23,828
And what's really important is the fifth
bullet,

132
00:09:23,828 --> 00:09:27,324
is the need to establish one code of
behavior in terms of

133
00:09:27,324 --> 00:09:32,910
how the organization will respond, top to
bottom, whenever an event occurs.

134
00:09:32,910 --> 00:09:36,482
I know in management there's a tendency to
knee jerk and

135
00:09:36,482 --> 00:09:41,250
insist that after an event occur, that,
you know, we go fire the staff.

136
00:09:41,250 --> 00:09:45,480
Well, in healthcare today, there's really
an issue with firing all the staff.

137
00:09:45,480 --> 00:09:48,850
Because I know we're going through a phase
where folks have been in

138
00:09:48,850 --> 00:09:51,590
the profession for a long time, more and
more people retiring,

139
00:09:51,590 --> 00:09:54,010
there's not enough people there to be able
to take their place.

140
00:09:54,010 --> 00:09:59,439
So really it comes down to defining, are
we dealing with system

141
00:09:59,439 --> 00:10:05,600
process issue or we dealing with,
intentional negligent behavior.

142
00:10:05,600 --> 00:10:09,490
And, the response that our organization
have to any of

143
00:10:09,490 --> 00:10:13,470
these events really does help establish or
sets the culture of the organization.

144
00:10:16,060 --> 00:10:18,661
So, just like we mentioned, there are
three components for

145
00:10:18,661 --> 00:10:20,670
a high reliability organization.

146
00:10:20,670 --> 00:10:24,140
What are the three key components for a
safety culture?

147
00:10:24,140 --> 00:10:26,020
Well, it starts off with trust.

148
00:10:27,400 --> 00:10:31,200
The little graphic showing the person
doing the trust fall,

149
00:10:31,200 --> 00:10:34,510
whether falling off the platform into the
hands of their colleagues.

150
00:10:34,510 --> 00:10:38,558
You know, does staff, medical staff, house
staff,

151
00:10:38,558 --> 00:10:43,410
administration, do they trust the system
to do the right thing?

152
00:10:44,740 --> 00:10:50,212
The second part, and you can see the red
phone, you know, the, Presidential,

153
00:10:50,212 --> 00:10:55,020
phone that connects to the premier in the
Kremlin during the Cold War.

154
00:10:55,020 --> 00:10:59,550
It's like a hotline between the two
leaders of the two countries.

155
00:10:59,550 --> 00:11:03,150
It's a reporting mechanism, a conduit to
raise the flag so

156
00:11:03,150 --> 00:11:07,902
that the organizations are aware that
there maybe some system process issue or

157
00:11:07,902 --> 00:11:10,909
events that occurred within the
organization.

158
00:11:12,060 --> 00:11:15,142
And then the third piece, which we've seen
earlier in

159
00:11:15,142 --> 00:11:20,028
a high reliability organization, is a
process improvement cycle.

160
00:11:20,028 --> 00:11:21,900
Some way to evaluate whether or

161
00:11:21,900 --> 00:11:26,508
not a change needs to be applied to
process, a process to evaluate whether or

162
00:11:26,508 --> 00:11:30,630
not it will work, and then modification of
the process if needed.

163
00:11:31,710 --> 00:11:36,285
It's interesting, when the organization's
staff, medical staff,

164
00:11:36,285 --> 00:11:41,440
trust the system, you'll get plenty of
reports of when there's disconnect.

165
00:11:41,440 --> 00:11:43,180
But the trust will not be gained or

166
00:11:43,180 --> 00:11:47,690
maintained if the improvement aren't put
in place to address those issues.

167
00:11:47,690 --> 00:11:52,774
So, the core foundation to making this
whole thing work really starts off

168
00:11:52,774 --> 00:11:54,880
with trusting the system.

169
00:11:54,880 --> 00:11:59,110
Staff can feel comfortable reporting
things, if they report things,

170
00:11:59,110 --> 00:12:02,290
the organization needs to respond to
improve things.

171
00:12:02,290 --> 00:12:04,810
And then that helps build trust in the
system.

172
00:12:04,810 --> 00:12:08,700
I'm sure many of us have dealt with
situations where many times you

173
00:12:08,700 --> 00:12:13,250
put in a report about a particular event
that's occurred over and over.

174
00:12:13,250 --> 00:12:16,910
And if nothing is done to address that
issue, then one would have to say,

175
00:12:16,910 --> 00:12:20,500
well, what kind of trust do I have in the
system to do the right thing?

176
00:12:20,500 --> 00:12:24,394
So, it's a, it's a continuous circle where
it's interlocked and they're

177
00:12:24,394 --> 00:12:28,210
interconnected in terms of establishing
the culture of an organization.

178
00:12:28,210 --> 00:12:31,859
[BLANK_AUDIO]
